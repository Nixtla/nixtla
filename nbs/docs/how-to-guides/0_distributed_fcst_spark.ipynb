{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff81b5a-514d-4d8b-953e-c8f7cb4ba215",
   "metadata": {},
   "source": [
    "# How to on Spark: Forecasting\n",
    "> Run TimeGPT distributedly on top of Spark.\n",
    "\n",
    "`TimeGPT` works on top of Spark, Dask, and Ray through Fugue. `TimeGPT` will read the input DataFrame and use the corresponding engine. For example, if the input is a Spark DataFrame, StatsForecast will use the existing Spark session to run the forecast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3119cd0-9b9d-4df9-9779-005847c46048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nixtlats.utils import colab_badge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd11fae-3219-4ffc-b2de-a96542362d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nixtla/nixtla/blob/main/nbs/docs/how-to-guides/0_distributed_fcst_spark.ipynb)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "colab_badge('docs/how-to-guides/0_distributed_fcst_spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d702c-361f-4321-85d3-2b76fb7b4937",
   "metadata": {},
   "source": [
    "# Installation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2854f3c-7dc4-4615-9a85-7d7762fea647",
   "metadata": {},
   "source": [
    "As long as Spark is installed and configured, `TimeGPT` will be able to use it. If executing on a distributed Spark cluster, make use the `nixtlats` library is installed across all the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b89bd-6406-4f90-b545-2bd84a8ae62a",
   "metadata": {},
   "source": [
    "## Executing on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18574a5-76f8-4156-8264-9adae43e715d",
   "metadata": {},
   "source": [
    "To run the forecasts distributed on Spark, just pass in a Spark DataFrame instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c950c-6252-4696-8ea8-2e1bb865847d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9207c-29d1-4034-8d2e-223abc831cf1",
   "metadata": {},
   "source": [
    "Instantiate `NixtlaClient` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6004b-ebd0-4a3c-8c02-d5463c62f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtlats import NixtlaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2b1fb-74fb-4464-b57b-84c676cb997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    # defaults to os.environ.get(\"NIXTLA_API_KEY\")\n",
    "    api_key = 'my_api_key_provided_by_nixtla'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57091ace-6068-410d-82fc-2b0bd20d63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nixtla_client = NixtlaClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357aade9-ffaa-44c6-b9cb-48be7bda71f4",
   "metadata": {},
   "source": [
    "Use Spark as an engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7644af0-f628-46ea-8fb7-474ee2fca39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/01 03:34:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395152be-c5c7-46bb-85d8-da739d470834",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac9c73-6644-47be-884c-23a682844e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----+\n",
      "|unique_id|                 ds|    y|\n",
      "+---------+-------------------+-----+\n",
      "|       BE|2016-12-01 00:00:00| 72.0|\n",
      "|       BE|2016-12-01 01:00:00| 65.8|\n",
      "|       BE|2016-12-01 02:00:00|59.99|\n",
      "|       BE|2016-12-01 03:00:00|50.69|\n",
      "|       BE|2016-12-01 04:00:00|52.58|\n",
      "+---------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_df = 'https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short.csv'\n",
    "spark_df = spark.createDataFrame(pd.read_csv(url_df))\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305167a0-1984-4004-aea3-b97402832491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------------+\n",
      "|unique_id|                 ds|           TimeGPT|\n",
      "+---------+-------------------+------------------+\n",
      "|       FR|2016-12-31 00:00:00|62.130218505859375|\n",
      "|       FR|2016-12-31 01:00:00|56.890830993652344|\n",
      "|       FR|2016-12-31 02:00:00| 52.23155212402344|\n",
      "|       FR|2016-12-31 03:00:00| 48.88866424560547|\n",
      "|       FR|2016-12-31 04:00:00| 46.49836730957031|\n",
      "+---------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fcst_df = nixtla_client.forecast(spark_df, h=12)\n",
    "fcst_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb15c0-1e13-43fc-a3ea-8f75b87cd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55c5fa-ddd7-454a-aa43-697aa8c805d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# test different results for different models\n",
    "fcst_df_1 = fcst_df.toPandas()\n",
    "fcst_df_2 = nixtla_client.forecast(spark_df, h=12, model='timegpt-1-long-horizon')\n",
    "fcst_df_2 = fcst_df_2.toPandas()\n",
    "test_fail(\n",
    "    lambda: pd.testing.assert_frame_equal(fcst_df_1[['TimeGPT']], fcst_df_2[['TimeGPT']]),\n",
    "    contains='(column name=\"TimeGPT\") are different'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008fbf0-9bd2-4974-904b-bb8dc90876e6",
   "metadata": {},
   "source": [
    "### Forecast with exogenous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d281c8d-3a5c-4b3e-8468-7699ef44933b",
   "metadata": {},
   "source": [
    "Exogenous variables or external factors are crucial in time series forecasting as they provide additional information that might influence the prediction. These variables could include holiday markers, marketing spending, weather data, or any other external data that correlate with the time series data you are forecasting.\n",
    "\n",
    "For example, if you're forecasting ice cream sales, temperature data could serve as a useful exogenous variable. On hotter days, ice cream sales may increase.\n",
    "\n",
    "To incorporate exogenous variables in TimeGPT, you'll need to pair each point in your time series data with the corresponding external data.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d7fd4-5d69-4b6e-b065-efeba63f5911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----+----------+----------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|unique_id|                 ds|    y|Exogenous1|Exogenous2|day_0|day_1|day_2|day_3|day_4|day_5|day_6|\n",
      "+---------+-------------------+-----+----------+----------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|       BE|2016-12-01 00:00:00| 72.0|   61507.0|   71066.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|  0.0|\n",
      "|       BE|2016-12-01 01:00:00| 65.8|   59528.0|   67311.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|  0.0|\n",
      "|       BE|2016-12-01 02:00:00|59.99|   58812.0|   67470.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|  0.0|\n",
      "|       BE|2016-12-01 03:00:00|50.69|   57676.0|   64529.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|  0.0|\n",
      "|       BE|2016-12-01 04:00:00|52.58|   56804.0|   62773.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|  0.0|\n",
      "+---------+-------------------+-----+----------+----------+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-with-ex-vars.csv')\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172dc4a-66dd-47dd-a30d-228bc2f14317",
   "metadata": {},
   "source": [
    "To produce forecasts we have to add the future values of the exogenous variables. Let's read this dataset. In this case we want to predict 24 steps ahead, therefore each unique id will have 24 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8697301-e53b-446b-a965-6f57383d1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+----------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|unique_id|                 ds|Exogenous1|Exogenous2|day_0|day_1|day_2|day_3|day_4|day_5|day_6|\n",
      "+---------+-------------------+----------+----------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|       BE|2016-12-31 00:00:00|   64108.0|   70318.0|  0.0|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|\n",
      "|       BE|2016-12-31 01:00:00|   62492.0|   67898.0|  0.0|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|\n",
      "|       BE|2016-12-31 02:00:00|   61571.0|   68379.0|  0.0|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|\n",
      "|       BE|2016-12-31 03:00:00|   60381.0|   64972.0|  0.0|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|\n",
      "|       BE|2016-12-31 04:00:00|   60298.0|   62900.0|  0.0|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|\n",
      "+---------+-------------------+----------+----------+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future_ex_vars_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-future-ex-vars.csv')\n",
    "spark_future_ex_vars_df = spark.createDataFrame(future_ex_vars_df)\n",
    "spark_future_ex_vars_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec94e4-98c5-48ee-ad2f-d6996e82b758",
   "metadata": {},
   "source": [
    "Let's call the `forecast` method, adding this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c51169-3561-4d00-adba-fd6e49ab6c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Validating inputs...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Preprocessing dataframes...\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Inferred freq: H\n",
      "INFO:nixtlats.timegpt:Using the following exogenous variables: Exogenous1, Exogenous2, day_0, day_1, day_2, day_3, day_4, day_5, day_6\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.timegpt:Using the following exogenous variables: Exogenous1, Exogenous2, day_0, day_1, day_2, day_3, day_4, day_5, day_6\n",
      "INFO:nixtlats.timegpt:Calling Forecast Endpoint...\n",
      "[Stage 33:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|unique_id|                 ds|           TimeGPT|     TimeGPT-lo-90|     TimeGPT-lo-80|    TimeGPT-hi-80|    TimeGPT-hi-90|\n",
      "+---------+-------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|       FR|2016-12-31 00:00:00| 59.39155162090687| 54.47111514324573| 56.13039408916859|62.65270915264515|  64.311988098568|\n",
      "|       FR|2016-12-31 01:00:00|  60.1843929541434|56.167005220683926|56.778585672649264|63.59020023563754|64.20178068760288|\n",
      "|       FR|2016-12-31 02:00:00| 58.12912691907976| 53.55469655256365| 55.23512607984636|61.02312775831316|62.70355728559587|\n",
      "|       FR|2016-12-31 03:00:00|53.825965179940155| 46.31002742817014| 50.66449432422726|56.98743603565305|61.34190293171017|\n",
      "|       FR|2016-12-31 04:00:00|  47.6941769331486| 38.21902702317546| 42.94538668046305|52.44296718583414|57.16932684312174|\n",
      "+---------+-------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timegpt_fcst_ex_vars_df = nixtla_client.forecast(df=spark_df, X_df=spark_future_ex_vars_df, h=24, level=[80, 90])\n",
    "timegpt_fcst_ex_vars_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ef1e3-da4f-4949-bf12-6fd3727dfec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
