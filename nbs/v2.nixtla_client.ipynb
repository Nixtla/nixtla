{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7c905-5495-4fdb-9651-eadbe99a5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeef897-a7b9-4cdc-be12-de1bf56ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp v2.nixtla_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de9d09-99fd-4c2e-8737-06b1f3674800",
   "metadata": {},
   "source": [
    "# V2 Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee3a4c-f29b-400a-846c-91f1ee50da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import chain\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "from pydantic import NonNegativeInt, PositiveInt\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.feature_engineering import _add_time_features, time_features\n",
    "from utilsforecast.validation import validate_format, validate_freq\n",
    "\n",
    "from nixtla.core.api_error import ApiError\n",
    "from nixtla.core.http_client import HttpClient\n",
    "from nixtla.utils import _restrict_input_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6dcc9-14a2-45f5-944d-62f31bb931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857515e-c24a-4764-9680-6935927b195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_LOSS = Literal[\"default\", \"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]\n",
    "_MODEL = Literal[\"timegpt-1\", \"timegpt-1-long-horizon\"]\n",
    "\n",
    "_date_features_by_freq = {\n",
    "    # Daily frequencies\n",
    "    'B': ['year', 'month', 'day', 'weekday'],\n",
    "    'C': ['year', 'month', 'day', 'weekday'],\n",
    "    'D': ['year', 'month', 'day', 'weekday'],\n",
    "    # Weekly\n",
    "    'W': ['year', 'week', 'weekday'],\n",
    "    # Monthly\n",
    "    'M': ['year', 'month'],\n",
    "    'SM': ['year', 'month', 'day'],\n",
    "    'BM': ['year', 'month'],\n",
    "    'CBM': ['year', 'month'],\n",
    "    'MS': ['year', 'month'],\n",
    "    'SMS': ['year', 'month', 'day'],\n",
    "    'BMS': ['year', 'month'],\n",
    "    'CBMS': ['year', 'month'],\n",
    "    # Quarterly\n",
    "    'Q': ['year', 'quarter'],\n",
    "    'BQ': ['year', 'quarter'],\n",
    "    'QS': ['year', 'quarter'],\n",
    "    'BQS': ['year', 'quarter'],\n",
    "    # Yearly\n",
    "    'A': ['year'],\n",
    "    'Y': ['year'],\n",
    "    'BA': ['year'],\n",
    "    'BY': ['year'],\n",
    "    'AS': ['year'],\n",
    "    'YS': ['year'],\n",
    "    'BAS': ['year'],\n",
    "    'BYS': ['year'],\n",
    "    # Hourly\n",
    "    'BH': ['year', 'month', 'day', 'hour', 'weekday'],\n",
    "    'H': ['year', 'month', 'day', 'hour'],\n",
    "    # Minutely\n",
    "    'T': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    'min': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    # Secondly\n",
    "    'S': ['year', 'month', 'day', 'hour', 'minute', 'second'],\n",
    "    # Milliseconds\n",
    "    'L': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    'ms': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    # Microseconds\n",
    "    'U': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    'us': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    # Nanoseconds\n",
    "    'N': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a14e6b-d604-4e40-8df1-ebb485480261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NixtlaClient:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        timeout: int = 60,\n",
    "        max_retries: int = 6,\n",
    "    ):\n",
    "        if api_key is None:\n",
    "            api_key = os.environ['NIXTLA_API_KEY']\n",
    "        if base_url is None:\n",
    "            base_url = os.getenv('NIXTLA_BASE_URL', 'https://dashboard.nixtla.io/api')\n",
    "        self._client_kwargs = {\n",
    "            'base_url': base_url,\n",
    "            'headers': {'Authorization': f'Bearer {api_key}'},\n",
    "            'timeout': timeout,\n",
    "        }\n",
    "        self.max_retries = max_retries\n",
    "        self._model_params: Dict[Tuple[str, str], Tuple[int, int]] = {}\n",
    "\n",
    "    def _make_request(self, client: HttpClient, endpoint: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        resp = client.request(\n",
    "            method='post',\n",
    "            url=endpoint,\n",
    "            json=payload,\n",
    "            max_retries=self.max_retries,\n",
    "        )\n",
    "        resp_body = resp.json()\n",
    "        if resp.status_code != 200:\n",
    "            raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        if 'data' in resp_body:\n",
    "            resp_body = resp_body['data']\n",
    "        return resp_body\n",
    "\n",
    "    def _make_partitioned_requests(\n",
    "        self,\n",
    "        client: HttpClient,\n",
    "        endpoint: str,\n",
    "        payloads: List[Dict[str, Any]],\n",
    "    ) -> Dict[str, Any]:\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        num_partitions = len(payloads)\n",
    "        results = num_partitions * [None]\n",
    "        max_workers = min(10, num_partitions)\n",
    "        with ThreadPoolExecutor(max_workers) as executor:\n",
    "            future2pos = {\n",
    "                executor.submit(self._make_request, client, endpoint, payload): i\n",
    "                for i, payload in enumerate(payloads)\n",
    "            }\n",
    "            for future in tqdm(as_completed(future2pos), total=len(future2pos)):\n",
    "                pos = future2pos[future]\n",
    "                results[pos] = future.result()\n",
    "        resp = {\"mean\": list(chain.from_iterable(res[\"mean\"] for res in results))}\n",
    "        first_res = results[0]\n",
    "        if first_res[\"intervals\"] is None:\n",
    "            resp[\"intervals\"] = None\n",
    "        else:\n",
    "            resp[\"intervals\"] = {}\n",
    "            for k in first_res[\"intervals\"].keys():\n",
    "                resp[\"intervals\"][k] = list(\n",
    "                    chain.from_iterable(res[\"intervals\"][k] for res in results)\n",
    "                )\n",
    "        if first_res[\"weights_x\"] is None:\n",
    "            resp[\"weights_x\"] = None\n",
    "        else:\n",
    "            resp[\"weights_x\"] = [res[\"weights_x\"] for res in results]\n",
    "        return resp\n",
    "\n",
    "    def _get_model_params(self, model: str, freq: str) -> Tuple[int, int]:\n",
    "        key = (model, freq)\n",
    "        if key not in self._model_params:\n",
    "            logger.info('Querying model metadata...')\n",
    "            payload = {'model': model, 'freq': freq}\n",
    "            with httpx.Client(**self._client_kwargs) as httpx_client:\n",
    "                client = HttpClient(httpx_client=httpx_client)\n",
    "                params = self._make_request(client, '/model_params', payload)['detail']\n",
    "            self._model_params[key] = (params['input_size'], params['horizon'])\n",
    "        return self._model_params[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _standardize_freq(freq: str) -> str:\n",
    "        return freq.replace('mo', 'MS')\n",
    "\n",
    "    @staticmethod\n",
    "    def _tail(proc: ufp.ProcessedDF, n: int) -> ufp.ProcessedDF:\n",
    "        n_series = proc.indptr.size - 1\n",
    "        new_sizes = np.minimum(np.diff(proc.indptr), n)\n",
    "        new_indptr = np.append(0, new_sizes.cumsum())\n",
    "        new_data = np.empty_like(proc.data, shape=(new_indptr[-1], proc.data.shape[1]))\n",
    "        for i in range(n_series):\n",
    "            new_data[new_indptr[i] : new_indptr[i + 1]] = proc.data[\n",
    "                proc.indptr[i + 1] - new_sizes[i] : proc.indptr[i + 1]\n",
    "            ]\n",
    "        return ufp.ProcessedDF(\n",
    "            uids=proc.uids,\n",
    "            last_times=proc.last_times,\n",
    "            data=new_data,\n",
    "            indptr=new_indptr,\n",
    "            sort_idxs=None,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _partition_series(\n",
    "        payload: Dict[str, Any], n_part: int, h: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        parts = []\n",
    "        series = payload.pop(\"series\")\n",
    "        n_series = len(series[\"sizes\"])\n",
    "        n_part = min(n_part, n_series)\n",
    "        series_per_part = math.ceil(n_series / n_part)\n",
    "        prev_size = 0\n",
    "        for i in range(0, n_series, series_per_part):\n",
    "            sizes = series[\"sizes\"][i : i + series_per_part]\n",
    "            curr_size = sum(sizes)\n",
    "            part_idxs = slice(prev_size, prev_size + curr_size)\n",
    "            prev_size += curr_size\n",
    "            part_series = {\n",
    "                \"y\": series[\"y\"][part_idxs],\n",
    "                \"sizes\": sizes,\n",
    "            }\n",
    "            if series[\"X\"] is None:\n",
    "                part_series[\"X\"] = None\n",
    "                part_series[\"X_future\"] = None\n",
    "            else:\n",
    "                part_series[\"X\"] = [x[part_idxs] for x in series[\"X\"]]\n",
    "                part_series[\"X_future\"] = [\n",
    "                    x[i * h : (i + series_per_part) * h] for x in series[\"X_future\"]\n",
    "                ]\n",
    "            parts.append({\"series\": part_series, **payload})\n",
    "        return parts\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_add_date_features(\n",
    "        df: DataFrame,\n",
    "        X_df: Optional[DataFrame],\n",
    "        features: Union[bool, List[Union[str, Callable]]],\n",
    "        one_hot: Union[bool, List[str]],\n",
    "        freq: str,\n",
    "        h: int,\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "    ) -> Tuple[DataFrame, Optional[DataFrame]]:\n",
    "        if not features:\n",
    "            return df, X_df\n",
    "        if isinstance(features, list):\n",
    "            date_features = features\n",
    "        else:\n",
    "            date_features = _date_features_by_freq.get(freq, [])\n",
    "            if not date_features:\n",
    "                warnings.warn(\n",
    "                    f'Non default date features for {freq} '\n",
    "                    'please provide a list of date features'\n",
    "                )\n",
    "        # add features\n",
    "        if X_df is None:\n",
    "            df, X_df = time_features(\n",
    "                df=df,\n",
    "                freq=freq,\n",
    "                features=date_features,\n",
    "                h=h,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "            )\n",
    "        else:\n",
    "            df = _add_time_features(df, features=date_features, time_col=time_col)\n",
    "            X_df = _add_time_features(X_df, features=date_features,time_col=time_col)\n",
    "        # one hot\n",
    "        if isinstance(one_hot, list):\n",
    "            features_one_hot = one_hot\n",
    "        elif one_hot:\n",
    "            features_one_hot = [f for f in date_features if not callable(f)]\n",
    "        else:\n",
    "            features_one_hot = []\n",
    "        if features_one_hot:\n",
    "            X_df = ufp.assign_columns(X_df, target_col, 0)\n",
    "            full_df = ufp.vertical_concat([df, X_df])\n",
    "            if isinstance(full_df, pd.DataFrame):\n",
    "                full_df = pd.get_dummies(\n",
    "                    full_df, columns=features_one_hot, dtype='float32'\n",
    "                )\n",
    "            else:\n",
    "                full_df = full_df.to_dummies(columns=features_one_hot)\n",
    "            df = ufp.take_rows(full_df, slice(0, df.shape[0]))\n",
    "            X_df = ufp.take_rows(full_df, slice(df.shape[0], full_df.shape[0]))\n",
    "            X_df = ufp.drop_columns(X_df, target_col)\n",
    "        return df, X_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_exog(\n",
    "        df: DataFrame,\n",
    "        X_df: Optional[DataFrame],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "    ) -> Optional[List[str]]:\n",
    "        exogs_df = [c for c in df.columns if c not in (id_col, time_col, target_col)]\n",
    "        if X_df is not None:\n",
    "            exogs_X = [c for c in X_df.columns if c not in (id_col, time_col)]\n",
    "            missing_df = set(exogs_X) - set(exogs_df)\n",
    "            if missing_df:\n",
    "                raise ValueError(\n",
    "                    'The following exogenous features are present in `X_df` '\n",
    "                    f'but not in `df`: {missing_df}.'\n",
    "                )\n",
    "            missing_X_df = set(exogs_df) - set(exogs_X)\n",
    "            if missing_X_df:\n",
    "                raise ValueError(\n",
    "                    'The following exogenous features are present in `df` '\n",
    "                    f'but not in `X_df`: {missing_X_df}.'\n",
    "                )\n",
    "            if exogs_df != exogs_X:\n",
    "                # rearrange columns if necessary\n",
    "                X_df = X_df[[id_col, time_col, *exogs_df]]\n",
    "            x_cols: Optional[List[str]] = exogs_df\n",
    "        else:\n",
    "            if exogs_df:\n",
    "                warnings.warn(\n",
    "                    f'`df` contains the following exogenous features: {exogs_df}, '\n",
    "                    'but `X_df` was not provided. They will be ignored.'\n",
    "                )\n",
    "                df = df[[id_col, time_col, target_col]]\n",
    "            x_cols = None\n",
    "            X_df = None\n",
    "        return df, X_df, x_cols\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_input_size(\n",
    "        df: DataFrame,\n",
    "        id_col: str,\n",
    "        model_input_size: int,\n",
    "        model_horizon: int,\n",
    "    ) -> None:\n",
    "        min_size = ufp.counts_by_id(df, id_col)['counts'].min()\n",
    "        if min_size < model_input_size + model_horizon:\n",
    "            raise ValueError(\n",
    "                'Your time series data is too short '\n",
    "                'Please make sure that your each serie contains '\n",
    "                f'at least {model_input_size + model_horizon} observations.'\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_level_and_quantiles(\n",
    "        level: Optional[List[Union[int, float]]], \n",
    "        quantiles: Optional[List[float]],\n",
    "    ) -> Tuple[List[Union[int, float]], Optional[List[float]]]:\n",
    "        if level is not None and quantiles is not None:\n",
    "            raise ValueError(\n",
    "                \"You should provide `level` or `quantiles`, but not both.\"\n",
    "            )\n",
    "        if quantiles is None:\n",
    "            return level, quantiles\n",
    "        # we recover level from quantiles\n",
    "        if not all(0 < q < 1 for q in quantiles):\n",
    "            raise ValueError(\"`quantiles` should be floats between 0 and 1.\")\n",
    "        level = [abs(int(100 - 200 * q)) for q in quantiles]\n",
    "        return level, quantiles\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_convert_level_to_quantiles(\n",
    "        df: DataFrame,\n",
    "        quantiles: Optional[List[float]],\n",
    "    ) -> DataFrame:\n",
    "        if quantiles is None:\n",
    "            return df\n",
    "        out_cols = [c for c in df.columns if '-lo-' not in col and '-hi-' not in col]\n",
    "        df = ufp.copy_if_pandas(df, deep=False)\n",
    "        for q in sorted(quantiles):\n",
    "            if q == 0.5:\n",
    "                col = 'TimeGPT'\n",
    "            else:\n",
    "                lv = int(100 - 200 * q)\n",
    "                hi_or_lo = 'lo' if lv > 0 else 'hi'\n",
    "                lv = abs(lv)\n",
    "                col = f\"TimeGPT-{hi_or_lo}-{lv}\"\n",
    "            q_col = f\"TimeGPT-q-{int(q * 100)}\"\n",
    "            df[q_col] = df[col]\n",
    "            out_cols.append(q_col)\n",
    "        return df[out_cols]\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        h: PositiveInt,\n",
    "        freq: str,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        quantiles: Optional[List[float]] = None,\n",
    "        finetune_steps: NonNegativeInt = 0,\n",
    "        finetune_loss: _LOSS = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        add_history: bool = False,\n",
    "        date_features: Union[bool, List[Union[str, Callable]]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        model: _MODEL = 'timegpt-1',\n",
    "        num_partitions: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        quantiles : List[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the\n",
    "            new data.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before \n",
    "            sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        logger.info('Validating inputs...')\n",
    "        validate_format(df=df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "        if ufp.is_nan_or_none(df[target_col]).any():\n",
    "            raise ValueError(f'Target column ({target_col}) cannot contain missing values.')\n",
    "        validate_freq(times=df[time_col], freq=freq)\n",
    "        df, X_df, x_cols = self._validate_exog(\n",
    "            df, X_df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        level, quantiles = self._prepare_level_and_quantiles(level, quantiles)\n",
    "        standard_freq = self._standardize_freq(freq)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        if finetune_steps > 0 or level is not None:\n",
    "            self._validate_input_size(df, id_col, model_input_size, model_horizon)\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        df, X_df = self._maybe_add_date_features(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            features=date_features,\n",
    "            one_hot=date_features_to_one_hot,\n",
    "            freq=standard_freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        processed = ufp.process_df(\n",
    "            df=df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        if X_df is not None:\n",
    "            processed_X = ufp.process_df(\n",
    "                df=X_df, id_col=id_col, time_col=time_col, target_col=None,\n",
    "            )\n",
    "            X_future = processed_X.data.T.tolist()\n",
    "        else:\n",
    "            X_future = None\n",
    "\n",
    "        if h > model_horizon:\n",
    "            logger.warning(\n",
    "                'The specified horizon \"h\" exceeds the model horizon. '\n",
    "                'This may lead to less accurate forecasts. '\n",
    "                'Please consider using a smaller horizon.'  \n",
    "            )\n",
    "        restrict_input = finetune_steps == 0 and X_df is None and not add_history\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            processed = self._tail(processed, new_input_size)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T.tolist()\n",
    "        else:\n",
    "            X = None\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0].tolist(),\n",
    "                'sizes': np.diff(processed.indptr).tolist(),\n",
    "                'X': X,\n",
    "                'X_future': X_future,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_loss': finetune_loss,\n",
    "        }\n",
    "\n",
    "        logger.info('Calling Forecast Endpoint...')\n",
    "        with httpx.Client(**self._client_kwargs) as httpx_client:\n",
    "            client = HttpClient(httpx_client=httpx_client)\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request(client, '/v2/forecast', payload)\n",
    "            else:\n",
    "                payloads = self._partition_series(payload, num_partitions, h)\n",
    "                resp = self._make_partitioned_requests(client, '/v2/forecast', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        out = ufp.make_future_dataframe(\n",
    "            uids=processed.uids,\n",
    "            last_times=type(processed.uids)(processed.last_times),\n",
    "            freq=freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        if resp['intervals'] is not None:\n",
    "            intervals_df = type(df)(\n",
    "                {\n",
    "                    f'TimeGPT-{k}': v for k, v in resp['intervals'].items()\n",
    "                }\n",
    "            )\n",
    "            intervals_df = self._maybe_convert_level_to_quantiles(\n",
    "                intervals_df, quantiles\n",
    "            )\n",
    "            out = ufp.horizontal_concat([out, intervals_df])\n",
    "        if resp['weights_x'] is not None:\n",
    "            self.weights_x = type(df)({\n",
    "                'features': x_cols,\n",
    "                'weights': resp['weights_x'],\n",
    "            })\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bc225-6ae3-4305-b002-ade5abbc145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from fastcore.test import test_warns\n",
    "from utilsforecast.data import generate_series\n",
    "\n",
    "from nixtla import NixtlaClient as NixtlaV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e4819-4a57-49b9-8ef0-38497d9f8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_series(\n",
    "    4, min_length=40, max_length=200, n_static_features=2, static_as_categorical=False\n",
    ")\n",
    "horizon = 20\n",
    "level = [80, 97.5]\n",
    "series['unique_id'] = series['unique_id'].astype('int64')\n",
    "valid = series.groupby('unique_id', observed=True).tail(horizon)\n",
    "X_df = valid.drop(columns='y')\n",
    "train = series.drop(valid.index)\n",
    "train_pl = pl.from_pandas(train)\n",
    "X_df_pl = pl.from_pandas(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ff47f-5bbe-4219-b90f-794acc9f9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = NixtlaClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef60dc2-34d0-4acf-b64f-d828daf027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resv2_pd = client.forecast(df=train, X_df=X_df, h=horizon, freq=\"D\", level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc288a9-66b5-4746-883f-158bfb71beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_warns(lambda: client.forecast(df=train, h=1, freq=\"D\", level=level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd08c9-2b2c-46ec-b77a-ed5b8dbac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resv2_pl = client.forecast(df=train_pl, X_df=X_df_pl, h=horizon, freq=\"1d\", level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(\n",
    "    client.forecast(df=series[['unique_id', 'ds', 'y']], h=horizon, freq=\"D\")['TimeGPT'].to_numpy(),\n",
    "    client.forecast(df=series[['unique_id', 'ds', 'y']], h=horizon, freq=\"D\", num_partitions=4)['TimeGPT'].to_numpy(),\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bbbc61-57d9-4c4e-9d2f-02b31c17a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(resv2_pd, resv2_pl.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e5db3-afde-4d55-80b5-18336e217623",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_client = NixtlaV1()\n",
    "resv1 = v1_client.forecast(df=train, X_df=X_df, h=horizon, freq=\"D\", level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d303843-4145-443f-866b-bd9bc61c4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(resv2_pd.astype({'ds': 'str'}), resv1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
