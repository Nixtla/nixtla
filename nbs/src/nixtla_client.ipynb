{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nixtla_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datetime\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from collections.abc import Sequence\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    overload,\n",
    ")\n",
    "\n",
    "import annotated_types\n",
    "import httpcore\n",
    "import httpx\n",
    "import numpy as np\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "import zstandard as zstd\n",
    "from pydantic import BaseModel\n",
    "from tenacity import (\n",
    "    RetryCallState,\n",
    "    retry,\n",
    "    retry_if_exception,\n",
    "    stop_after_attempt,\n",
    "    stop_after_delay,\n",
    "    wait_fixed,\n",
    ")\n",
    "from utilsforecast.compat import DFType, DataFrame, pl_DataFrame\n",
    "from utilsforecast.feature_engineering import _add_time_features, time_features\n",
    "from utilsforecast.preprocessing import id_time_grid\n",
    "from utilsforecast.validation import ensure_time_dtype, validate_format\n",
    "if TYPE_CHECKING:\n",
    "    try:\n",
    "        from fugue import AnyDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        import plotly\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        import triad\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from polars import DataFrame as PolarsDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from dask.dataframe import DataFrame as DaskDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from ray.data import Dataset as RayDataset\n",
    "    except ModuleNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "AnyDFType = TypeVar(\n",
    "    \"AnyDFType\",\n",
    "    \"DaskDataFrame\",\n",
    "    pd.DataFrame,\n",
    "    \"PolarsDataFrame\",\n",
    "    \"RayDataset\",\n",
    "    \"SparkDataFrame\",\n",
    ")\n",
    "DistributedDFType = TypeVar(\n",
    "    \"DistributedDFType\",\n",
    "    \"DaskDataFrame\",\n",
    "    \"RayDataset\",\n",
    "    \"SparkDataFrame\",\n",
    ")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('httpx').setLevel(logging.ERROR)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import time\n",
    "import uuid\n",
    "from contextlib import contextmanager\n",
    "from itertools import product\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastcore.test import test_eq, test_fail\n",
    "from utilsforecast.data import generate_series\n",
    "from utilsforecast.evaluation import evaluate\n",
    "from utilsforecast.feature_engineering import fourier\n",
    "from utilsforecast.losses import rmse\n",
    "\n",
    "from nixtla.date_features import SpecialDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_PositiveInt = Annotated[int, annotated_types.Gt(0)]\n",
    "_NonNegativeInt = Annotated[int, annotated_types.Ge(0)]\n",
    "_Loss = Literal[\"default\", \"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]\n",
    "_Model = Literal[\"azureai\", \"timegpt-1\", \"timegpt-1-long-horizon\"]\n",
    "_FinetuneDepth = Literal[1, 2, 3, 4, 5]\n",
    "_Freq = Union[str, int, pd.offsets.BaseOffset]\n",
    "_FreqType = TypeVar(\"_FreqType\", str, int, pd.offsets.BaseOffset)\n",
    "_ThresholdMethod = Literal[\"univariate\", \"multivariate\"]\n",
    "\n",
    "class FinetunedModel(BaseModel, extra='allow'):  # type: ignore\n",
    "    id: str\n",
    "    created_at: datetime.datetime\n",
    "    created_by: str\n",
    "    base_model_id: str\n",
    "    steps: int\n",
    "    depth: int\n",
    "    loss: _Loss\n",
    "    model: _Model\n",
    "    freq: str\n",
    "\n",
    "_date_features_by_freq = {\n",
    "    # Daily frequencies\n",
    "    'B': ['year', 'month', 'day', 'weekday'],\n",
    "    'C': ['year', 'month', 'day', 'weekday'],\n",
    "    'D': ['year', 'month', 'day', 'weekday'],\n",
    "    # Weekly\n",
    "    'W': ['year', 'week', 'weekday'],\n",
    "    # Monthly\n",
    "    'M': ['year', 'month'],\n",
    "    'SM': ['year', 'month', 'day'],\n",
    "    'BM': ['year', 'month'],\n",
    "    'CBM': ['year', 'month'],\n",
    "    'MS': ['year', 'month'],\n",
    "    'SMS': ['year', 'month', 'day'],\n",
    "    'BMS': ['year', 'month'],\n",
    "    'CBMS': ['year', 'month'],\n",
    "    # Quarterly\n",
    "    'Q': ['year', 'quarter'],\n",
    "    'BQ': ['year', 'quarter'],\n",
    "    'QS': ['year', 'quarter'],\n",
    "    'BQS': ['year', 'quarter'],\n",
    "    # Yearly\n",
    "    'A': ['year'],\n",
    "    'Y': ['year'],\n",
    "    'BA': ['year'],\n",
    "    'BY': ['year'],\n",
    "    'AS': ['year'],\n",
    "    'YS': ['year'],\n",
    "    'BAS': ['year'],\n",
    "    'BYS': ['year'],\n",
    "    # Hourly\n",
    "    'BH': ['year', 'month', 'day', 'hour', 'weekday'],\n",
    "    'H': ['year', 'month', 'day', 'hour'],\n",
    "    # Minutely\n",
    "    'T': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    'min': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    # Secondly\n",
    "    'S': ['year', 'month', 'day', 'hour', 'minute', 'second'],\n",
    "    # Milliseconds\n",
    "    'L': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    'ms': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    # Microseconds\n",
    "    'U': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    'us': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    # Nanoseconds\n",
    "    'N': []\n",
    "}\n",
    "\n",
    "def _retry_strategy(max_retries: int, retry_interval: int, max_wait_time: int):\n",
    "    def should_retry(exc: Exception) -> bool:\n",
    "        retriable_exceptions = (\n",
    "            ConnectionResetError,\n",
    "            httpcore.ConnectError,\n",
    "            httpcore.RemoteProtocolError,\n",
    "            httpx.ConnectTimeout,\n",
    "            httpx.ReadError,\n",
    "            httpx.RemoteProtocolError,\n",
    "            httpx.ReadTimeout,\n",
    "            httpx.PoolTimeout,\n",
    "            httpx.WriteError,\n",
    "            httpx.WriteTimeout,\n",
    "        )\n",
    "        retriable_codes = [408, 409, 429, 502, 503, 504]\n",
    "        return (\n",
    "            isinstance(exc, retriable_exceptions)\n",
    "            or (isinstance(exc, ApiError) and exc.status_code in retriable_codes)\n",
    "        )\n",
    "\n",
    "    def after_retry(retry_state: RetryCallState) -> None:\n",
    "        error = retry_state.outcome.exception()\n",
    "        logger.error(\n",
    "            f\"Attempt {retry_state.attempt_number} failed with error: {error}\"\n",
    "        )\n",
    "\n",
    "    return retry(\n",
    "        retry=retry_if_exception(should_retry),\n",
    "        wait=wait_fixed(retry_interval),\n",
    "        after=after_retry,\n",
    "        stop=stop_after_attempt(max_retries) | stop_after_delay(max_wait_time),\n",
    "        reraise=True,\n",
    "    )\n",
    "\n",
    "def _maybe_infer_freq(\n",
    "    df: DataFrame,\n",
    "    freq: Optional[_FreqType],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    ") -> _FreqType:\n",
    "    if freq is not None:\n",
    "        return freq\n",
    "    if isinstance(df, pl_DataFrame):\n",
    "        raise ValueError(\n",
    "            \"Cannot infer frequency for a polars DataFrame, please set the \"\n",
    "            \"`freq` argument to a valid polars offset.\\nYou can find them at \"\n",
    "            \"https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.Expr.dt.offset_by.html\"\n",
    "        )\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    sizes = df[id_col].value_counts(sort=True)\n",
    "    times = df.loc[df[id_col] == sizes.index[0], time_col].sort_values()\n",
    "    if times.dt.tz is not None:\n",
    "        times = times.dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    inferred_freq = pd.infer_freq(times.values)\n",
    "    if inferred_freq is None:\n",
    "        raise RuntimeError(\n",
    "            'Could not infer the frequency of the time column. This could be due '\n",
    "            'to inconsistent intervals. Please check your data for missing, '\n",
    "            'duplicated or irregular timestamps'\n",
    "        )\n",
    "    logger.info(f'Inferred freq: {inferred_freq}')\n",
    "    return inferred_freq\n",
    "\n",
    "def _standardize_freq(freq: _Freq, processed: ufp.ProcessedDF) -> str:\n",
    "    if isinstance(freq, str):\n",
    "        # polars uses 'mo' for months, all other strings are compatible with pandas\n",
    "        freq = freq.replace('mo', 'MS')\n",
    "    elif isinstance(freq, pd.offsets.BaseOffset):\n",
    "        freq = freq.freqstr\n",
    "    elif isinstance(freq, int):\n",
    "        freq = 'MS'\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"`freq` must be a string, int or pandas offset, got {type(freq).__name__}\"\n",
    "        )\n",
    "    return freq\n",
    "\n",
    "def _array_tails(\n",
    "    x: np.ndarray,\n",
    "    indptr: np.ndarray,\n",
    "    out_sizes: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    if (out_sizes > np.diff(indptr)).any():\n",
    "        raise ValueError('out_sizes must be at most the original sizes.')\n",
    "    idxs = np.hstack(\n",
    "        [\n",
    "            np.arange(end - size, end)\n",
    "            for end, size in zip(indptr[1:], out_sizes)\n",
    "        ]\n",
    "    )\n",
    "    return x[idxs]\n",
    "\n",
    "def _tail(proc: ufp.ProcessedDF, n: int) -> ufp.ProcessedDF:\n",
    "    new_sizes = np.minimum(np.diff(proc.indptr), n)\n",
    "    new_indptr = np.append(0, new_sizes.cumsum())\n",
    "    new_data = _array_tails(proc.data, proc.indptr, new_sizes)\n",
    "    return ufp.ProcessedDF(\n",
    "        uids=proc.uids,\n",
    "        last_times=proc.last_times,\n",
    "        data=new_data,\n",
    "        indptr=new_indptr,\n",
    "        sort_idxs=None,\n",
    "    )\n",
    "\n",
    "def _partition_series(\n",
    "    payload: dict[str, Any], n_part: int, h: int\n",
    ") -> list[dict[str, Any]]:\n",
    "    parts = []\n",
    "    series = payload.pop(\"series\")\n",
    "    n_series = len(series[\"sizes\"])\n",
    "    n_part = min(n_part, n_series)\n",
    "    series_per_part = math.ceil(n_series / n_part)\n",
    "    prev_size = 0\n",
    "    for i in range(0, n_series, series_per_part):\n",
    "        sizes = series[\"sizes\"][i : i + series_per_part]\n",
    "        curr_size = sum(sizes)\n",
    "        part_idxs = slice(prev_size, prev_size + curr_size)\n",
    "        prev_size += curr_size\n",
    "        part_series = {\n",
    "            \"y\": series[\"y\"][part_idxs],\n",
    "            \"sizes\": sizes,\n",
    "        }\n",
    "        if series[\"X\"] is None:\n",
    "            part_series[\"X\"] = None\n",
    "            if h > 0:\n",
    "                part_series[\"X_future\"] = None\n",
    "        else:\n",
    "            part_series[\"X\"] = [x[part_idxs] for x in series[\"X\"]]\n",
    "            if h > 0:\n",
    "                part_series[\"X_future\"] = [\n",
    "                    x[i * h : (i + series_per_part) * h] for x in series[\"X_future\"]\n",
    "                ]\n",
    "        parts.append({\"series\": part_series, **payload})\n",
    "    return parts\n",
    "\n",
    "def _maybe_add_date_features(\n",
    "    df: DFType,\n",
    "    X_df: Optional[DFType],\n",
    "    features: Union[bool, Sequence[Union[str, Callable]]],\n",
    "    one_hot: Union[bool, list[str]],\n",
    "    freq: _Freq,\n",
    "    h: int,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    ") -> tuple[DFType, Optional[DFType]]:\n",
    "    if not features or not isinstance(freq, str):\n",
    "        return df, X_df\n",
    "    if isinstance(features, list):\n",
    "        date_features: Sequence[Union[str, Callable]] = features\n",
    "    else:\n",
    "        date_features = _date_features_by_freq.get(freq, [])\n",
    "        if not date_features:\n",
    "            warnings.warn(\n",
    "                f'Non default date features for {freq} '\n",
    "                'please provide a list of date features'\n",
    "            )\n",
    "    # add features\n",
    "    if X_df is None:\n",
    "        df, X_df = time_features(\n",
    "            df=df,\n",
    "            freq=freq,\n",
    "            features=date_features,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "    else:\n",
    "        df = _add_time_features(df, features=date_features, time_col=time_col)\n",
    "        X_df = _add_time_features(X_df, features=date_features,time_col=time_col)\n",
    "    # one hot\n",
    "    if isinstance(one_hot, list):\n",
    "        features_one_hot = one_hot\n",
    "    elif one_hot:\n",
    "        features_one_hot = [f for f in date_features if not callable(f)]\n",
    "    else:\n",
    "        features_one_hot = []\n",
    "    if features_one_hot:\n",
    "        X_df = ufp.assign_columns(X_df, target_col, 0)\n",
    "        full_df = ufp.vertical_concat([df, X_df])\n",
    "        if isinstance(full_df, pd.DataFrame):\n",
    "            full_df = pd.get_dummies(\n",
    "                full_df, columns=features_one_hot, dtype='float32'\n",
    "            )\n",
    "        else:\n",
    "            full_df = full_df.to_dummies(columns=features_one_hot)\n",
    "        df = ufp.take_rows(full_df, slice(0, df.shape[0]))\n",
    "        X_df = ufp.take_rows(full_df, slice(df.shape[0], full_df.shape[0]))\n",
    "        X_df = ufp.drop_columns(X_df, target_col)\n",
    "        X_df = ufp.drop_index_if_pandas(X_df)\n",
    "    if h == 0:\n",
    "        # time_features returns an empty df, we use it as None here\n",
    "        X_df = None\n",
    "    return df, X_df\n",
    "\n",
    "def _validate_exog(\n",
    "    df: DFType,\n",
    "    X_df: Optional[DFType],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    hist_exog: Optional[list[str]],\n",
    ") -> tuple[DFType, Optional[DFType]]:\n",
    "    base_cols = {id_col, time_col, target_col}\n",
    "    exogs = [c for c in df.columns if c not in base_cols]\n",
    "    if hist_exog is None:\n",
    "        hist_exog = []\n",
    "    if X_df is None:\n",
    "        # all exogs must be historic\n",
    "        ignored_exogs = [c for c in exogs if c not in hist_exog]\n",
    "        if ignored_exogs:\n",
    "            warnings.warn(\n",
    "                f\"`df` contains the following exogenous features: {ignored_exogs}, \"\n",
    "                \"but `X_df` was not provided and they were not declared in `hist_exog_list`. \"\n",
    "                \"They will be ignored.\"\n",
    "            )\n",
    "        exogs = [c for c in exogs if c in hist_exog]\n",
    "        df = df[[id_col, time_col, target_col, *exogs]]\n",
    "        return df, None\n",
    "\n",
    "    # exogs in df that weren't declared as historic nor future\n",
    "    futr_exog = [c for c in X_df.columns if c not in base_cols]\n",
    "    declared_exogs = {*hist_exog, *futr_exog}\n",
    "    ignored_exogs = [c for c in exogs if c not in declared_exogs]\n",
    "    if ignored_exogs:\n",
    "        warnings.warn(\n",
    "            f\"`df` contains the following exogenous features: {ignored_exogs}, \"\n",
    "            \"but they were not found in `X_df` nor declared in `hist_exog_list`. \"\n",
    "            \"They will be ignored.\"\n",
    "        )\n",
    "\n",
    "    # future exogenous are provided in X_df that are not in df\n",
    "    missing_futr = set(futr_exog) - set(exogs)\n",
    "    if missing_futr:\n",
    "        raise ValueError(\n",
    "            \"The following exogenous features are present in `X_df` \"\n",
    "            f\"but not in `df`: {missing_futr}.\"\n",
    "        )\n",
    "\n",
    "    # features are provided through X_df but declared as historic\n",
    "    futr_and_hist = set(futr_exog) & set(hist_exog)\n",
    "    if futr_and_hist:\n",
    "        warnings.warn(\n",
    "            \"The following features were declared as historic but found in `X_df`: \"\n",
    "            f\"{futr_and_hist}, they will be considered as historic.\"\n",
    "        )\n",
    "        futr_exog = [f for f in futr_exog if f not in hist_exog]\n",
    "\n",
    "    # Make sure df and X_df are in right order\n",
    "    df = df[[id_col, time_col, target_col, *futr_exog, *hist_exog]]\n",
    "    X_df = X_df[[id_col, time_col, *futr_exog]]\n",
    "\n",
    "    return df, X_df\n",
    "\n",
    "def _validate_input_size(\n",
    "    processed: ufp.ProcessedDF,\n",
    "    model_input_size: int,\n",
    "    model_horizon: int,\n",
    ") -> None:\n",
    "    min_size = np.diff(processed.indptr).min().item()\n",
    "    if min_size < model_input_size + model_horizon:\n",
    "        raise ValueError(\n",
    "            'Some series are too short. '\n",
    "            'Please make sure that each serie contains '\n",
    "            f'at least {model_input_size + model_horizon} observations.'\n",
    "        )\n",
    "\n",
    "def _prepare_level_and_quantiles(\n",
    "    level: Optional[list[Union[int, float]]], \n",
    "    quantiles: Optional[list[float]],\n",
    ") -> tuple[Optional[list[Union[int, float]]], Optional[list[float]]]:\n",
    "    if level is not None and quantiles is not None:\n",
    "        raise ValueError(\n",
    "            \"You should provide `level` or `quantiles`, but not both.\"\n",
    "        )\n",
    "    if quantiles is None:\n",
    "        return level, quantiles\n",
    "    # we recover level from quantiles\n",
    "    if not all(0 < q < 1 for q in quantiles):\n",
    "        raise ValueError(\"`quantiles` should be floats between 0 and 1.\")\n",
    "    level = [abs(int(100 - 200 * q)) for q in quantiles]\n",
    "    return level, quantiles\n",
    "\n",
    "def _maybe_convert_level_to_quantiles(\n",
    "    df: DFType,\n",
    "    quantiles: Optional[list[float]],\n",
    ") -> DFType:\n",
    "    if quantiles is None:\n",
    "        return df\n",
    "    out_cols = [c for c in df.columns if '-lo-' not in c and '-hi-' not in c]\n",
    "    df = ufp.copy_if_pandas(df, deep=False)\n",
    "    for q in sorted(quantiles):\n",
    "        if q == 0.5:\n",
    "            col = 'TimeGPT'\n",
    "        else:\n",
    "            lv = int(100 - 200 * q)\n",
    "            hi_or_lo = 'lo' if lv > 0 else 'hi'\n",
    "            lv = abs(lv)\n",
    "            col = f\"TimeGPT-{hi_or_lo}-{lv}\"\n",
    "        q_col = f\"TimeGPT-q-{int(q * 100)}\"\n",
    "        df = ufp.assign_columns(df, q_col, df[col])\n",
    "        out_cols.append(q_col)\n",
    "    return df[out_cols]\n",
    "\n",
    "def _preprocess(\n",
    "    df: DFType,\n",
    "    X_df: Optional[DFType],\n",
    "    h: int,\n",
    "    freq: str,\n",
    "    date_features: Union[bool, Sequence[Union[str, Callable]]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    ") -> tuple[ufp.ProcessedDF, Optional[DFType], list[str], Optional[list[str]]]:\n",
    "    df, X_df = _maybe_add_date_features(\n",
    "        df=df,\n",
    "        X_df=X_df,\n",
    "        features=date_features,\n",
    "        one_hot=date_features_to_one_hot,\n",
    "        freq=freq,\n",
    "        h=h,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "    )\n",
    "    processed = ufp.process_df(\n",
    "        df=df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "    )\n",
    "    if X_df is not None and X_df.shape[1] > 2:\n",
    "        X_df = ensure_time_dtype(X_df, time_col=time_col)\n",
    "        processed_X = ufp.process_df(\n",
    "            df=X_df, id_col=id_col, time_col=time_col, target_col=None,\n",
    "        )\n",
    "        X_future = processed_X.data.T\n",
    "        futr_cols = [c for c in X_df.columns if c not in (id_col, time_col)]\n",
    "    else:\n",
    "        X_future = None\n",
    "        futr_cols = None\n",
    "    x_cols = [c for c in df.columns if c not in (id_col, time_col, target_col)]\n",
    "    return processed, X_future, x_cols, futr_cols\n",
    "\n",
    "def _forecast_payload_to_in_sample(payload):\n",
    "    in_sample_payload = {\n",
    "        k: v\n",
    "        for k, v in payload.items()\n",
    "        if k not in ('h', 'finetune_steps', 'finetune_loss')\n",
    "    }\n",
    "    del in_sample_payload['series']['X_future']\n",
    "    return in_sample_payload\n",
    "\n",
    "def _maybe_add_intervals(\n",
    "    df: DFType,\n",
    "    intervals: Optional[dict[str, list[float]]],\n",
    ") -> DFType:\n",
    "    if intervals is None:\n",
    "        return df\n",
    "    first_key = next(iter(intervals), None)\n",
    "    if first_key is None or intervals[first_key] is None:\n",
    "        return df\n",
    "    intervals_df = type(df)(\n",
    "        {f'TimeGPT-{k}': intervals[k] for k in sorted(intervals.keys())}\n",
    "    )\n",
    "    return ufp.horizontal_concat([df, intervals_df])\n",
    "\n",
    "def _maybe_drop_id(df: DFType, id_col: str, drop: bool) -> DFType:\n",
    "    if drop:\n",
    "        df = ufp.drop_columns(df, id_col)\n",
    "    return df\n",
    "\n",
    "def _parse_in_sample_output(\n",
    "    in_sample_output: dict[str, Union[list[float], dict[str, list[float]]]],\n",
    "    df: DataFrame,\n",
    "    processed: ufp.ProcessedDF,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    ") -> DataFrame:\n",
    "    times = df[time_col].to_numpy()\n",
    "    targets = df[target_col].to_numpy()\n",
    "    if processed.sort_idxs is not None:\n",
    "        times = times[processed.sort_idxs]\n",
    "        targets = targets[processed.sort_idxs]\n",
    "    times = _array_tails(\n",
    "        times, processed.indptr, in_sample_output['sizes']\n",
    "    )\n",
    "    targets = _array_tails(\n",
    "        targets, processed.indptr, in_sample_output['sizes']\n",
    "    )\n",
    "    uids = ufp.repeat(processed.uids, in_sample_output['sizes'])\n",
    "    out = type(df)(\n",
    "        {\n",
    "            id_col: uids,\n",
    "            time_col: times,\n",
    "            target_col: targets,\n",
    "            'TimeGPT': in_sample_output['mean'],\n",
    "        }\n",
    "    )\n",
    "    return _maybe_add_intervals(out, in_sample_output['intervals'])  # type: ignore\n",
    "\n",
    "def _restrict_input_samples(level, input_size, model_horizon, h) -> int:\n",
    "    if level is not None:\n",
    "        # add sufficient info to compute\n",
    "        # conformal interval\n",
    "        # @AzulGarza\n",
    "        #  this is an old opinionated decision\n",
    "        #  about reducing the data sent to the api\n",
    "        #  to reduce latency when\n",
    "        #  a user passes level. since currently the model\n",
    "        #  uses conformal prediction, we can change a minimum\n",
    "        #  amount of data if the series are too large\n",
    "        new_input_size = 3 * input_size + max(model_horizon, h)\n",
    "    else:\n",
    "        # we only want to forecast\n",
    "        new_input_size = input_size\n",
    "    return new_input_size\n",
    "\n",
    "def _extract_target_array(df: DataFrame, target_col: str) -> np.ndarray:\n",
    "    # in pandas<2.2 to_numpy can lead to an object array if\n",
    "    # the type is a pandas nullable type, e.g. pd.Float64Dtype\n",
    "    # we thus use the dtype's type as the target dtype\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        target_dtype = df.dtypes[target_col].type\n",
    "        targets = df[target_col].to_numpy(dtype=target_dtype)\n",
    "    else:\n",
    "        targets = df[target_col].to_numpy()\n",
    "    return targets\n",
    "\n",
    "def _process_exog_features(\n",
    "    processed_data: np.ndarray,\n",
    "    x_cols: list[str],\n",
    "    hist_exog_list: Optional[list[str]] = None\n",
    ") -> tuple[Optional[np.ndarray], Optional[list[int]]]:\n",
    "    X = None\n",
    "    hist_exog = None\n",
    "    if processed_data.shape[1] > 1:\n",
    "        X = processed_data[:, 1:].T\n",
    "        if hist_exog_list is None:\n",
    "            futr_exog = x_cols\n",
    "        else:\n",
    "            missing_hist: set[str] = set(hist_exog_list) - set(x_cols)\n",
    "            if missing_hist:\n",
    "                raise ValueError(\n",
    "                    \"The following exogenous features were declared as historic \"\n",
    "                    f\"but were not found in `df`: {missing_hist}.\"\n",
    "                )\n",
    "            futr_exog = [c for c in x_cols if c not in hist_exog_list]\n",
    "            # match the forecast method order [future, historic]\n",
    "            fcst_features_order = futr_exog + hist_exog_list\n",
    "            x_idxs = [x_cols.index(c) for c in fcst_features_order]\n",
    "            X = X[x_idxs]\n",
    "            hist_exog = [fcst_features_order.index(c) for c in hist_exog_list]\n",
    "        if futr_exog and logger:\n",
    "            logger.info(f'Using future exogenous features: {futr_exog}')\n",
    "        if hist_exog_list and logger:\n",
    "            logger.info(f'Using historical exogenous features: {hist_exog_list}')\n",
    "\n",
    "    return X, hist_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ApiError(Exception):\n",
    "    status_code: Optional[int]\n",
    "    body: Any\n",
    "\n",
    "    def __init__(self, *, status_code: Optional[int] = None, body: Optional[Any] = None):\n",
    "        self.status_code = status_code\n",
    "        self.body = body\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"status_code: {self.status_code}, body: {self.body}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NixtlaClient:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        timeout: int = 60,\n",
    "        max_retries: int = 6,\n",
    "        retry_interval: int = 10,\n",
    "        max_wait_time: int = 6 * 60,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Client to interact with the Nixtla API.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_key : str, optional (default=None)\n",
    "            The authorization api_key interacts with the Nixtla API.\n",
    "            If not provided, will use the NIXTLA_API_KEY environment variable.\n",
    "        base_url : str, optional (default=None)\n",
    "            Custom base_url.\n",
    "            If not provided, will use the NIXTLA_BASE_URL environment variable.\n",
    "        timeout : int, optional (default=60)\n",
    "            Request timeout in seconds. Set this to `None` to disable it.\n",
    "        max_retries : int (default=6)\n",
    "            The maximum number of attempts to make when calling the API before giving up. \n",
    "            It defines how many times the client will retry the API call if it fails. \n",
    "            Default value is 6, indicating the client will attempt the API call up to 6 times in total\n",
    "        retry_interval : int (default=10)\n",
    "            The interval in seconds between consecutive retry attempts. \n",
    "            This is the waiting period before the client tries to call the API again after a failed attempt. \n",
    "            Default value is 10 seconds, meaning the client waits for 10 seconds between retries.\n",
    "        max_wait_time : int (default=360)\n",
    "            The maximum total time in seconds that the client will spend on all retry attempts before giving up. \n",
    "            This sets an upper limit on the cumulative waiting time for all retry attempts. \n",
    "            If this time is exceeded, the client will stop retrying and raise an exception. \n",
    "            Default value is 360 seconds, meaning the client will cease retrying if the total time \n",
    "            spent on retries exceeds 360 seconds. \n",
    "            The client throws a ReadTimeout error after 60 seconds of inactivity. If you want to \n",
    "            catch these errors, use max_wait_time >> 60. \n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            api_key = os.environ['NIXTLA_API_KEY']\n",
    "        if base_url is None:\n",
    "            base_url = os.getenv('NIXTLA_BASE_URL', 'https://api.nixtla.io')\n",
    "        self._client_kwargs = {\n",
    "            'base_url': base_url,\n",
    "            'headers': {\n",
    "                'Authorization': f'Bearer {api_key}',\n",
    "                'Content-Type': 'application/json',\n",
    "            },\n",
    "            'timeout': timeout,\n",
    "        }\n",
    "        self._retry_strategy = _retry_strategy(\n",
    "            max_retries=max_retries, retry_interval=retry_interval, max_wait_time=max_wait_time\n",
    "        )\n",
    "        self._model_params: dict[tuple[str, str], tuple[int, int]] = {}\n",
    "        self._is_azure = 'ai.azure' in base_url\n",
    "        if self._is_azure:\n",
    "            self.supported_models = ['azureai']\n",
    "        else:\n",
    "            self.supported_models = ['timegpt-1', 'timegpt-1-long-horizon']\n",
    "\n",
    "    def _make_request(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        payload: dict[str, Any],\n",
    "        multithreaded_compress: bool,\n",
    "    ) -> dict[str, Any]:\n",
    "        def ensure_contiguous_arrays(d: dict[str, Any]) -> None:\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, np.ndarray):\n",
    "                    if np.issubdtype(v.dtype, np.floating):\n",
    "                        v_cont = np.ascontiguousarray(v, dtype=np.float32)\n",
    "                        d[k] = np.nan_to_num(\n",
    "                            v_cont, \n",
    "                            nan=np.nan, \n",
    "                            posinf=np.finfo(np.float32).max, \n",
    "                            neginf=np.finfo(np.float32).min,\n",
    "                            copy=False,\n",
    "                        )\n",
    "                    else:\n",
    "                        d[k] = np.ascontiguousarray(v)\n",
    "                elif isinstance(v, dict):\n",
    "                    ensure_contiguous_arrays(v) \n",
    "\n",
    "        ensure_contiguous_arrays(payload)\n",
    "        content = orjson.dumps(payload, option=orjson.OPT_SERIALIZE_NUMPY)\n",
    "        content_size_mb = len(content) / 2**20\n",
    "        if content_size_mb > 200:\n",
    "            raise ValueError(f'The payload is too large. Set num_partitions={math.ceil(content_size_mb / 200)}')\n",
    "        headers = {}\n",
    "        if content_size_mb > 1:\n",
    "            threads = -1 if multithreaded_compress else 0\n",
    "            content = zstd.ZstdCompressor(level=1, threads=threads).compress(content)\n",
    "            headers['content-encoding'] = 'zstd'\n",
    "        resp = client.post(url=endpoint, content=content, headers=headers)\n",
    "        try:\n",
    "            resp_body = orjson.loads(resp.content)\n",
    "        except orjson.JSONDecodeError:\n",
    "            raise ApiError(\n",
    "                status_code=resp.status_code,\n",
    "                body=f'Could not parse JSON: {resp.content}',\n",
    "            )\n",
    "        if resp.status_code != 200:\n",
    "            raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        if 'data' in resp_body:\n",
    "            resp_body = resp_body['data']\n",
    "        return resp_body\n",
    "\n",
    "    def _make_request_with_retries(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        payload: dict[str, Any],\n",
    "        multithreaded_compress: bool = True,\n",
    "    ) -> dict[str, Any]:\n",
    "        return self._retry_strategy(self._make_request)(\n",
    "            client=client,\n",
    "            endpoint=endpoint,\n",
    "            payload=payload,\n",
    "            multithreaded_compress=multithreaded_compress,\n",
    "        )\n",
    "\n",
    "    def _get_request(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        params: Optional[dict[str, Any]] = None,\n",
    "    ) -> dict[str, Any]:\n",
    "        resp = client.get(endpoint, params=params)\n",
    "        resp_body = resp.json()\n",
    "        if resp.status_code != 200:\n",
    "            raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        return resp_body\n",
    "\n",
    "    def _make_partitioned_requests(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        payloads: list[dict[str, Any]],\n",
    "    ) -> dict[str, Any]:\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        num_partitions = len(payloads)\n",
    "        results: list[dict[str, Any]] = [{} for _ in range(num_partitions)]\n",
    "        max_workers = min(10, num_partitions)\n",
    "        with ThreadPoolExecutor(max_workers) as executor:\n",
    "            future2pos = {\n",
    "                executor.submit(\n",
    "                    self._make_request_with_retries,\n",
    "                    client=client,\n",
    "                    endpoint=endpoint,\n",
    "                    payload=payload,\n",
    "                    multithreaded_compress=False,\n",
    "                ): i\n",
    "                for i, payload in enumerate(payloads)\n",
    "            }\n",
    "            for future in tqdm(as_completed(future2pos), total=len(future2pos)):\n",
    "                pos = future2pos[future]\n",
    "                results[pos] = future.result()\n",
    "        resp = {\"mean\": np.hstack([res[\"mean\"] for res in results])}\n",
    "        first_res = results[0]\n",
    "        for k in ('sizes', 'anomaly'):\n",
    "            if k in first_res:\n",
    "                resp[k] = np.hstack([res[k] for res in results])\n",
    "        if 'idxs' in first_res:\n",
    "            offsets = [0] + [sum(p['series']['sizes']) for p in payloads[:-1]]\n",
    "            resp['idxs'] = np.hstack(\n",
    "                [\n",
    "                    np.array(res['idxs'], dtype=np.int64) + offset\n",
    "                    for res, offset in zip(results, offsets)\n",
    "                ]\n",
    "            )\n",
    "        if 'anomaly_score' in first_res:\n",
    "            resp['anomaly_score'] = np.hstack([res['anomaly_score'] for res in results])\n",
    "        if first_res[\"intervals\"] is None:\n",
    "            resp[\"intervals\"] = None\n",
    "        else:\n",
    "            resp[\"intervals\"] = {}\n",
    "            for k in first_res[\"intervals\"].keys():\n",
    "                resp[\"intervals\"][k] = np.hstack(\n",
    "                    [res[\"intervals\"][k] for res in results]\n",
    "                )\n",
    "        if \"weights_x\" not in first_res or first_res[\"weights_x\"] is None:\n",
    "            resp[\"weights_x\"] = None\n",
    "        else:\n",
    "            resp[\"weights_x\"] = [res[\"weights_x\"] for res in results]\n",
    "        if \"feature_contributions\" not in first_res or first_res[\"feature_contributions\"] is None:\n",
    "            resp[\"feature_contributions\"] = None\n",
    "        else:\n",
    "            resp[\"feature_contributions\"] = np.vstack([\n",
    "                np.stack(res[\"feature_contributions\"], axis=1) for res in results\n",
    "            ]).T\n",
    "        return resp\n",
    "\n",
    "    def _maybe_override_model(self, model: _Model) -> _Model:\n",
    "        if self._is_azure and model != 'azureai':\n",
    "            warnings.warn(\"Azure endpoint detected, setting `model` to 'azureai'.\")\n",
    "            model = 'azureai'\n",
    "        return model\n",
    "\n",
    "    def _get_model_params(self, model: _Model, freq: str) -> tuple[int, int]:\n",
    "        key = (model, freq)\n",
    "        if key not in self._model_params:\n",
    "            logger.info('Querying model metadata...')\n",
    "            payload = {'model': model, 'freq': freq}\n",
    "            with httpx.Client(**self._client_kwargs) as client:\n",
    "                if self._is_azure:\n",
    "                    resp_body = self._make_request_with_retries(\n",
    "                        client, 'model_params', payload\n",
    "                    )\n",
    "                else:\n",
    "                    resp_body = self._retry_strategy(self._get_request)(\n",
    "                        client, '/model_params', payload\n",
    "                    )\n",
    "            params = resp_body['detail']\n",
    "            self._model_params[key] = (params['input_size'], params['horizon'])\n",
    "        return self._model_params[key]\n",
    "\n",
    "    def _maybe_assign_weights(\n",
    "        self,\n",
    "        weights: Optional[Union[list[float], list[list[float]]]],\n",
    "        df: DataFrame,\n",
    "        x_cols: list[str],\n",
    "    ) -> None:\n",
    "        if weights is None:\n",
    "            return\n",
    "        if isinstance(weights[0], list):\n",
    "            self.weights_x = [\n",
    "                type(df)({'features': x_cols, 'weights': w}) for w in weights\n",
    "            ]\n",
    "        else:\n",
    "            self.weights_x = type(df)(\n",
    "                {'features': x_cols, 'weights': weights}\n",
    "            )\n",
    "\n",
    "    def _maybe_assign_feature_contributions(\n",
    "        self,\n",
    "        expected_contributions: bool,\n",
    "        resp: dict[str, Any],\n",
    "        x_cols: list[str],\n",
    "        out_df: DataFrame,\n",
    "        insample_feat_contributions: Optional[list[list[float]]],\n",
    "    ) -> None:\n",
    "        if not expected_contributions:\n",
    "            return\n",
    "        if 'feature_contributions' not in resp:\n",
    "            if self._is_azure:\n",
    "                warnings.warn(\n",
    "                    \"feature_contributions aren't implemented in Azure yet.\"\n",
    "                )\n",
    "                return\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    'feature_contributions expected in response but not found'\n",
    "                )\n",
    "        feature_contributions = resp['feature_contributions']\n",
    "        if feature_contributions is None:\n",
    "            return     \n",
    "        shap_cols = x_cols + [\"base_value\"]\n",
    "        shap_df = type(out_df)(dict(zip(shap_cols, feature_contributions)))\n",
    "        if insample_feat_contributions is not None:\n",
    "            insample_shap_df = type(out_df)(\n",
    "                dict(zip(shap_cols, insample_feat_contributions))\n",
    "            )\n",
    "            shap_df = ufp.vertical_concat([insample_shap_df, shap_df])\n",
    "        self.feature_contributions = ufp.horizontal_concat([out_df, shap_df])\n",
    "\n",
    "    def _run_validations(\n",
    "        self,\n",
    "        df: DFType,\n",
    "        X_df: Optional[DFType],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        model: _Model,\n",
    "        validate_api_key: bool,\n",
    "        freq: Optional[_FreqType],\n",
    "    ) -> tuple[DFType, Optional[DFType], bool, _FreqType]:\n",
    "        if validate_api_key and not self.validate_api_key(log=False):\n",
    "            raise Exception('API Key not valid, please email support@nixtla.io')\n",
    "        if model not in self.supported_models:\n",
    "            raise ValueError(\n",
    "                f'unsupported model: {model}. supported models: {self.supported_models}'\n",
    "            )\n",
    "        drop_id = id_col not in df.columns\n",
    "        if drop_id:\n",
    "            df = ufp.copy_if_pandas(df, deep=False)\n",
    "            df = ufp.assign_columns(df, id_col, 0)\n",
    "            if X_df is not None:\n",
    "                X_df = ufp.copy_if_pandas(X_df, deep=False)\n",
    "                X_df = ufp.assign_columns(X_df, id_col, 0)\n",
    "        if (\n",
    "            isinstance(df, pd.DataFrame)\n",
    "            and time_col not in df\n",
    "            and pd.api.types.is_datetime64_any_dtype(df.index)\n",
    "        ):\n",
    "            df.index.name = time_col\n",
    "            df = df.reset_index()\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "        validate_format(df=df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "        if ufp.is_nan_or_none(df[target_col]).any():\n",
    "            raise ValueError(f'Target column ({target_col}) cannot contain missing values.')\n",
    "        freq = _maybe_infer_freq(df, freq=freq, id_col=id_col, time_col=time_col)\n",
    "        if isinstance(freq, (str, int)):\n",
    "            expected_ids_times = id_time_grid(\n",
    "                df,\n",
    "                freq=freq,\n",
    "                start=\"per_serie\",\n",
    "                end=\"per_serie\",\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "            )\n",
    "            freq_ok = len(df) == len(expected_ids_times)\n",
    "        elif isinstance(freq, pd.offsets.BaseOffset):\n",
    "            times_by_id = df.groupby(id_col, observed=True)[time_col].agg(['min', 'max', 'size'])\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "                expected_ends = times_by_id['min'] + freq * (times_by_id['size'] - 1)\n",
    "            freq_ok = (expected_ends == times_by_id['max']).all()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`freq` should be a string, integer or pandas offset, \"\n",
    "                f\"got {type(freq).__name__}.\"\n",
    "            )\n",
    "        if not freq_ok:\n",
    "            raise ValueError(\n",
    "                \"Series contain missing or duplicate timestamps, or the timestamps \"\n",
    "                \"do not match the provided frequency.\\n\"\n",
    "                \"Please make sure that all series have a single observation from the first \"\n",
    "                \"to the last timestamp and that the provided frequency matches the timestamps'.\\n\"\n",
    "                \"You can refer to https://docs.nixtla.io/docs/tutorials-missing_values \"\n",
    "                \"for an end to end example.\"\n",
    "            )\n",
    "        return df, X_df, drop_id, freq\n",
    "\n",
    "    def validate_api_key(self, log: bool = True) -> bool:\n",
    "        \"\"\"Check API key status.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        log : bool (default=True)\n",
    "            Show the endpoint's response.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Whether API key is valid.\"\"\"\n",
    "        if self._is_azure:\n",
    "            raise NotImplementedError(\n",
    "                'validate_api_key is not implemented for Azure deployments, '\n",
    "                'you can try using the forecasting methods directly.'\n",
    "            )\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            resp = client.get(\"/validate_api_key\")\n",
    "            body = resp.json()\n",
    "        if log:\n",
    "            logger.info(body[\"detail\"])\n",
    "        return resp.status_code == 200\n",
    "\n",
    "    def usage(self) -> dict[str, dict[str, int]]:\n",
    "        \"\"\"Query consumed requests and limits\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Consumed requests and limits by minute and month.\"\"\"\n",
    "        if self._is_azure:\n",
    "            raise NotImplementedError('usage is not implemented for Azure deployments')\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            return self._get_request(client, '/usage')\n",
    "\n",
    "    def finetune(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        freq: Optional[_Freq] = None,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        finetune_steps: _NonNegativeInt = 10,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        output_model_id: Optional[str] = None,\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        model: _Model = 'timegpt-1',\n",
    "    ) -> str:\n",
    "        \"\"\"Fine-tune TimeGPT to your series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        finetune_steps : int (default=10)\n",
    "            Number of steps used to finetune learning TimeGPT in the new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        output_model_id : str, optional(default=None)\n",
    "            ID to assign to the fine-tuned model. If `None`, an UUID is used. \n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use as base.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ID of the fine-tuned model\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            raise ValueError(\"Can only fine-tune on pandas or polars dataframes.\")\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, X_df, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=False,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, *_ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=False,\n",
    "            date_features_to_one_hot=False,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        _validate_input_size(processed, model_input_size, model_horizon)\n",
    "        logger.info('Calling Fine-tune Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "            },\n",
    "            'model': model,\n",
    "            'freq': standard_freq,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'output_model_id': output_model_id,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "        }\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            resp = self._make_request_with_retries(client, 'v2/finetune', payload)\n",
    "        return resp['finetuned_model_id']\n",
    "\n",
    "    @overload\n",
    "    def finetuned_models(self, as_df: Literal[False]) -> list[FinetunedModel]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def finetuned_models(self, as_df: Literal[True]) -> pd.DataFrame:\n",
    "        ...\n",
    "\n",
    "    def finetuned_models(\n",
    "        self,\n",
    "        as_df: bool = False,\n",
    "    ) -> Union[list[FinetunedModel], pd.DataFrame]:\n",
    "        \"\"\"List fine-tuned models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        as_df : bool\n",
    "            Return the fine-tuned models as a pandas dataframe\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of FinetunedModel\n",
    "            List of available fine-tuned models.\"\"\"\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            resp_body = self._get_request(client, '/v2/finetuned_models')\n",
    "        models = [FinetunedModel(**m) for m in resp_body['finetuned_models']]\n",
    "        if as_df:\n",
    "            models = pd.DataFrame([m.model_dump() for m in models])\n",
    "        return models\n",
    "\n",
    "    def delete_finetuned_model(self, finetuned_model_id: str) -> bool:\n",
    "        \"\"\"Delete a previously fine-tuned model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        finetuned_model_id : str\n",
    "            ID of the fine-tuned model to be deleted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Whether delete was successful.\"\"\"\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            resp = client.delete(\n",
    "                f\"/v2/finetuned_models/{finetuned_model_id}\",\n",
    "                headers={'accept-encoding': 'identity'},\n",
    "            )\n",
    "        return resp.status_code == 204\n",
    "\n",
    "    def _distributed_forecast(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        X_df: Optional[DistributedDFType],\n",
    "        level: Optional[list[Union[int, float]]],\n",
    "        quantiles: Optional[list[float]],\n",
    "        finetune_steps: _NonNegativeInt,\n",
    "        finetune_depth: _FinetuneDepth,\n",
    "        finetune_loss: _Loss,\n",
    "        finetuned_model_id: Optional[str],\n",
    "        clean_ex_first: bool,\n",
    "        hist_exog_list: Optional[list[str]],\n",
    "        validate_api_key: bool,\n",
    "        add_history: bool,\n",
    "        date_features: Union[bool, list[Union[str, Callable]]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        num_partitions: Optional[int],\n",
    "        feature_contributions: bool,\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "    \n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='forecast',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        if X_df is not None:\n",
    "            def format_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "                return df.assign(_in_sample=True)\n",
    "    \n",
    "            def format_X_df(\n",
    "                X_df: pd.DataFrame,\n",
    "                target_col: str,\n",
    "                df_cols: list[str],\n",
    "            ) -> pd.DataFrame:\n",
    "                return X_df.assign(**{'_in_sample': False, target_col: 0.0})[df_cols]\n",
    "    \n",
    "            df = fa.transform(df, format_df, schema='*,_in_sample:bool')\n",
    "            X_df = fa.transform(\n",
    "                X_df,\n",
    "                format_X_df,\n",
    "                schema=fa.get_schema(df),\n",
    "                params={'target_col': target_col, 'df_cols': fa.get_column_names(df)},\n",
    "            )\n",
    "            df = fa.union(df, X_df)\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_forecast_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=None,\n",
    "                feature_contributions=feature_contributions,   \n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq] = None,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[AnyDFType] = None,\n",
    "        level: Optional[list[Union[int, float]]] = None,\n",
    "        quantiles: Optional[list[float]] = None,\n",
    "        finetune_steps: _NonNegativeInt = 0,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        clean_ex_first: bool = True,\n",
    "        hist_exog_list: Optional[list[str]] = None,\n",
    "        validate_api_key: bool = False,\n",
    "        add_history: bool = False,\n",
    "        date_features: Union[bool, list[Union[str, Callable]]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "        feature_contributions: bool = False\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : list[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        quantiles : list[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the\n",
    "            new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        hist_exog_list : list of str, optional (default=None)\n",
    "            Column names of the historical exogenous features.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        feature_contributions: bool (default=False)\n",
    "            Compute SHAP values\n",
    "            Gives access to computed SHAP values to explain the impact\n",
    "            of features on the final predictions.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset.\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_forecast(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "                feature_contributions=feature_contributions,\n",
    "            )\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        self.__dict__.pop('feature_contributions', None)\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, X_df, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=validate_api_key,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "        df, X_df = _validate_exog(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            hist_exog=hist_exog_list,\n",
    "        )\n",
    "        level, quantiles = _prepare_level_and_quantiles(level, quantiles)\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, X_future, x_cols, futr_cols = _preprocess(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            h=h,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        if finetune_steps > 0 or level is not None or add_history:\n",
    "            _validate_input_size(processed, model_input_size, model_horizon)\n",
    "        if h > model_horizon:\n",
    "            logger.warning(\n",
    "                'The specified horizon \"h\" exceeds the model horizon, '\n",
    "                'this may lead to less accurate forecasts. '\n",
    "                'Please consider using a smaller horizon.'  \n",
    "            )\n",
    "        restrict_input = finetune_steps == 0 and not x_cols and not add_history\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            processed = _tail(processed, new_input_size)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T\n",
    "            if futr_cols is not None:\n",
    "                logger.info(f'Using future exogenous features: {futr_cols}')\n",
    "            if hist_exog_list:\n",
    "                logger.info(f'Using historical exogenous features: {hist_exog_list}')\n",
    "        else:\n",
    "            X = None\n",
    "\n",
    "        logger.info('Calling Forecast Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "                'X': X,\n",
    "                'X_future': X_future,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "            'feature_contributions': feature_contributions and X is not None,\n",
    "        }\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            insample_feat_contributions = None\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(client, 'v2/forecast', payload)\n",
    "                if add_history:\n",
    "                    in_sample_payload = _forecast_payload_to_in_sample(payload)\n",
    "                    logger.info('Calling Historical Forecast Endpoint...')\n",
    "                    in_sample_resp = self._make_request_with_retries(\n",
    "                        client, 'v2/historic_forecast', in_sample_payload\n",
    "                    )\n",
    "                    insample_feat_contributions = in_sample_resp.get(\n",
    "                        'feature_contributions', None\n",
    "                    )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/forecast', payloads)\n",
    "                if add_history:\n",
    "                    in_sample_payloads = [\n",
    "                        _forecast_payload_to_in_sample(p) for p in payloads\n",
    "                    ]\n",
    "                    logger.info('Calling Historical Forecast Endpoint...')\n",
    "                    in_sample_resp = self._make_partitioned_requests(\n",
    "                        client, 'v2/historic_forecast', in_sample_payloads\n",
    "                    )\n",
    "                    insample_feat_contributions = in_sample_resp.get(\n",
    "                        'feature_contributions', None\n",
    "                    )\n",
    "\n",
    "        # assemble result\n",
    "        out = ufp.make_future_dataframe(\n",
    "            uids=processed.uids,\n",
    "            last_times=type(processed.uids)(processed.last_times),\n",
    "            freq=freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        out = _maybe_add_intervals(out, resp['intervals'])\n",
    "        if add_history:\n",
    "            in_sample_df = _parse_in_sample_output(\n",
    "                in_sample_output=in_sample_resp,\n",
    "                df=df,\n",
    "                processed=processed,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "            in_sample_df = ufp.drop_columns(in_sample_df, target_col)\n",
    "            out = ufp.vertical_concat([in_sample_df, out])\n",
    "        out = _maybe_convert_level_to_quantiles(out, quantiles)\n",
    "        self._maybe_assign_feature_contributions(\n",
    "            expected_contributions=feature_contributions,\n",
    "            resp=resp,\n",
    "            x_cols=x_cols,\n",
    "            out_df=out[[id_col, time_col, 'TimeGPT']],\n",
    "            insample_feat_contributions=insample_feat_contributions,\n",
    "        )\n",
    "        if add_history:\n",
    "            sort_idxs = ufp.maybe_compute_sort_indices(out, id_col=id_col, time_col=time_col)\n",
    "            if sort_idxs is not None:\n",
    "                out = ufp.take_rows(out, sort_idxs)\n",
    "                out = ufp.drop_index_if_pandas(out)\n",
    "                if hasattr(self, 'feature_contributions'):\n",
    "                    self.feature_contributions = ufp.take_rows(self.feature_contributions, sort_idxs)\n",
    "                    self.feature_contributions = ufp.drop_index_if_pandas(self.feature_contributions)\n",
    "        out = _maybe_drop_id(df=out, id_col=id_col, drop=drop_id)\n",
    "        self._maybe_assign_weights(weights=resp['weights_x'], df=df, x_cols=x_cols)\n",
    "        return out\n",
    "\n",
    "    def _distributed_detect_anomalies(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        level: Union[int, float],\n",
    "        finetuned_model_id: Optional[str],\n",
    "        clean_ex_first: bool,\n",
    "        validate_api_key: bool,\n",
    "        date_features: Union[bool, list[str]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        num_partitions: Optional[int],\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "    \n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='detect_anomalies',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=None,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_detect_anomalies_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=None,   \n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def detect_anomalies(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        freq: Optional[_Freq] = None,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Union[int, float] = 99,\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        clean_ex_first: bool = True,\n",
    "        validate_api_key: bool = False,\n",
    "        date_features: Union[bool, list[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"Detect anomalies in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before sending requests.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset.\n",
    "            DataFrame with anomalies flagged by TimeGPT.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_detect_anomalies(\n",
    "                df=df,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, _, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=validate_api_key,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, _, x_cols, _ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T\n",
    "            logger.info(f'Using the following exogenous features: {x_cols}')\n",
    "        else:\n",
    "            X = None\n",
    "\n",
    "        logger.info('Calling Anomaly Detector Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "                'X': X,\n",
    "            },\n",
    "            'model': model,\n",
    "            'freq': standard_freq,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "        }\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(\n",
    "                    client, 'v2/anomaly_detection', payload\n",
    "                )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h=0)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/anomaly_detection', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        out = _parse_in_sample_output(\n",
    "            in_sample_output=resp,\n",
    "            df=df,\n",
    "            processed=processed,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'anomaly', resp['anomaly'])\n",
    "        out = _maybe_drop_id(df=out, id_col=id_col, drop=drop_id)\n",
    "        self._maybe_assign_weights(weights=resp[\"weights_x\"], df=df, x_cols=x_cols)\n",
    "        return out\n",
    "    \n",
    "    def _distributed_detect_anomalies_online(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        h: _PositiveInt,\n",
    "        detection_size: _PositiveInt,\n",
    "        threshold_method: _ThresholdMethod,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        level: Union[int, float],\n",
    "        clean_ex_first: bool,\n",
    "        step_size: Optional[_PositiveInt],\n",
    "        finetune_steps: _NonNegativeInt,\n",
    "        finetune_depth: _FinetuneDepth,\n",
    "        finetune_loss: _Loss,\n",
    "        hist_exog_list: Optional[list[str]],\n",
    "        date_features: Union[bool, list[str]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        refit: bool,\n",
    "        num_partitions: Optional[int],\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "    \n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='detect_anomalies_online',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=None,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_detect_anomalies_online_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                h=h,\n",
    "                detection_size=detection_size,\n",
    "                threshold_method=threshold_method,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetune_depth=finetune_depth,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                refit=refit,\n",
    "                num_partitions=None,   \n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def detect_anomalies_online(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        h: _PositiveInt,\n",
    "        detection_size: _PositiveInt,\n",
    "        threshold_method: _ThresholdMethod = 'univariate',\n",
    "        freq: Optional[_Freq] = None,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Union[int, float] = 99,\n",
    "        clean_ex_first: bool = True,\n",
    "        step_size: Optional[_PositiveInt] = None,\n",
    "        finetune_steps: _NonNegativeInt = 0,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        hist_exog_list: Optional[list[str]] = None,\n",
    "        date_features: Union[bool, list[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        refit: bool = False,\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"\n",
    "        Online anomaly detection in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        detection_size : int\n",
    "            The length of the sequence where anomalies will be detected starting from the end of the dataset.\n",
    "        threshold_method : str, optional (default='univariate')\n",
    "            The method used to calculate the intervals for anomaly detection.\n",
    "            Use `univariate` to flag anomalies independently for each series in the dataset.\n",
    "            Use `multivariate` to have a global threshold across all series in the dataset. For this method, all series\n",
    "            must have the same length.\n",
    "        freq : str, optional\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str, optional (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str, optional (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str, optional (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float, optional (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        clean_ex_first : bool, optional (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        hist_exog_list : list of str, optional (default=None)\n",
    "            Column names of the historical exogenous features.\n",
    "        date_features : bool or list of str, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True, automatically adds most used date features for the frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str, optional (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are one-hot encoded by default.\n",
    "        model : str, optional (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`.\n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting if you want to predict more than one seasonal\n",
    "            period given the frequency of your data.\n",
    "        refit : bool, optional (default=False)\n",
    "            Fine-tune the model in each window. If False, only fine-tunes on the first window.\n",
    "            Only used if finetune_steps > 0.e\n",
    "        num_partitions : int, optional (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal to the available parallel resources in distributed environments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset\n",
    "            DataFrame with anomalies flagged by TimeGPT.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_detect_anomalies_online(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                detection_size=detection_size,\n",
    "                threshold_method=threshold_method,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                refit=refit,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        if threshold_method == \"multivariate\" and num_partitions is not None and num_partitions > 1:\n",
    "            raise ValueError(\n",
    "                \"Cannot use more than 1 partition for multivariate anomaly detection. \"\n",
    "                \"Either set threshold_method to univariate \"\n",
    "                \"or set num_partitions to None.\"\n",
    "            )\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, _, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=False,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, _, x_cols, _ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        targets = _extract_target_array(df, target_col)\n",
    "        times = df[time_col].to_numpy()\n",
    "        if processed.sort_idxs is not None:\n",
    "            targets = targets[processed.sort_idxs]\n",
    "            times = times[processed.sort_idxs]\n",
    "        X, hist_exog = _process_exog_features(processed.data, x_cols, hist_exog_list)\n",
    "        sizes = np.diff(processed.indptr)\n",
    "        if np.all(sizes <= 6 * detection_size):\n",
    "            logger.warn('Detection size is large. Using the entire series to compute the anomaly threshold...')\n",
    "        logger.info('Calling Online Anomaly Detector Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': sizes,\n",
    "                'X': X,\n",
    "            },\n",
    "            'h': h,\n",
    "            'detection_size': detection_size,\n",
    "            'threshold_method': threshold_method,\n",
    "            'model': model,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'step_size': step_size,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'refit': refit,\n",
    "            'hist_exog': hist_exog,\n",
    "        }\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(\n",
    "                    client, 'v2/online_anomaly_detection', payload\n",
    "                )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h=0)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/online_anomaly_detection', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        idxs = np.array(resp['idxs'], dtype=np.int64)\n",
    "        sizes = np.array(resp['sizes'], dtype=np.int64)\n",
    "        out = type(df)(\n",
    "            {\n",
    "                id_col: ufp.repeat(processed.uids, sizes),\n",
    "                time_col: times[idxs],\n",
    "                target_col: targets[idxs],\n",
    "            }\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        out = ufp.assign_columns(out, 'anomaly', resp['anomaly'])\n",
    "        out = ufp.assign_columns(out, 'anomaly_score', resp['anomaly_score'])\n",
    "        if threshold_method == 'multivariate':\n",
    "            out = ufp.assign_columns(out, 'accumulated_anomaly_score', resp['accumulated_anomaly_score'])\n",
    "        return _maybe_add_intervals(out, resp['intervals'])\n",
    "\n",
    "    def _distributed_cross_validation(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        level: Optional[list[Union[int, float]]],\n",
    "        quantiles: Optional[list[float]],\n",
    "        validate_api_key: bool,\n",
    "        n_windows: _PositiveInt,\n",
    "        step_size: Optional[_PositiveInt],\n",
    "        finetune_steps: _NonNegativeInt,\n",
    "        finetune_depth: _FinetuneDepth,\n",
    "        finetune_loss: _Loss,\n",
    "        finetuned_model_id: Optional[str],\n",
    "        refit: bool,\n",
    "        clean_ex_first: bool,\n",
    "        hist_exog_list: Optional[list[str]],\n",
    "        date_features: Union[bool, Sequence[Union[str, Callable]]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        num_partitions: Optional[int],\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "    \n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='cross_validation',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_cross_validation_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                validate_api_key=validate_api_key,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                refit=refit,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=None,   \n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq] = None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        level: Optional[list[Union[int, float]]] = None,\n",
    "        quantiles: Optional[list[float]] = None,\n",
    "        validate_api_key: bool = False,\n",
    "        n_windows: _PositiveInt = 1,\n",
    "        step_size: Optional[_PositiveInt] = None,\n",
    "        finetune_steps: _NonNegativeInt = 0,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        refit: bool = True,\n",
    "        clean_ex_first: bool = True,\n",
    "        hist_exog_list: Optional[list[str]] = None,\n",
    "        date_features: Union[bool, list[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"Perform cross validation in your time series using TimeGPT.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for prediction intervals.\n",
    "        quantiles : list[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before sending requests.\n",
    "        n_windows : int (defaul=1)\n",
    "            Number of windows to evaluate.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use.\n",
    "        refit : bool (default=True)\n",
    "            Fine-tune the model in each window. If `False`, only fine-tunes on the first window.\n",
    "            Only used if `finetune_steps` > 0.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        hist_exog_list : list of str, optional (default=None)\n",
    "            Column names of the historical exogenous features.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the\n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset.\n",
    "            DataFrame with cross validation forecasts.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_cross_validation(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                validate_api_key=validate_api_key,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                refit=refit,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, _, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=validate_api_key,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "        level, quantiles = _prepare_level_and_quantiles(level, quantiles)\n",
    "        if step_size is None:\n",
    "            step_size = h\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, _, x_cols, _ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        targets = _extract_target_array(df, target_col)\n",
    "        times = df[time_col].to_numpy()\n",
    "        if processed.sort_idxs is not None:\n",
    "            targets = targets[processed.sort_idxs]\n",
    "            times = times[processed.sort_idxs]\n",
    "        restrict_input = finetune_steps == 0 and not x_cols\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            new_input_size += h + step_size * (n_windows - 1)\n",
    "            orig_indptr = processed.indptr\n",
    "            processed = _tail(processed, new_input_size)\n",
    "            times = _array_tails(times, orig_indptr, np.diff(processed.indptr))\n",
    "            targets = _array_tails(targets, orig_indptr, np.diff(processed.indptr))\n",
    "        X, hist_exog = _process_exog_features(processed.data, x_cols, hist_exog_list)\n",
    "\n",
    "        logger.info('Calling Cross Validation Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': targets,\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "                'X': X,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'n_windows': n_windows,\n",
    "            'step_size': step_size,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'hist_exog': hist_exog,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "            'refit': refit,\n",
    "        }\n",
    "        with httpx.Client(**self._client_kwargs) as client:\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(\n",
    "                    client, 'v2/cross_validation', payload\n",
    "                )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h=0)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/cross_validation', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        idxs = np.array(resp['idxs'], dtype=np.int64)\n",
    "        sizes = np.array(resp['sizes'], dtype=np.int64)\n",
    "        window_starts = np.arange(0, sizes.sum(), h)\n",
    "        cutoff_idxs = np.repeat(idxs[window_starts] - 1, h)\n",
    "        out = type(df)(\n",
    "            {\n",
    "                id_col: ufp.repeat(processed.uids, sizes),\n",
    "                time_col: times[idxs],\n",
    "                'cutoff': times[cutoff_idxs],\n",
    "                target_col: targets[idxs],\n",
    "            }\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        out = _maybe_add_intervals(out, resp['intervals'])\n",
    "        out = _maybe_drop_id(df=out, id_col=id_col, drop=drop_id)\n",
    "        return _maybe_convert_level_to_quantiles(out, quantiles)\n",
    "\n",
    "    def plot(\n",
    "        self,\n",
    "        df: Optional[DataFrame] = None,\n",
    "        forecasts_df: Optional[DataFrame] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        unique_ids: Union[Optional[list[str]], np.ndarray] = None,\n",
    "        plot_random: bool = True,\n",
    "        max_ids: int = 8,\n",
    "        models: Optional[list[str]] = None,\n",
    "        level: Optional[list[Union[int, float]]] = None,\n",
    "        max_insample_length: Optional[int] = None,\n",
    "        plot_anomalies: bool = False,\n",
    "        engine: Literal['matplotlib', 'plotly', 'plotly-resampler'] = 'matplotlib',\n",
    "        resampler_kwargs: Optional[dict] = None,\n",
    "        ax: Optional[Union[\"plt.Axes\", np.ndarray, \"plotly.graph_objects.Figure\"]] = None,\n",
    "    ):\n",
    "        \"\"\"Plot forecasts and insample values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame, optional (default=None)\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        forecasts_df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and models.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        unique_ids : list[str], optional (default=None)\n",
    "            Time Series to plot.\n",
    "            If None, time series are selected randomly.\n",
    "        plot_random : bool (default=True)\n",
    "            Select time series to plot randomly.\n",
    "        max_ids : int (default=8)\n",
    "            Maximum number of ids to plot.\n",
    "        models : list[str], optional (default=None)\n",
    "            list of models to plot.\n",
    "        level : list[float], optional (default=None)\n",
    "            list of prediction intervals to plot if paseed.\n",
    "        max_insample_length : int, optional (default=None)\n",
    "            Max number of train/insample observations to be plotted.\n",
    "        plot_anomalies : bool (default=False)\n",
    "            Plot anomalies for each prediction interval.\n",
    "        engine : str (default='matplotlib')\n",
    "            Library used to plot. 'matplotlib', 'plotly' or 'plotly-resampler'.\n",
    "        resampler_kwargs : dict\n",
    "            Kwargs to be passed to plotly-resampler constructor.\n",
    "            For further custumization (\"show_dash\") call the method,\n",
    "            store the plotting object and add the extra arguments to\n",
    "            its `show_dash` method.\n",
    "        ax : matplotlib axes, array of matplotlib axes or plotly Figure, optional (default=None)\n",
    "            Object where plots will be added.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from utilsforecast.plotting import plot_series\n",
    "        except ModuleNotFoundError:\n",
    "            raise Exception(\n",
    "                'You have to install additional dependencies to use this method, '\n",
    "                'please install them using `pip install \"nixtla[plotting]\"`'\n",
    "            )\n",
    "        if df is not None and id_col not in df.columns:\n",
    "            df = ufp.copy_if_pandas(df, deep=False)\n",
    "            df = ufp.assign_columns(df, id_col, 'ts_0')\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "        if forecasts_df is not None:\n",
    "            if id_col not in forecasts_df.columns:\n",
    "                forecasts_df = ufp.copy_if_pandas(forecasts_df, deep=False)\n",
    "                forecasts_df = ufp.assign_columns(forecasts_df, id_col, 'ts_0')\n",
    "            forecasts_df = ensure_time_dtype(forecasts_df, time_col=time_col)\n",
    "            if 'anomaly' in forecasts_df.columns:\n",
    "                # special case to plot outputs\n",
    "                # from detect_anomalies\n",
    "                df = None\n",
    "                forecasts_df = ufp.drop_columns(forecasts_df, 'anomaly')\n",
    "                cols = [\n",
    "                    c.replace('TimeGPT-lo-', '')\n",
    "                    for c in forecasts_df.columns\n",
    "                    if 'TimeGPT-lo-' in c\n",
    "                ]\n",
    "                level = [float(c) if '.' in c else int(c) for c in cols]\n",
    "                plot_anomalies = True\n",
    "                models = ['TimeGPT']\n",
    "        return plot_series(\n",
    "            df=df,\n",
    "            forecasts_df=forecasts_df,\n",
    "            ids=unique_ids,\n",
    "            plot_random=plot_random,\n",
    "            max_ids=max_ids,\n",
    "            models=models,\n",
    "            level=level,\n",
    "            max_insample_length=max_insample_length,\n",
    "            plot_anomalies=plot_anomalies,\n",
    "            engine=engine,\n",
    "            resampler_kwargs=resampler_kwargs,\n",
    "            palette=\"tab20b\",\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _forecast_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    h: _PositiveInt,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[list[Union[int, float]]],\n",
    "    quantiles: Optional[list[float]],\n",
    "    finetune_steps: _NonNegativeInt,\n",
    "    finetune_depth: _FinetuneDepth,\n",
    "    finetune_loss: _Loss,\n",
    "    finetuned_model_id: Optional[str],\n",
    "    clean_ex_first: bool,\n",
    "    hist_exog_list: Optional[list[str]],\n",
    "    validate_api_key: bool,\n",
    "    add_history: bool,\n",
    "    date_features: Union[bool, list[Union[str, Callable]]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    "    feature_contributions: bool,\n",
    ") -> pd.DataFrame:\n",
    "    if '_in_sample' in df:\n",
    "        in_sample_mask = df['_in_sample']\n",
    "        X_df = df.loc[~in_sample_mask].drop(columns=['_in_sample', target_col])\n",
    "        df = df.loc[in_sample_mask].drop(columns='_in_sample')\n",
    "    else:\n",
    "        X_df = None\n",
    "    return client.forecast(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        X_df=X_df,\n",
    "        level=level,\n",
    "        quantiles=quantiles,\n",
    "        finetune_steps=finetune_steps,\n",
    "        finetune_depth=finetune_depth,\n",
    "        finetune_loss=finetune_loss,\n",
    "        finetuned_model_id=finetuned_model_id,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        validate_api_key=validate_api_key,\n",
    "        add_history=add_history,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        num_partitions=num_partitions,\n",
    "        feature_contributions=feature_contributions,\n",
    "    )\n",
    "\n",
    "def _detect_anomalies_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Union[int, float],\n",
    "    finetuned_model_id: Optional[str],\n",
    "    clean_ex_first: bool,\n",
    "    validate_api_key: bool,\n",
    "    date_features: Union[bool, list[str]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    ") -> pd.DataFrame:\n",
    "    return client.detect_anomalies(\n",
    "        df=df,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        finetuned_model_id=finetuned_model_id,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        validate_api_key=validate_api_key,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        num_partitions=num_partitions,\n",
    "    )\n",
    "\n",
    "def _detect_anomalies_online_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    h: _PositiveInt,\n",
    "    detection_size: _PositiveInt,\n",
    "    threshold_method: _ThresholdMethod,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Union[int, float],\n",
    "    clean_ex_first: bool,\n",
    "    step_size: _PositiveInt,\n",
    "    finetune_steps: _NonNegativeInt,\n",
    "    finetune_depth: _FinetuneDepth,\n",
    "    finetune_loss: _Loss,\n",
    "    hist_exog_list: Optional[list[str]],\n",
    "    date_features: Union[bool, list[str]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    refit: bool,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    ") -> pd.DataFrame:\n",
    "    return client.detect_anomalies_online(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        detection_size=detection_size,\n",
    "        threshold_method=threshold_method,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        step_size=step_size,\n",
    "        finetune_steps=finetune_steps,\n",
    "        finetune_depth=finetune_depth,\n",
    "        finetune_loss=finetune_loss,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        refit=refit,\n",
    "        num_partitions=num_partitions,\n",
    "    )\n",
    "\n",
    "def _cross_validation_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    h: _PositiveInt,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[list[Union[int, float]]],\n",
    "    quantiles: Optional[list[float]],\n",
    "    validate_api_key: bool,\n",
    "    n_windows: _PositiveInt,\n",
    "    step_size: Optional[_PositiveInt],\n",
    "    finetune_steps: _NonNegativeInt,\n",
    "    finetune_depth: _FinetuneDepth,\n",
    "    finetune_loss: _Loss,\n",
    "    finetuned_model_id: Optional[str],\n",
    "    refit: bool,\n",
    "    clean_ex_first: bool,\n",
    "    hist_exog_list: Optional[list[str]],\n",
    "    date_features: Union[bool, list[str]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    ") -> pd.DataFrame:\n",
    "    return client.cross_validation(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        quantiles=quantiles,\n",
    "        validate_api_key=validate_api_key,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "        finetune_steps=finetune_steps,\n",
    "        finetune_depth=finetune_depth,\n",
    "        finetune_loss=finetune_loss,\n",
    "        finetuned_model_id=finetuned_model_id,\n",
    "        refit=refit,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        num_partitions=num_partitions,\n",
    "    )\n",
    "\n",
    "def _get_schema(\n",
    "    df: 'AnyDataFrame',\n",
    "    method: str,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[Union[int, float, list[Union[int, float]]]],\n",
    "    quantiles: Optional[list[float]],\n",
    ") -> 'triad.Schema':\n",
    "    import fugue.api as fa\n",
    "\n",
    "    base_cols = [id_col, time_col]\n",
    "    if method != 'forecast':\n",
    "        base_cols.append(target_col)\n",
    "    schema = fa.get_schema(df).extract(base_cols).copy()\n",
    "    schema.append('TimeGPT:double')\n",
    "    if method == 'detect_anomalies':\n",
    "        schema.append('anomaly:bool')\n",
    "    if method == 'detect_anomalies_online':\n",
    "        schema.append('anomaly:bool')\n",
    "        schema.append('anomaly_score:double')\n",
    "    elif method == 'cross_validation':\n",
    "        schema.append(('cutoff', schema[time_col].type))\n",
    "    if level is not None and quantiles is not None:\n",
    "        raise ValueError(\"You should provide `level` or `quantiles` but not both.\")\n",
    "    if level is not None:\n",
    "        if not isinstance(level, list):\n",
    "            level = [level]\n",
    "        level = sorted(level)\n",
    "        schema.append(\",\".join(f\"TimeGPT-lo-{lv}:double\" for lv in reversed(level)))\n",
    "        schema.append(\",\".join(f\"TimeGPT-hi-{lv}:double\" for lv in level))\n",
    "    if quantiles is not None:\n",
    "        quantiles = sorted(quantiles)\n",
    "        q_cols = [f'TimeGPT-q-{int(q * 100)}:double' for q in quantiles]\n",
    "        schema.append(\",\".join(q_cols))\n",
    "    return schema\n",
    "\n",
    "def _distributed_setup(\n",
    "    df: 'AnyDataFrame',\n",
    "    method: str,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[Union[int, float, list[Union[int, float]]]],\n",
    "    quantiles: Optional[list[float]],\n",
    "    num_partitions: Optional[int],\n",
    ") -> tuple['triad.Schema', dict[str, Any]]:\n",
    "    from fugue.execution import infer_execution_engine\n",
    "\n",
    "    if infer_execution_engine([df]) is None:\n",
    "        raise ValueError(\n",
    "            f'Could not infer execution engine for type {type(df).__name__}. '\n",
    "            'Expected a spark or dask DataFrame or a ray Dataset.'\n",
    "        )\n",
    "    schema = _get_schema(\n",
    "        df=df,\n",
    "        method=method,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        quantiles=quantiles,\n",
    "    )\n",
    "    partition_config: dict[str, Any] = dict(by=id_col, algo='coarse')\n",
    "    if num_partitions is not None:\n",
    "        partition_config['num'] = num_partitions\n",
    "    return schema, partition_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "@contextmanager\n",
    "def delete_env_var(key):\n",
    "    original_value = os.environ.get(key)\n",
    "    rm = False\n",
    "    if key in os.environ:\n",
    "        del os.environ[key]\n",
    "        rm = True\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if rm:\n",
    "            os.environ[key] = original_value\n",
    "# test api_key fail\n",
    "with delete_env_var('NIXTLA_API_KEY'), delete_env_var('TIMEGPT_TOKEN'):\n",
    "    test_fail(\n",
    "        lambda: NixtlaClient(),\n",
    "        contains='NIXTLA_API_KEY',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nixtla_client = NixtlaClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# custom client\n",
    "custom_client = NixtlaClient(\n",
    "    base_url=os.environ['NIXTLA_BASE_URL_CUSTOM'],\n",
    "    api_key=os.environ['NIXTLA_API_KEY_CUSTOM'],\n",
    ")\n",
    "assert custom_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# usage endpoint\n",
    "usage = custom_client.usage()\n",
    "assert sorted(usage.keys()) == ['minute', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# finetuning\n",
    "h = 5\n",
    "series = generate_series(10, equal_ends=True)\n",
    "train_end = series['ds'].max() - h * pd.offsets.Day()\n",
    "train_mask = series['ds'] <= train_end\n",
    "train = series[train_mask]\n",
    "valid = series[~train_mask]\n",
    "model_id1 = str(uuid.uuid4())\n",
    "finetune_resp = custom_client.finetune(train, output_model_id=model_id1)\n",
    "assert finetune_resp == model_id1\n",
    "model_id2 = custom_client.finetune(train, finetuned_model_id=model_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# forecast with fine-tuned models\n",
    "fcst_base = custom_client.forecast(train, h=h)\n",
    "fcst1 = custom_client.forecast(train, h=h, finetuned_model_id=model_id1)\n",
    "fcst2 = custom_client.forecast(train, h=h, finetuned_model_id=model_id2)\n",
    "all_fcsts = fcst_base.assign(ten_rounds=fcst1['TimeGPT'], twenty_rounds=fcst2['TimeGPT'])\n",
    "fcst_rmse = evaluate(\n",
    "    all_fcsts.merge(valid),\n",
    "    metrics=[rmse],\n",
    "    agg_fn='mean',\n",
    ").loc[0]\n",
    "# error was reduced over 40% by finetuning\n",
    "assert 1 - fcst_rmse['ten_rounds'] / fcst_rmse['TimeGPT'] > 0.4\n",
    "# error was reduced over 30% by further finetuning\n",
    "assert 1 - fcst_rmse['twenty_rounds'] / fcst_rmse['ten_rounds'] > 0.3\n",
    "\n",
    "# non-existent model returns 404\n",
    "try:\n",
    "    custom_client.forecast(train, h=5, finetuned_model_id='unexisting')\n",
    "except ApiError as e:\n",
    "    assert e.status_code == 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cv with fine-tuned model\n",
    "cv_base = custom_client.cross_validation(series, n_windows=2, h=h)\n",
    "cv_finetune = custom_client.cross_validation(series, n_windows=2, h=h, finetuned_model_id=model_id1)\n",
    "all_fcsts = fcst_base.assign(ten_rounds=fcst1['TimeGPT'], twenty_rounds=fcst2['TimeGPT'])\n",
    "cv_rmse = evaluate(\n",
    "    cv_base.merge(\n",
    "        cv_finetune,\n",
    "        on=['unique_id', 'ds', 'cutoff', 'y'],\n",
    "        suffixes=('_base', '_finetune')\n",
    "    ).drop(columns='cutoff'),\n",
    "    metrics=[rmse],\n",
    "    agg_fn='mean',\n",
    ").loc[0]\n",
    "# error was reduced over 40% by finetuning\n",
    "assert 1 - cv_rmse['TimeGPT_finetune'] / cv_rmse['TimeGPT_base'] > 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# delete finetuned model\n",
    "custom_client.delete_finetuned_model(model_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# anomaly detection with fine-tuned model\n",
    "train_anomalies = train.copy()\n",
    "anomaly_date = train_end - 2 * pd.offsets.Day()\n",
    "train_anomalies.loc[train['ds'] == anomaly_date, 'y'] *= 2\n",
    "anomaly_base = custom_client.detect_anomalies(train_anomalies)\n",
    "anomaly_finetune = custom_client.detect_anomalies(train_anomalies, finetuned_model_id=model_id2)\n",
    "detected_anomalies_base = anomaly_base.set_index('ds').loc[anomaly_date, 'anomaly'].sum()\n",
    "detected_anomalies_finetune = anomaly_finetune.set_index('ds').loc[anomaly_date, 'anomaly'].sum()\n",
    "assert detected_anomalies_base < detected_anomalies_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# list finetuned models\n",
    "models = custom_client.finetuned_models()\n",
    "ids = {m.id for m in models}\n",
    "assert model_id1 not in ids and model_id2 in ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test compression\n",
    "captured_request = None\n",
    "\n",
    "class CapturingClient(httpx.Client):  \n",
    "    def post(self, *args, **kwargs):\n",
    "        request = self.build_request('POST', *args, **kwargs)\n",
    "        global captured_request\n",
    "        captured_request = {\n",
    "            'headers': dict(request.headers),\n",
    "            'content': request.content,\n",
    "            'method': request.method,\n",
    "            'url': str(request.url)\n",
    "        }\n",
    "        return super().post(*args, **kwargs)\n",
    "\n",
    "@contextmanager\n",
    "def capture_request():\n",
    "    original_client = httpx.Client\n",
    "    httpx.Client = CapturingClient\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        httpx.Client = original_client\n",
    "\n",
    "# this produces a 1MB payload\n",
    "series = generate_series(250, n_static_features=2)\n",
    "with capture_request():\n",
    "    nixtla_client.forecast(df=series, freq='D', h=1, hist_exog_list=['static_0', 'static_1'])\n",
    "\n",
    "assert captured_request['headers']['content-encoding'] == 'zstd'\n",
    "content = captured_request['content']\n",
    "assert len(content) < 2**20\n",
    "assert len(zstd.ZstdDecompressor().decompress(content)) > 2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# missing times\n",
    "series = generate_series(2, min_length=100, freq='5min')\n",
    "with_gaps = series.sample(frac=0.5, random_state=0)\n",
    "expected_msg = 'missing or duplicate timestamps, or the timestamps do not match'\n",
    "# gaps\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=with_gaps, h=1, freq='5min'),\n",
    "    contains=expected_msg,\n",
    ")\n",
    "# duplicates\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=pd.concat([series, series]), h=1, freq='5min'),\n",
    "    contains=expected_msg,\n",
    ")\n",
    "# wrong freq\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=series, h=1, freq='1min'),\n",
    "    contains=expected_msg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# historic exog in cv\n",
    "freq = 'D'\n",
    "h = 5\n",
    "series = generate_series(2, freq=freq)\n",
    "series_with_features, _ = fourier(series, freq=freq, season_length=7, k=2)\n",
    "splits = ufp.backtest_splits(\n",
    "    df=series_with_features,\n",
    "    n_windows=1,\n",
    "    h=h,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    freq=freq,\n",
    ")\n",
    "_, train, valid = next(splits)\n",
    "x_cols = train.columns.drop(['unique_id', 'ds', 'y']).tolist()\n",
    "for hist_exog_list in [None, [], [x_cols[2], x_cols[1]], x_cols]:\n",
    "    cv_res = nixtla_client.cross_validation(\n",
    "        series_with_features,\n",
    "        n_windows=1,\n",
    "        h=h,\n",
    "        freq=freq,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "    )\n",
    "    fcst_res = nixtla_client.forecast(\n",
    "        train,\n",
    "        h=h,\n",
    "        freq=freq,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        X_df=valid,\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        cv_res['TimeGPT'], fcst_res['TimeGPT'], atol=1e-4, rtol=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# different hist exog, different results\n",
    "for X_df in (None, valid):\n",
    "    res1 = nixtla_client.forecast(train, h=h, X_df=X_df, freq=freq, hist_exog_list=x_cols[:2])\n",
    "    res2 = nixtla_client.forecast(train, h=h, X_df=X_df, freq=freq, hist_exog_list=x_cols[2:])\n",
    "    np.testing.assert_raises(\n",
    "        AssertionError,\n",
    "        np.testing.assert_allclose,\n",
    "        res1['TimeGPT'],\n",
    "        res2['TimeGPT'],\n",
    "        atol=1e-4,\n",
    "        rtol=1e-3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# custom freq\n",
    "client = NixtlaClient()\n",
    "custom_business_hours = pd.tseries.offsets.CustomBusinessHour(\n",
    "    start='09:00',\n",
    "    end='16:00',\n",
    "    holidays=[\n",
    "        '2022-12-25',  # Christmas\n",
    "        '2022-01-01',   # New Year's Day\n",
    "    ]\n",
    ")\n",
    "series = pd.DataFrame({\n",
    "    'unique_id': 1,\n",
    "    'ds': pd.date_range(start='2000-01-03 09', freq=custom_business_hours, periods=200),\n",
    "    'y': np.arange(200) % 7,\n",
    "})\n",
    "series = pd.concat([series.assign(unique_id=i) for i in range(10)]).reset_index(drop=True)\n",
    "client.detect_anomalies(df=series, freq=custom_business_hours, level=90)\n",
    "client.cross_validation(df=series, freq=custom_business_hours, h=7)\n",
    "fcst = client.forecast(df=series, freq=custom_business_hours, h=7)\n",
    "assert sorted(fcst['ds'].dt.hour.unique().tolist()) == list(range(9, 16))\n",
    "assert [(model, freq.lower()) for (model, freq) in client._model_params.keys()] == [('timegpt-1', 'cbh')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# integer freq\n",
    "client = NixtlaClient()\n",
    "series = generate_series(5, freq='H', min_length=200)\n",
    "series['ds'] = series.groupby('unique_id', observed=True)['ds'].cumcount()\n",
    "client.detect_anomalies(df=series, level=90, freq=1)\n",
    "client.cross_validation(df=series, h=7, freq=1)\n",
    "fcst = client.forecast(df=series, h=7, freq=1)\n",
    "train_ends = series.groupby('unique_id', observed=True)['ds'].max()\n",
    "fcst_ends = fcst.groupby('unique_id', observed=True)['ds'].max()\n",
    "pd.testing.assert_series_equal(fcst_ends, train_ends + 7)\n",
    "assert list(client._model_params.keys()) == [('timegpt-1', 'MS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    lambda: NixtlaClient(api_key='transphobic').forecast(df=pd.DataFrame(), h=None, validate_api_key=True),\n",
    "    contains='nixtla'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test input_size\n",
    "test_eq(\n",
    "    nixtla_client._get_model_params(model='timegpt-1', freq='D'),\n",
    "    (28, 7),\n",
    ")\n",
    "test_eq(\n",
    "    custom_client._get_model_params(model='timegpt-1', freq='D'),\n",
    "    (28, 7),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to make forecasts! Let's import an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv',\n",
    "    parse_dates=['timestamp'],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test date_features with multiple series\n",
    "# and different ends\n",
    "test_series = generate_series(n_series=2, min_length=5, max_length=20)\n",
    "h = 12\n",
    "fcst_test_series = nixtla_client.forecast(test_series, h=12, date_features=['dayofweek'])\n",
    "uids = test_series['unique_id']\n",
    "for uid in uids:\n",
    "    test_eq(\n",
    "        fcst_test_series.query('unique_id == @uid')['ds'].tolist(),\n",
    "        pd.date_range(periods=h + 1, start=test_series.query('unique_id == @uid')['ds'].max())[1:].tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cv refit\n",
    "cv_kwargs = dict(\n",
    "    df=df,\n",
    "    n_windows=2,\n",
    "    h=12,\n",
    "    freq='MS',\n",
    "    time_col='timestamp',\n",
    "    target_col='value',\n",
    "    finetune_steps=2,\n",
    ")\n",
    "res_refit = nixtla_client.cross_validation(refit=True, **cv_kwargs)\n",
    "res_no_refit = nixtla_client.cross_validation(refit=False, **cv_kwargs)\n",
    "np.testing.assert_allclose(res_refit['value'], res_no_refit['value'])\n",
    "np.testing.assert_raises(\n",
    "    AssertionError,\n",
    "    np.testing.assert_allclose,\n",
    "    res_refit['TimeGPT'],\n",
    "    res_no_refit['TimeGPT'],\n",
    "    atol=1e-4,\n",
    "    rtol=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test quantiles\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=12, \n",
    "        time_col='timestamp', \n",
    "        target_col='value', \n",
    "        level=[80], \n",
    "        quantiles=[0.2, 0.3]\n",
    "    ),\n",
    "    contains='not both'\n",
    ")\n",
    "test_qls = list(np.arange(0.1, 1, 0.1))\n",
    "exp_q_cols = [f\"TimeGPT-q-{int(100 * q)}\" for q in test_qls]\n",
    "def test_method_qls(method, **kwargs):\n",
    "    df_qls = method(\n",
    "        df=df, \n",
    "        h=12, \n",
    "        time_col='timestamp', \n",
    "        target_col='value', \n",
    "        quantiles=test_qls,\n",
    "        **kwargs\n",
    "    )\n",
    "    assert all(col in df_qls.columns for col in exp_q_cols)\n",
    "    assert not any('-lo-' in col for col in df_qls.columns)\n",
    "    # test monotonicity of quantiles\n",
    "    for c1, c2 in zip(exp_q_cols[:-1], exp_q_cols[1:]):\n",
    "        assert df_qls[c1].lt(df_qls[c2]).all()\n",
    "test_method_qls(nixtla_client.forecast)\n",
    "test_method_qls(nixtla_client.forecast, add_history=True)\n",
    "test_method_qls(nixtla_client.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num partitions\n",
    "# we need to be sure that we can recover the same results\n",
    "# using a for loop\n",
    "# A: be aware that num partitons can produce different results\n",
    "# when used finetune_steps\n",
    "def test_num_partitions_same_results(method: Callable, num_partitions: int, **kwargs):\n",
    "    res_partitioned = method(**kwargs, num_partitions=num_partitions)\n",
    "    res_no_partitioned = method(**kwargs, num_partitions=1)\n",
    "    sort_by = ['unique_id', 'ds']\n",
    "    if 'cutoff' in res_partitioned:\n",
    "        sort_by.extend(['cutoff'])\n",
    "    pd.testing.assert_frame_equal(\n",
    "        res_partitioned.sort_values(sort_by).reset_index(drop=True), \n",
    "        res_no_partitioned.sort_values(sort_by).reset_index(drop=True),\n",
    "        rtol=1e-2,\n",
    "        atol=1e-2,\n",
    "    )\n",
    "\n",
    "freqs = {'D': 7, 'W-THU': 52, 'Q-DEC': 8, '15T': 4 * 24 * 7}\n",
    "for freq, h in freqs.items():\n",
    "    df_freq = generate_series(\n",
    "        10, \n",
    "        min_length=500 if freq != '15T' else 1_200, \n",
    "        max_length=550 if freq != '15T' else 2_000,\n",
    "    )\n",
    "    #df_freq['y'] = df_freq['y'].astype(np.float32)\n",
    "    df_freq['ds'] = df_freq.groupby('unique_id', observed=True)['ds'].transform(\n",
    "        lambda x: pd.date_range(periods=len(x), freq=freq, end='2023-01-01')\n",
    "    )\n",
    "    min_size = df_freq.groupby('unique_id', observed=True).size().min()\n",
    "    test_num_partitions_same_results(\n",
    "        nixtla_client.detect_anomalies,\n",
    "        level=98,\n",
    "        df=df_freq,\n",
    "        num_partitions=2,\n",
    "    )\n",
    "    test_num_partitions_same_results(\n",
    "        nixtla_client.cross_validation,\n",
    "        h=7,\n",
    "        n_windows=2,\n",
    "        df=df_freq,\n",
    "        num_partitions=2,\n",
    "    )\n",
    "    test_num_partitions_same_results(\n",
    "        nixtla_client.forecast,\n",
    "        df=df_freq,\n",
    "        h=7,\n",
    "        add_history=True,\n",
    "        num_partitions=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_retry_behavior(side_effect, max_retries=5, retry_interval=5, max_wait_time=40, should_retry=True, sleep_seconds=5):\n",
    "    mock_nixtla_client = NixtlaClient(\n",
    "        max_retries=max_retries, \n",
    "        retry_interval=retry_interval, \n",
    "        max_wait_time=max_wait_time,\n",
    "    )\n",
    "    mock_nixtla_client._make_request = side_effect\n",
    "    init_time = time.time()\n",
    "    test_fail(\n",
    "        lambda: mock_nixtla_client.forecast(df=df, h=12, time_col='timestamp', target_col='value'),\n",
    "    )\n",
    "    total_mock_time = time.time() - init_time\n",
    "    if should_retry:\n",
    "        approx_expected_time = min((max_retries - 1) * retry_interval, max_wait_time)\n",
    "        upper_expected_time = min(max_retries * retry_interval, max_wait_time)\n",
    "        assert total_mock_time >= approx_expected_time, \"It is not retrying as expected\"\n",
    "        # preprocessing time before the first api call should be less than 60 seconds\n",
    "        assert total_mock_time - upper_expected_time - (max_retries - 1) * sleep_seconds <= sleep_seconds\n",
    "    else:\n",
    "        assert total_mock_time <= max_wait_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we want the api to retry in these cases\n",
    "def raise_api_error_with_text(*args, **kwargs):\n",
    "    raise ApiError(\n",
    "        status_code=503, \n",
    "        body=\"\"\"\n",
    "        <html><head>\n",
    "        <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
    "        <title>503 Server Error</title>\n",
    "        </head>\n",
    "        <body text=#000000 bgcolor=#ffffff>\n",
    "        <h1>Error: Server Error</h1>\n",
    "        <h2>The service you requested is not available at this time.<p>Service error -27.</h2>\n",
    "        <h2></h2>\n",
    "        </body></html>\n",
    "        \"\"\")\n",
    "test_retry_behavior(raise_api_error_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we want the api to not retry in these cases\n",
    "# here A is assuming that the endpoint responds always\n",
    "# with a json\n",
    "def raise_api_error_with_json(*args, **kwargs):\n",
    "    raise ApiError(\n",
    "        status_code=422, \n",
    "        body=dict(detail='Please use numbers'),\n",
    "    )\n",
    "test_retry_behavior(raise_api_error_with_json, should_retry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test resilience of api calls\n",
    "def raise_read_timeout_error(*args, **kwargs):\n",
    "    sleep_seconds = 5\n",
    "    print(f'raising ReadTimeout error after {sleep_seconds} seconds')\n",
    "    time.sleep(sleep_seconds)\n",
    "    raise httpx.ReadTimeout('Timed out')\n",
    "\n",
    "def raise_http_error(*args, **kwargs):\n",
    "    print('raising HTTP error')\n",
    "    raise ApiError(status_code=503, body='HTTP error')\n",
    "    \n",
    "combs = [\n",
    "    (2, 5, 30),\n",
    "    (10, 1, 5),\n",
    "]\n",
    "side_effects = [raise_read_timeout_error, raise_http_error]\n",
    "\n",
    "for (max_retries, retry_interval, max_wait_time), side_effect in product(combs, side_effects):\n",
    "    test_retry_behavior(\n",
    "        max_retries=max_retries, \n",
    "        retry_interval=retry_interval, \n",
    "        max_wait_time=max_wait_time, \n",
    "        side_effect=side_effect,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nixtla_client.plot(df, time_col='timestamp', target_col='value', engine='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we recover the same <mean> forecasts\n",
    "# with and without restricting input\n",
    "# (add_history)\n",
    "def test_equal_fcsts_add_history(**kwargs):\n",
    "    fcst_no_rest_df = nixtla_client.forecast(**kwargs, add_history=True)\n",
    "    fcst_no_rest_df = fcst_no_rest_df.groupby('unique_id', observed=True).tail(kwargs['h']).reset_index(drop=True)\n",
    "    fcst_rest_df = nixtla_client.forecast(**kwargs)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_no_rest_df,\n",
    "        fcst_rest_df,\n",
    "        atol=1e-4,\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "    return fcst_rest_df\n",
    "\n",
    "freqs = {'D': 7, 'W-THU': 52, 'Q-DEC': 8, '15T': 4 * 24 * 7}\n",
    "for freq, h in freqs.items():\n",
    "    df_freq = generate_series(\n",
    "        10, \n",
    "        min_length=500 if freq != '15T' else 1_200, \n",
    "        max_length=550 if freq != '15T' else 2_000,\n",
    "    )\n",
    "    df_freq['ds'] = df_freq.groupby('unique_id', observed=True)['ds'].transform(\n",
    "        lambda x: pd.date_range(periods=len(x), freq=freq, end='2023-01-01')\n",
    "    )\n",
    "    kwargs = dict(\n",
    "        df=df_freq,\n",
    "        h=h,\n",
    "    )\n",
    "    fcst_1_df = test_equal_fcsts_add_history(**{**kwargs, 'model': 'timegpt-1'})\n",
    "    fcst_2_df = test_equal_fcsts_add_history(**{**kwargs, 'model': 'timegpt-1-long-horizon'})\n",
    "    test_fail(\n",
    "        lambda: pd.testing.assert_frame_equal(fcst_1_df, fcst_2_df),\n",
    "        contains='(column name=\"TimeGPT\") are different',\n",
    "    )\n",
    "    # add test num_partitions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test different results for different models\n",
    "fcst_kwargs = dict(\n",
    "    df=df,\n",
    "    h=12,\n",
    "    level=[90, 95],\n",
    "    add_history=True,\n",
    "    time_col='timestamp',\n",
    "    target_col='value',\n",
    ")\n",
    "fcst_kwargs['model'] = 'timegpt-1'\n",
    "fcst_timegpt_1 = nixtla_client.forecast(**fcst_kwargs)\n",
    "fcst_kwargs['model'] = 'timegpt-1-long-horizon'\n",
    "fcst_timegpt_long = nixtla_client.forecast(**fcst_kwargs)\n",
    "test_fail(\n",
    "    lambda: pd.testing.assert_frame_equal(fcst_timegpt_1[['TimeGPT']], fcst_timegpt_long[['TimeGPT']]),\n",
    "    contains='(column name=\"TimeGPT\") are different'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test different results for different models\n",
    "# cross validation\n",
    "cv_kwargs = dict(\n",
    "    df=df,\n",
    "    h=12,\n",
    "    level=[90, 95],\n",
    "    time_col='timestamp',\n",
    "    target_col='value',\n",
    ")\n",
    "cv_kwargs['model'] = 'timegpt-1'\n",
    "cv_timegpt_1 = nixtla_client.cross_validation(**cv_kwargs)\n",
    "cv_kwargs['model'] = 'timegpt-1-long-horizon'\n",
    "cv_timegpt_long = nixtla_client.cross_validation(**cv_kwargs)\n",
    "test_fail(\n",
    "    lambda: pd.testing.assert_frame_equal(cv_timegpt_1[['TimeGPT']], cv_timegpt_long[['TimeGPT']]),\n",
    "    contains='(column name=\"TimeGPT\") are different'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test different results for different models\n",
    "# anomalies\n",
    "anomalies_kwargs = dict(\n",
    "    df=df,\n",
    "    level=99,\n",
    "    time_col='timestamp',\n",
    "    target_col='value',\n",
    ")\n",
    "anomalies_kwargs['model'] = 'timegpt-1'\n",
    "anomalies_timegpt_1 = nixtla_client.detect_anomalies(**anomalies_kwargs)\n",
    "anomalies_kwargs['model'] = 'timegpt-1-long-horizon'\n",
    "anomalies_timegpt_long = nixtla_client.detect_anomalies(**anomalies_kwargs)\n",
    "test_fail(\n",
    "    lambda: pd.testing.assert_frame_equal(anomalies_timegpt_1[['TimeGPT']], anomalies_timegpt_long[['TimeGPT']]),\n",
    "    contains='(column name=\"TimeGPT\") are different'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unsupported model\n",
    "fcst_kwargs['model'] = 'a-model'\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(**fcst_kwargs),\n",
    "    contains='unsupported model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unsupported model\n",
    "anomalies_kwargs['model'] = 'my-awesome-model'\n",
    "test_fail(\n",
    "    lambda: nixtla_client.detect_anomalies(**anomalies_kwargs),\n",
    "    contains='unsupported model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features\n",
    "df_ = df.rename(columns={'timestamp': 'ds', 'value': 'y'})\n",
    "df_.insert(0, 'unique_id', 'AirPassengers')\n",
    "date_features = ['year', 'month']\n",
    "df_date_features, future_df = _maybe_add_date_features(\n",
    "    df=df_,\n",
    "    X_df=None,\n",
    "    h=12, \n",
    "    freq='MS', \n",
    "    features=date_features,\n",
    "    one_hot=False,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "assert all(col in df_date_features for col in date_features)\n",
    "assert all(col in future_df for col in date_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test shap values are returned and sum to predictions\n",
    "h=12\n",
    "fcst_df = nixtla_client.forecast(df=df_date_features, h=h, X_df=future_df, feature_contributions=True)\n",
    "shap_values = nixtla_client.feature_contributions\n",
    "assert len(shap_values) == len(fcst_df)\n",
    "np.testing.assert_allclose(fcst_df[\"TimeGPT\"].values, shap_values.iloc[:, 3:].sum(axis=1).values)\n",
    "\n",
    "fcst_hist_df = nixtla_client.forecast(df=df_date_features, h=h, X_df=future_df, add_history=True, feature_contributions=True)\n",
    "shap_values_hist = nixtla_client.feature_contributions\n",
    "assert len(shap_values_hist) == len(fcst_hist_df)\n",
    "np.testing.assert_allclose(fcst_hist_df[\"TimeGPT\"].values, shap_values_hist.iloc[:, 3:].sum(axis=1).values, atol=1e-4)\n",
    "\n",
    "# test num partitions\n",
    "_ = nixtla_client.forecast(df=df_date_features, h=h, X_df=future_df, add_history=True, feature_contributions=True, num_partitions=2)\n",
    "pd.testing.assert_frame_equal(nixtla_client.feature_contributions, shap_values_hist, atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cross validation tests\n",
    "df_copy = df_.copy()\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_copy,\n",
    "    df_,\n",
    ")\n",
    "df_test = df_.groupby('unique_id').tail(12)\n",
    "df_train = df_.drop(df_test.index)\n",
    "hyps = [\n",
    "    # finetune steps is unstable due\n",
    "    # to numerical reasons\n",
    "    # dict(finetune_steps=2),\n",
    "    dict(),\n",
    "    dict(clean_ex_first=False),\n",
    "    dict(date_features=['month']),\n",
    "    dict(level=[80, 90]),\n",
    "    #dict(level=[80, 90], finetune_steps=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test exogenous variables cv\n",
    "df_ex_ = df_.copy()\n",
    "df_ex_['exogenous_var'] = df_ex_['y'] + np.random.normal(size=len(df_ex_))\n",
    "x_df_test = df_test.drop(columns='y').merge(df_ex_.drop(columns='y'))\n",
    "for hyp in hyps:\n",
    "    logger.info(f'Hyperparameters: {hyp}')\n",
    "    logger.info('\\n\\nPerforming forecast\\n')\n",
    "    fcst_test = nixtla_client.forecast(\n",
    "        df_train.merge(df_ex_.drop(columns='y')), h=12, X_df=x_df_test, **hyp\n",
    "    )\n",
    "    fcst_test = df_test[['unique_id', 'ds', 'y']].merge(fcst_test)\n",
    "    fcst_test = fcst_test.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    logger.info('\\n\\nPerforming Cross validation\\n')\n",
    "    fcst_cv = nixtla_client.cross_validation(df_ex_, h=12, **hyp)\n",
    "    fcst_cv = fcst_cv.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    logger.info('\\n\\nVerify difference\\n')\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "        atol=1e-4,\n",
    "        rtol=1e-3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test finetune cv\n",
    "finetune_cv = nixtla_client.cross_validation(\n",
    "            df=df_,\n",
    "            h=12,\n",
    "            n_windows=1,\n",
    "            finetune_steps=1\n",
    "        )\n",
    "test_eq(finetune_cv is not None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for hyp in hyps:\n",
    "    fcst_test = nixtla_client.forecast(df_train, h=12, **hyp)\n",
    "    fcst_test = df_test[['unique_id', 'ds', 'y']].merge(fcst_test)\n",
    "    fcst_test = fcst_test.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    fcst_cv = nixtla_client.cross_validation(df_, h=12, **hyp)\n",
    "    fcst_cv = fcst_cv.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "        rtol=1e-2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for hyp in hyps:\n",
    "    fcst_test = nixtla_client.forecast(df_train, h=12, **hyp)\n",
    "    fcst_test.insert(2, 'y', df_test['y'].values)\n",
    "    fcst_test = fcst_test.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    fcst_cv = nixtla_client.cross_validation(df_, h=12, **hyp)\n",
    "    fcst_cv = fcst_cv.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "        rtol=1e-2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add callables\n",
    "date_features = [SpecialDates({'first_dates': ['2021-01-1'], 'second_dates': ['2021-01-01']})]\n",
    "df_daily = df_.copy()\n",
    "df_daily['ds'] = pd.date_range(end='2021-01-01', periods=len(df_daily))\n",
    "df_date_features, future_df = _maybe_add_date_features(\n",
    "    df=df_,\n",
    "    X_df=None,\n",
    "    h=12, \n",
    "    freq='D', \n",
    "    features=date_features,\n",
    "    one_hot=False,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "assert all(col in df_date_features for col in ['first_dates', 'second_dates'])\n",
    "assert all(col in future_df for col in ['first_dates', 'second_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features one hot encoded\n",
    "date_features = ['year', 'month']\n",
    "date_features_to_one_hot = ['month']\n",
    "df_date_features, future_df = _maybe_add_date_features(\n",
    "    df=df_,\n",
    "    X_df=None,\n",
    "    h=12, \n",
    "    freq='D', \n",
    "    features=date_features,\n",
    "    one_hot=date_features_to_one_hot,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test pass dataframe with index\n",
    "df_ds_index = df_.set_index('ds')[['unique_id', 'y']]\n",
    "df_ds_index.index = pd.DatetimeIndex(df_ds_index.index)\n",
    "fcst_inferred_df_index = nixtla_client.forecast(df_ds_index, h=10)\n",
    "anom_inferred_df_index = nixtla_client.detect_anomalies(df_ds_index)\n",
    "fcst_inferred_df = nixtla_client.forecast(df_[['ds', 'unique_id', 'y']], h=10)\n",
    "anom_inferred_df = nixtla_client.detect_anomalies(df_[['ds', 'unique_id', 'y']])\n",
    "pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df, atol=1e-4, rtol=1e-3)\n",
    "pd.testing.assert_frame_equal(anom_inferred_df_index, anom_inferred_df, atol=1e-4, rtol=1e-3)\n",
    "df_ds_index = df_ds_index.groupby('unique_id').tail(80)\n",
    "for freq in ['Y', 'W-MON', 'Q-DEC', 'H']:\n",
    "    df_ds_index.index = np.concatenate(\n",
    "        df_ds_index['unique_id'].nunique() * [pd.date_range(end='2023-01-01', periods=80, freq=freq)]\n",
    "    )\n",
    "    df_ds_index.index.name = 'ds'\n",
    "    fcst_inferred_df_index = nixtla_client.forecast(df_ds_index, h=10)\n",
    "    df_test = df_ds_index.reset_index()\n",
    "    fcst_inferred_df = nixtla_client.forecast(df_test, h=10)\n",
    "    pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df, atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features with exogenous variables \n",
    "# and multiple series\n",
    "date_features = ['year', 'month']\n",
    "df_actual_future = df_.tail(12)[['unique_id', 'ds']]\n",
    "df_date_features, future_df = _maybe_add_date_features(\n",
    "    df=df_,\n",
    "    X_df=df_actual_future,\n",
    "    h=24, \n",
    "    freq='H', \n",
    "    features=date_features,\n",
    "    one_hot=False,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "assert all(col in df_date_features for col in date_features)\n",
    "assert all(col in future_df for col in date_features)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_date_features[df_.columns],\n",
    "    df_,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    future_df[df_actual_future.columns],\n",
    "    df_actual_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features one hot with exogenous variables \n",
    "# and multiple series\n",
    "date_features = ['month', 'day']\n",
    "df_date_features, future_df = _maybe_add_date_features(\n",
    "    df=df_,\n",
    "    X_df=df_actual_future,\n",
    "    h=24, \n",
    "    freq='H', \n",
    "    features=date_features,\n",
    "    one_hot=date_features,\n",
    "    id_col='unique_id',\n",
    "    time_col='ds',\n",
    "    target_col='y',\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_date_features[df_.columns],\n",
    "    df_,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    future_df[df_actual_future.columns],\n",
    "    df_actual_future.reset_index(drop=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test warning horizon too long\n",
    "nixtla_client.forecast(df=df.tail(3), h=100, time_col='timestamp', target_col='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with add_history\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', add_history=True),\n",
    "    contains='make sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with finetunning\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', finetune_steps=10, finetune_loss='mae'),\n",
    "    contains='make sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with level\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', level=[80, 90]),\n",
    "    contains='make sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test using index as time_col\n",
    "# same results\n",
    "df_test = df.copy()\n",
    "df_test[\"timestamp\"] = pd.to_datetime(df_test[\"timestamp\"])\n",
    "df_test.set_index(df_test[\"timestamp\"], inplace=True)\n",
    "df_test.drop(columns=\"timestamp\", inplace=True)\n",
    "\n",
    "# Using user_provided time_col and freq\n",
    "timegpt_anomalies_df_1 = nixtla_client.detect_anomalies(df, time_col='timestamp', target_col='value', freq= 'M')\n",
    "# Infer time_col and freq from index\n",
    "timegpt_anomalies_df_2 = nixtla_client.detect_anomalies(df_test, time_col='timestamp', target_col='value')\n",
    "\n",
    "pd.testing.assert_frame_equal(\n",
    "    timegpt_anomalies_df_1,\n",
    "    timegpt_anomalies_df_2,\n",
    "    atol=1e-4,\n",
    "    rtol=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test large requests raise error and suggest partition number\n",
    "df = generate_series(20_000, min_length=1_000, max_length=1_000, freq='min')\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df, h=1, freq='min', finetune_steps=2),\n",
    "    contains=\"num_partitions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# future and historic exogs\n",
    "df = generate_series(n_series=2, min_length=5, max_length=20)\n",
    "train, future = time_features(df, freq='D', features=['year', 'month'], h=5)\n",
    "\n",
    "# features in df but not in X_df\n",
    "missing_exogenous = train.columns.drop(['unique_id', 'ds', 'y']).tolist()\n",
    "expected_warning = (\n",
    "    f'`df` contains the following exogenous features: {missing_exogenous}, '\n",
    "    'but `X_df` was not provided and they were not declared in `hist_exog_list`. '\n",
    "    'They will be ignored.'\n",
    ")\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    forecasts = nixtla_client.forecast(train, h=5)\n",
    "    assert any(expected_warning in str(warning.message) for warning in w)\n",
    "\n",
    "# features in df not set as historic nor in X_df\n",
    "expected_warning = (\n",
    "    f\"`df` contains the following exogenous features: ['month'], \"\n",
    "    'but they were not found in `X_df` nor declared in `hist_exog_list`. '\n",
    "    'They will be ignored.'\n",
    ")\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    forecasts = nixtla_client.forecast(train, h=5, X_df=future[['unique_id', 'ds', 'year']])\n",
    "    assert any(expected_warning in str(warning.message) for warning in w)\n",
    "\n",
    "# features in X_df not in df\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(\n",
    "        train[['unique_id', 'ds', 'y']],\n",
    "        h=5,\n",
    "        X_df=future,\n",
    "    ),\n",
    "    contains='features are present in `X_df` but not in `df`'\n",
    ")\n",
    "\n",
    "# test setting one as historic and other as future\n",
    "nixtla_client.forecast(train, h=5, X_df=future[['unique_id', 'ds', 'year']], hist_exog_list=['month'])\n",
    "test_eq(nixtla_client.weights_x['features'].tolist(), ['year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test real-time anomaly detection\n",
    "detection_size = 5\n",
    "n_series = 2\n",
    "size = 100\n",
    "\n",
    "ds = pd.date_range(start='2023-01-01', periods=size, freq='W')\n",
    "x = np.arange(size)\n",
    "y = 10 * np.sin(0.1 * x) + 12\n",
    "y = np.tile(y, n_series)\n",
    "y[size - 5] = 30\n",
    "y[2*size - 1] = 30\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'unique_id': np.repeat(np.arange(1, n_series + 1), size),\n",
    "    'ds': np.tile(ds, n_series),\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "anomaly_df = nixtla_client.detect_anomalies_online(\n",
    "    df, \n",
    "    h=20, \n",
    "    detection_size=detection_size, \n",
    "    threshold_method=\"univariate\", \n",
    "    freq='W-SUN', \n",
    "    level=99,\n",
    ")\n",
    "assert len(anomaly_df) == n_series * detection_size\n",
    "assert len(anomaly_df.columns) == 8 # [unique_id, ds, TimeGPT, y, anomaly, anomaly_score, hi, lo]\n",
    "assert anomaly_df['anomaly'].sum() == 2 \n",
    "assert anomaly_df['anomaly'].iloc[0] and anomaly_df['anomaly'].iloc[-1]\n",
    "\n",
    "multi_anomaly_df = nixtla_client.detect_anomalies_online(\n",
    "    df, \n",
    "    h=20, \n",
    "    detection_size=detection_size, \n",
    "    threshold_method=\"multivariate\", \n",
    "    freq='W-SUN', \n",
    "    level=99,\n",
    ")\n",
    "\n",
    "assert len(multi_anomaly_df) == n_series * detection_size\n",
    "assert len(multi_anomaly_df.columns) == 7 # [unique_id, ds, TimeGPT, y, anomaly, anomaly_score, accumulated_anomaly_score]\n",
    "assert multi_anomaly_df['anomaly'].sum() == 4\n",
    "assert (multi_anomaly_df['anomaly'].iloc[0] and \n",
    "        multi_anomaly_df['anomaly'].iloc[4] and\n",
    "        multi_anomaly_df['anomaly'].iloc[5] and\n",
    "        multi_anomaly_df['anomaly'].iloc[9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| distributed\n",
    "import dask.dataframe as dd\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "import ray\n",
    "from dask.distributed import Client\n",
    "from pyspark.sql import SparkSession\n",
    "from ray.cluster_utils import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| distributed\n",
    "ATOL = 1e-3\n",
    "\n",
    "def test_forecast(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    horizon: int = 12,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    **fcst_kwargs,\n",
    "):\n",
    "    fcst_df = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    test_eq(n_series * 12, len(fcst_df))\n",
    "    cols = fcst_df.columns.to_list()\n",
    "    exp_cols = [id_col, time_col, 'TimeGPT']\n",
    "    if 'level' in fcst_kwargs:\n",
    "        level = sorted(fcst_kwargs['level'])\n",
    "        exp_cols.extend([f'TimeGPT-lo-{lv}' for lv in reversed(level)])\n",
    "        exp_cols.extend([f'TimeGPT-hi-{lv}' for lv in level])\n",
    "    test_eq(cols, exp_cols)\n",
    "\n",
    "def test_forecast_diff_results_diff_models(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    horizon: int = 12, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    **fcst_kwargs,\n",
    "):\n",
    "    fcst_df = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        model='timegpt-1',\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        model='timegpt-1-long-horizon',\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    test_fail(\n",
    "        lambda: pd.testing.assert_frame_equal(\n",
    "            fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "            fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        ),\n",
    "        contains='(column name=\"TimeGPT\") are different',\n",
    "    )\n",
    "\n",
    "def test_forecast_same_results_num_partitions(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    horizon: int = 12, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    **fcst_kwargs,\n",
    "):\n",
    "    fcst_df = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        atol=ATOL,\n",
    "    )\n",
    "\n",
    "def test_cv_same_results_num_partitions(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    horizon: int = 12, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    **fcst_kwargs,\n",
    "):\n",
    "    fcst_df = nixtla_client.cross_validation(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = nixtla_client.cross_validation(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        atol=ATOL,\n",
    "    )\n",
    "\n",
    "def test_forecast_dataframe(df: fugue.AnyDataFrame):\n",
    "    test_cv_same_results_num_partitions(df, n_windows=2, step_size=1)\n",
    "    test_cv_same_results_num_partitions(df, n_windows=3, step_size=None, horizon=1)\n",
    "    test_cv_same_results_num_partitions(df, model='timegpt-1-long-horizon', horizon=1)\n",
    "    test_forecast_diff_results_diff_models(df)\n",
    "    test_forecast(df, num_partitions=1)\n",
    "    test_forecast(df, level=[90, 80], num_partitions=1)\n",
    "    test_forecast_same_results_num_partitions(df)\n",
    "\n",
    "def test_forecast_dataframe_diff_cols(\n",
    "    df: fugue.AnyDataFrame,\n",
    "    id_col: str = 'id_col',\n",
    "    time_col: str = 'time_col',\n",
    "    target_col: str = 'target_col',\n",
    "):\n",
    "    test_forecast(df, id_col=id_col, time_col=time_col, target_col=target_col, num_partitions=1)\n",
    "    test_forecast(\n",
    "        df, id_col=id_col, time_col=time_col, target_col=target_col, level=[90, 80], num_partitions=1\n",
    "    )\n",
    "    test_forecast_same_results_num_partitions(\n",
    "        df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "    )\n",
    "\n",
    "def test_forecast_x(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    X_df: fugue.AnyDataFrame,\n",
    "    horizon: int = 24,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    **fcst_kwargs,\n",
    "):\n",
    "    fcst_df = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        X_df=X_df,\n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    n_series = fa.as_pandas(X_df)[id_col].nunique()\n",
    "    test_eq(n_series * horizon, len(fcst_df))\n",
    "    cols = fcst_df.columns.to_list()\n",
    "    exp_cols = [id_col, time_col, 'TimeGPT']\n",
    "    if 'level' in fcst_kwargs:\n",
    "        level = sorted(fcst_kwargs['level'])\n",
    "        exp_cols.extend([f'TimeGPT-lo-{lv}' for lv in reversed(level)])\n",
    "        exp_cols.extend([f'TimeGPT-hi-{lv}' for lv in level])\n",
    "    test_eq(cols, exp_cols)\n",
    "    fcst_df_2 = nixtla_client.forecast(\n",
    "        df=df,\n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    equal_arrays = np.array_equal(\n",
    "        fcst_df.sort_values([id_col, time_col])['TimeGPT'].values,\n",
    "        fcst_df_2.sort_values([id_col, time_col])['TimeGPT'].values,\n",
    "    )\n",
    "    assert not equal_arrays, 'Forecasts with and without ex vars are equal'\n",
    "\n",
    "def test_forecast_x_same_results_num_partitions(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    X_df: fugue.AnyDataFrame,\n",
    "    horizon: int = 24, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    **fcst_kwargs,\n",
    "):\n",
    "    fcst_df = nixtla_client.forecast(\n",
    "        df=df, \n",
    "        X_df=X_df,\n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = nixtla_client.forecast(\n",
    "        df=df,\n",
    "        h=horizon,\n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    equal_arrays = np.array_equal(\n",
    "        fcst_df.sort_values([id_col, time_col])['TimeGPT'].values,\n",
    "        fcst_df_2.sort_values([id_col, time_col])['TimeGPT'].values,\n",
    "    )\n",
    "    assert not equal_arrays, 'Forecasts with and without ex vars are equal'\n",
    "\n",
    "def test_forecast_x_dataframe(df: fugue.AnyDataFrame, X_df: fugue.AnyDataFrame):\n",
    "    test_forecast_x(df, X_df, num_partitions=1)\n",
    "    test_forecast_x(df, X_df, level=[90, 80], num_partitions=1)\n",
    "    test_forecast_x_same_results_num_partitions(df, X_df)\n",
    "\n",
    "def test_forecast_x_dataframe_diff_cols(\n",
    "    df: fugue.AnyDataFrame,\n",
    "    X_df: fugue.AnyDataFrame,\n",
    "    id_col: str = 'id_col',\n",
    "    time_col: str = 'time_col',\n",
    "    target_col: str = 'target_col'\n",
    "):\n",
    "    test_forecast_x(\n",
    "        df, X_df, id_col=id_col, time_col=time_col, target_col=target_col, num_partitions=1\n",
    "    )\n",
    "    test_forecast_x(\n",
    "        df, X_df, id_col=id_col, time_col=time_col, target_col=target_col, level=[90, 80], num_partitions=1\n",
    "    )\n",
    "    test_forecast_x_same_results_num_partitions(\n",
    "        df, X_df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "    )\n",
    "\n",
    "def test_anomalies(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    **anomalies_kwargs,\n",
    "):\n",
    "    anomalies_df = nixtla_client.detect_anomalies(\n",
    "        df=df, \n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **anomalies_kwargs,\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    test_eq(fa.as_pandas(df)[id_col].unique(), anomalies_df[id_col].unique())\n",
    "    cols = anomalies_df.columns.to_list()\n",
    "    level = anomalies_kwargs.get('level', 99)\n",
    "    exp_cols = [\n",
    "        id_col,\n",
    "        time_col,\n",
    "        target_col,\n",
    "        'TimeGPT',\n",
    "        'anomaly',\n",
    "        f'TimeGPT-lo-{level}',\n",
    "        f'TimeGPT-hi-{level}',\n",
    "    ]\n",
    "    test_eq(cols, exp_cols)\n",
    "\n",
    "def test_anomalies_same_results_num_partitions(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    **anomalies_kwargs,\n",
    "):\n",
    "    anomalies_df = nixtla_client.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    anomalies_df_2 = nixtla_client.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df_2 = fa.as_pandas(anomalies_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        anomalies_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        anomalies_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        atol=ATOL,\n",
    "    )\n",
    "\n",
    "def test_online_anomalies(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    level=99,\n",
    "    **reatlime_anomalies_kwargs\n",
    "):\n",
    "    anomalies_df = nixtla_client.detect_anomalies_online(\n",
    "        df=df, \n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        **reatlime_anomalies_kwargs,\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    test_eq(fa.as_pandas(df)[id_col].unique(), anomalies_df[id_col].unique())\n",
    "    cols = anomalies_df.columns.to_list()\n",
    "    level = anomalies_kwargs.get('level', 99)\n",
    "    exp_cols = [\n",
    "        id_col,\n",
    "        time_col,\n",
    "        target_col,\n",
    "        'TimeGPT',\n",
    "        'anomaly',\n",
    "        'anomaly_score',\n",
    "        f'TimeGPT-lo-{level}',\n",
    "        f'TimeGPT-hi-{level}',\n",
    "    ]\n",
    "    test_eq(cols, exp_cols)\n",
    "\n",
    "def test_anomalies_online_same_results_num_partitions(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    **reatlime_anomalies_kwargs\n",
    "):\n",
    "    anomalies_df = nixtla_client.detect_anomalies_online(\n",
    "        df=df,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        num_partitions=1,\n",
    "        **reatlime_anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    anomalies_df_2 = nixtla_client.detect_anomalies_online(\n",
    "        df=df, \n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        num_partitions=2,\n",
    "        **reatlime_anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df_2 = fa.as_pandas(anomalies_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        anomalies_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        anomalies_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        atol=ATOL,\n",
    "    )\n",
    "\n",
    "def test_anomalies_diff_results_diff_models(\n",
    "    df: fugue.AnyDataFrame, \n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y',\n",
    "    **anomalies_kwargs,\n",
    "):\n",
    "    anomalies_df = nixtla_client.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        model='timegpt-1',\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    anomalies_df_2 = nixtla_client.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        model='timegpt-1-long-horizon',\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df_2 = fa.as_pandas(anomalies_df_2)\n",
    "    test_fail(\n",
    "        lambda: pd.testing.assert_frame_equal(\n",
    "            anomalies_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "            anomalies_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        ),\n",
    "        contains='(column name=\"TimeGPT\") are different',\n",
    "    )\n",
    "\n",
    "def test_anomalies_dataframe(df: fugue.AnyDataFrame):\n",
    "    test_anomalies(df, num_partitions=1)\n",
    "    test_anomalies(df, level=90, num_partitions=1)\n",
    "    test_anomalies_same_results_num_partitions(df)\n",
    "\n",
    "def test_anomalies_online_dataframe(df: fugue.AnyDataFrame):\n",
    "    test_online_anomalies(df, h=20, detection_size=5, threshold_method='univariate', level=99, num_partitions=1)\n",
    "    test_anomalies_online_same_results_num_partitions(df, h=20, detection_size=5, threshold_method='univariate', level=99)\n",
    "\n",
    "def test_anomalies_dataframe_diff_cols(\n",
    "    df: fugue.AnyDataFrame,\n",
    "    id_col: str = 'id_col',\n",
    "    time_col: str = 'time_col',\n",
    "    target_col: str = 'target_col',\n",
    "):\n",
    "    test_anomalies(df, id_col=id_col, time_col=time_col, target_col=target_col, num_partitions=1)\n",
    "    test_anomalies(df, id_col=id_col, time_col=time_col, target_col=target_col, level=90, num_partitions=1)\n",
    "    test_anomalies_same_results_num_partitions(df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "    # @A: document behavior with exogenous variables in distributed environments.  \n",
    "    #test_anomalies_same_results_num_partitions(df, id_col=id_col, time_col=time_col, date_features=True, clean_ex_first=False)\n",
    "\n",
    "def test_quantiles(df: fugue.AnyDataFrame, id_col: str = 'id_col', time_col: str = 'time_col'):\n",
    "    test_qls = list(np.arange(0.1, 1, 0.1))\n",
    "    exp_q_cols = [f\"TimeGPT-q-{int(q * 100)}\" for q in test_qls]\n",
    "    def test_method_qls(method, **kwargs):\n",
    "        df_qls = method(\n",
    "            df=df,\n",
    "            h=12,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            quantiles=test_qls,\n",
    "            **kwargs\n",
    "        )\n",
    "        df_qls = fa.as_pandas(df_qls)\n",
    "        assert all(col in df_qls.columns for col in exp_q_cols)\n",
    "        # test monotonicity of quantiles\n",
    "        df_qls.apply(lambda x: x.is_monotonic_increasing, axis=1).sum() == len(exp_q_cols)\n",
    "    test_method_qls(nixtla_client.forecast)\n",
    "    test_method_qls(nixtla_client.forecast, add_history=True)\n",
    "    test_method_qls(nixtla_client.cross_validation)\n",
    "\n",
    "\n",
    "def test_finetuned_model(df):\n",
    "    # fine-tuning on distributed fails\n",
    "    test_fail(\n",
    "        lambda: custom_client.finetune(df=df),\n",
    "        contains='Can only fine-tune on pandas or polars dataframes.'\n",
    "    )\n",
    "    \n",
    "    # forecast\n",
    "    local_fcst = custom_client.forecast(\n",
    "        df=fa.as_pandas(df), h=5, finetuned_model_id=model_id2\n",
    "    )\n",
    "    distr_fcst = fa.as_pandas(\n",
    "        custom_client.forecast(df=df, h=5, finetuned_model_id=model_id2)\n",
    "    ).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        local_fcst, \n",
    "        distr_fcst,\n",
    "        check_dtype=False,\n",
    "        atol=1e-4,\n",
    "        rtol=1e-2,\n",
    "    )\n",
    "\n",
    "    # cross-validation\n",
    "    local_cv = custom_client.cross_validation(\n",
    "        df=fa.as_pandas(df), n_windows=2, h=5, finetuned_model_id=model_id2\n",
    "    )\n",
    "    distr_cv = fa.as_pandas(\n",
    "        custom_client.cross_validation(df=df, n_windows=2, h=5, finetuned_model_id=model_id2)\n",
    "    ).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        local_cv,\n",
    "        distr_cv[local_cv.columns],\n",
    "        check_dtype=False,\n",
    "        atol=1e-4,\n",
    "        rtol=1e-2,\n",
    "    )\n",
    "\n",
    "    # anomaly detection\n",
    "    local_anomaly = custom_client.detect_anomalies(\n",
    "        df=fa.as_pandas(df), finetuned_model_id=model_id2\n",
    "    )\n",
    "    distr_anomaly = fa.as_pandas(\n",
    "        custom_client.detect_anomalies(df=df, finetuned_model_id=model_id2)\n",
    "    ).sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        local_anomaly, \n",
    "        distr_anomaly[local_anomaly.columns],\n",
    "        check_dtype=False,\n",
    "        atol=1e-3,\n",
    "        rtol=1e-2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| distributed\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series, min_length=100)\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "series_diff_cols = series.copy()\n",
    "renamer = {'unique_id': 'id_col', 'ds': 'time_col', 'y': 'target_col'}\n",
    "series_diff_cols = series_diff_cols.rename(columns=renamer)\n",
    "\n",
    "# data for exogenous tests\n",
    "df_x = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-with-ex-vars.csv',\n",
    "    parse_dates=['ds'],\n",
    ")\n",
    "df_x = df_x.rename(columns=str.lower)\n",
    "future_ex_vars_df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-future-ex-vars.csv',\n",
    "    parse_dates=['ds'],\n",
    ")\n",
    "future_ex_vars_df = future_ex_vars_df.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| distributed\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df = spark.createDataFrame(series).repartition(2)\n",
    "spark_diff_cols_df = spark.createDataFrame(series_diff_cols).repartition(2)\n",
    "\n",
    "test_quantiles(spark_df, id_col=\"unique_id\", time_col=\"ds\")\n",
    "\n",
    "test_forecast_dataframe(spark_df)\n",
    "test_forecast_dataframe_diff_cols(spark_diff_cols_df)\n",
    "test_anomalies_dataframe(spark_df)\n",
    "test_anomalies_online_dataframe(spark_df)\n",
    "test_anomalies_dataframe_diff_cols(spark_diff_cols_df)\n",
    "# test exogenous variables\n",
    "spark_df_x = spark.createDataFrame(df_x).repartition(2)\n",
    "spark_future_ex_vars_df = spark.createDataFrame(future_ex_vars_df).repartition(2)\n",
    "test_forecast_x_dataframe(spark_df_x, spark_future_ex_vars_df)\n",
    "# test x different cols\n",
    "spark_df_x_diff_cols = spark.createDataFrame(df_x.rename(columns=renamer)).repartition(2)\n",
    "spark_future_ex_vars_df_diff_cols = spark.createDataFrame(\n",
    "    future_ex_vars_df.rename(columns=renamer)\n",
    ").repartition(2)\n",
    "test_forecast_x_dataframe_diff_cols(spark_df_x_diff_cols, spark_future_ex_vars_df_diff_cols)\n",
    "\n",
    "# test finetuning\n",
    "test_finetuned_model(spark_df)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| distributed\n",
    "client = Client()\n",
    "dask_df = dd.from_pandas(series, npartitions=2)\n",
    "dask_diff_cols_df = dd.from_pandas(series_diff_cols, npartitions=2)\n",
    "\n",
    "\n",
    "test_quantiles(dask_df, id_col=\"unique_id\", time_col=\"ds\")\n",
    "\n",
    "\n",
    "test_forecast_dataframe(dask_df)\n",
    "test_forecast_dataframe_diff_cols(dask_diff_cols_df)\n",
    "test_anomalies_dataframe(dask_df)\n",
    "test_anomalies_online_dataframe(dask_df)\n",
    "test_anomalies_dataframe_diff_cols(dask_diff_cols_df)\n",
    "\n",
    "# test exogenous variables\n",
    "dask_df_x = dd.from_pandas(df_x, npartitions=2)\n",
    "dask_future_ex_vars_df = dd.from_pandas(future_ex_vars_df, npartitions=2)\n",
    "test_forecast_x_dataframe(dask_df_x, dask_future_ex_vars_df)\n",
    "\n",
    "# test x different cols\n",
    "dask_df_x_diff_cols = dd.from_pandas(df_x.rename(columns=renamer), npartitions=2)\n",
    "dask_future_ex_vars_df_diff_cols = dd.from_pandas(future_ex_vars_df.rename(columns=renamer), npartitions=2)\n",
    "test_forecast_x_dataframe_diff_cols(dask_df_x_diff_cols, dask_future_ex_vars_df_diff_cols)\n",
    "\n",
    "# test finetuning\n",
    "test_finetuned_model(dask_df)\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| distributed\n",
    "ray_cluster = Cluster(\n",
    "    initialize_head=True,\n",
    "    head_node_args={\"num_cpus\": 2}\n",
    ")\n",
    "ray.init(address=ray_cluster.address, ignore_reinit_error=True)\n",
    "# add mock node to simulate a cluster\n",
    "mock_node = ray_cluster.add_node(num_cpus=2)\n",
    "\n",
    "ray_df = ray.data.from_pandas(series)\n",
    "ray_diff_cols_df = ray.data.from_pandas(series_diff_cols)\n",
    "\n",
    "test_quantiles(ray_df, id_col=\"unique_id\", time_col=\"ds\")\n",
    "\n",
    "test_forecast_dataframe(ray_df)\n",
    "test_forecast_dataframe_diff_cols(ray_diff_cols_df)\n",
    "test_anomalies_dataframe(ray_df)\n",
    "test_anomalies_online_dataframe(ray_df)\n",
    "test_anomalies_dataframe_diff_cols(ray_diff_cols_df)\n",
    "\n",
    "# test exogenous variables\n",
    "ray_df_x = ray.data.from_pandas(df_x)\n",
    "ray_future_ex_vars_df = ray.data.from_pandas(future_ex_vars_df)\n",
    "test_forecast_x_dataframe(ray_df_x, ray_future_ex_vars_df)\n",
    "\n",
    "# test x different cols\n",
    "ray_df_x_diff_cols = ray.data.from_pandas(df_x.rename(columns=renamer))\n",
    "ray_future_ex_vars_df_diff_cols = ray.data.from_pandas(future_ex_vars_df.rename(columns=renamer))\n",
    "test_forecast_x_dataframe_diff_cols(ray_df_x_diff_cols, ray_future_ex_vars_df_diff_cols)\n",
    "\n",
    "# test finetuning\n",
    "test_finetuned_model(ray_df)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cleanup\n",
    "custom_client.delete_finetuned_model(model_id2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
