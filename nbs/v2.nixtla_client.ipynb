{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7c905-5495-4fdb-9651-eadbe99a5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeef897-a7b9-4cdc-be12-de1bf56ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp v2.nixtla_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de9d09-99fd-4c2e-8737-06b1f3674800",
   "metadata": {},
   "source": [
    "# V2 Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee3a4c-f29b-400a-846c-91f1ee50da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "from pydantic import NonNegativeInt, PositiveInt\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.validation import validate_format, validate_freq\n",
    "\n",
    "from nixtla.core.api_error import ApiError\n",
    "from nixtla.core.http_client import HttpClient\n",
    "from nixtla.utils import _restrict_input_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6dcc9-14a2-45f5-944d-62f31bb931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268dbd1-e52a-455d-8acb-5430b09d554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_LOSS = Literal[\"default\", \"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]\n",
    "_MODEL = Literal[\"timegpt-1\", \"timegpt-1-long-horizon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a14e6b-d604-4e40-8df1-ebb485480261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NixtlaClient:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        timeout: int = 60,\n",
    "        max_retries: int = 6,\n",
    "    ):\n",
    "        if api_key is None:\n",
    "            api_key = os.environ['NIXTLA_API_KEY']\n",
    "        if base_url is None:\n",
    "            base_url = os.getenv('NIXTLA_BASE_URL', 'https://dashboard.nixtla.io/api')\n",
    "        self._client_kwargs = {\n",
    "            'base_url': base_url,\n",
    "            'headers': {'Authorization': f'Bearer {api_key}'},\n",
    "            'timeout': timeout,\n",
    "        }\n",
    "        self.max_retries = max_retries\n",
    "        self._model_params: Dict[Tuple[str, str], Tuple[int, int]] = {}\n",
    "\n",
    "    def _make_request(self, client: HttpClient, endpoint: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        resp = client.request(\n",
    "            method='post',\n",
    "            url=endpoint,\n",
    "            json=payload,\n",
    "            max_retries=self.max_retries,\n",
    "        )\n",
    "        resp_body = resp.json()\n",
    "        if resp.status_code != 200:\n",
    "            raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        if 'data' in resp_body:\n",
    "            resp_body = resp_body['data']\n",
    "        return resp_body\n",
    "\n",
    "    def _make_partitioned_requests(self, client: HttpClient, endpoint: str, payloads: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        num_partitions = len(payloads)\n",
    "        results = num_partitions * [None]\n",
    "        max_workers = min(10, num_partitions)\n",
    "        with ThreadPoolExecutor(max_workers) as executor:\n",
    "            future2pos = {\n",
    "                executor.submit(self._make_request, client, endpoint, payload): i\n",
    "                for i, payload in enumerate(payloads)\n",
    "            }\n",
    "            for future in tqdm(as_completed(future2pos), total=len(future2pos)):\n",
    "                pos = future2pos[future]\n",
    "                results[pos] = future.result()\n",
    "        resp = {\"mean\": list(chain.from_iterable(res[\"mean\"] for res in results))}\n",
    "        first_res = results[0]\n",
    "        if first_res[\"intervals\"] is None:\n",
    "            resp[\"intervals\"] = None\n",
    "        else:\n",
    "            resp[\"intervals\"] = {}\n",
    "            for k in first_res[\"intervals\"].keys():\n",
    "                resp[\"intervals\"][k] = list(\n",
    "                    chain.from_iterable(res[\"intervals\"][k] for res in results)\n",
    "                )\n",
    "        if first_res[\"weights_x\"] is None:\n",
    "            resp[\"weights_x\"] = None\n",
    "        else:\n",
    "            resp[\"weights_x\"] = [res[\"weights_x\"] for res in results]\n",
    "        return resp\n",
    "\n",
    "    def _standardize_freq(self, freq: str) -> str:\n",
    "        return freq.replace('mo', 'MS')\n",
    "\n",
    "    def _get_model_params(self, model: str, freq: str):\n",
    "        key = (model, freq)\n",
    "        if key not in self._model_params:\n",
    "            logger.info('Querying model metadata...')\n",
    "            payload = {'model': model, 'freq': freq}\n",
    "            with httpx.Client(**self._client_kwargs) as httpx_client:\n",
    "                client = HttpClient(httpx_client=httpx_client)\n",
    "                params = self._make_request(client, '/model_params', payload)['detail']\n",
    "            self._model_params[key] = (params['input_size'], params['horizon'])\n",
    "        return self._model_params[key]\n",
    "\n",
    "    def _tail(self, proc: ufp.ProcessedDF, n: int) -> ufp.ProcessedDF:\n",
    "        n_series = proc.indptr.size - 1\n",
    "        new_sizes = np.minimum(np.diff(proc.indptr), n)\n",
    "        new_indptr = np.append(0, new_sizes.cumsum())\n",
    "        new_data = np.empty_like(proc.data, shape=(new_indptr[-1], proc.data.shape[1]))\n",
    "        for i in range(n_series):\n",
    "            new_data[new_indptr[i] : new_indptr[i + 1]] = proc.data[\n",
    "                proc.indptr[i + 1] - new_sizes[i] : proc.indptr[i + 1]\n",
    "            ]\n",
    "        return ufp.ProcessedDF(\n",
    "            uids=proc.uids,\n",
    "            last_times=proc.last_times,\n",
    "            data=new_data,\n",
    "            indptr=new_indptr,\n",
    "            sort_idxs=None,\n",
    "        )\n",
    "\n",
    "    def _partition_series(\n",
    "        self, payload: Dict[str, Any], n_part: int, h: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        parts = []\n",
    "        series = payload.pop(\"series\")\n",
    "        n_series = len(series[\"sizes\"])\n",
    "        n_part = min(n_part, n_series)\n",
    "        series_per_part = math.ceil(n_series / n_part)\n",
    "        prev_size = 0\n",
    "        for i in range(0, n_series, series_per_part):\n",
    "            sizes = series[\"sizes\"][i : i + series_per_part]\n",
    "            curr_size = sum(sizes)\n",
    "            part_idxs = slice(prev_size, prev_size + curr_size)\n",
    "            prev_size += curr_size\n",
    "            part_series = {\n",
    "                \"y\": series[\"y\"][part_idxs],\n",
    "                \"sizes\": sizes,\n",
    "            }\n",
    "            if series[\"X\"] is None:\n",
    "                part_series[\"X\"] = None\n",
    "                part_series[\"X_future\"] = None\n",
    "            else:\n",
    "                part_series[\"X\"] = [x[part_idxs] for x in series[\"X\"]]\n",
    "                part_series[\"X_future\"] = [\n",
    "                    x[i * h : (i + series_per_part) * h] for x in series[\"X_future\"]\n",
    "                ]\n",
    "            parts.append({\"series\": part_series, **payload})\n",
    "        return parts\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        h: PositiveInt,\n",
    "        freq: str,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        finetune_steps: NonNegativeInt = 0,\n",
    "        finetune_loss: _LOSS = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        model: _MODEL = 'timegpt-1',\n",
    "        num_partitions: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            dataframe to forecast. Must have at least [id_col, time_col, target_col] columns.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            pandas or polars offset alias.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values must be timestamps.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas or polars DataFrame, optional (default=None)\n",
    "            dataframe with [id_col, time_col] columns and `df`'s future exogenous.\n",
    "        level : list of int or float, optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the new data.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: 'default', 'mae', 'mse', 'rmse', 'mape', and 'smape'.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: 'timegpt-1', and 'timegpt-1-long-horizon'. \n",
    "            We recommend using 'timegpt-1-long-horizon' if you want to predict more than one seasonal period.\n",
    "        num_partitions : int, optional (default=None)\n",
    "            Split the series into this number of partitions and make that many requests in parallel.\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        pandas or polars DataFrame\n",
    "            dataframe with TimeGPT forecasts for point and probabilistic predictions (if level is not `None`).\n",
    "        \"\"\"\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        logger.info('Validating inputs...')\n",
    "        validate_format(df=df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "        validate_freq(times=df[time_col], freq=freq)\n",
    "        exog_df = [c for c in df.columns if c not in (id_col, time_col, target_col)]\n",
    "        if X_df is not None:\n",
    "            exog_X_df = [c for c in X_df.columns if c not in (id_col, time_col)]\n",
    "            missing_df = set(exog_X_df) - set(exog_df)\n",
    "            if missing_df:\n",
    "                raise ValueError(\n",
    "                    'The following exogenous features are present in `X_df` '\n",
    "                    f'but not in `df`: {missing_df}.'\n",
    "                )\n",
    "            missing_X_df = set(exog_df) - set(exog_X_df)\n",
    "            if missing_X_df:\n",
    "                raise ValueError(\n",
    "                    'The following exogenous features are present in `df` '\n",
    "                    f'but not in `X_df`: {missing_X_df}.'\n",
    "                )\n",
    "            if exog_df != exog_X_df:\n",
    "                # rearrange columns if necessary\n",
    "                X_df = X_df[[id_col, time_col, *exog_df]]\n",
    "            x_cols: Optional[List[str]] = exog_df\n",
    "        else:\n",
    "            if exog_df:\n",
    "                warnings.warn(\n",
    "                    f'`df` contains the following exogenous features: {exog_df}, '\n",
    "                    'but `X_df` was not provided. They will be ignored.'\n",
    "                )\n",
    "                df = df[[id_col, time_col, target_col]]\n",
    "            x_cols = None\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed = ufp.process_df(\n",
    "            df=df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        if X_df is not None:\n",
    "            processed_X = ufp.process_df(\n",
    "                df=X_df, id_col=id_col, time_col=time_col, target_col=None,\n",
    "            )\n",
    "            X_future = processed_X.data.T.tolist()\n",
    "        else:\n",
    "            X_future = None\n",
    "\n",
    "        standard_freq = self._standardize_freq(freq)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        restrict_input = finetune_steps == 0 and X_df is None\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            processed = self._tail(processed, new_input_size)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T.tolist()\n",
    "        else:\n",
    "            X = None\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0].tolist(),\n",
    "                'sizes': np.diff(processed.indptr).tolist(),\n",
    "                'X': X,\n",
    "                'X_future': X_future,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_loss': finetune_loss,\n",
    "        }\n",
    "\n",
    "        logger.info('Calling Forecast Endpoint...')\n",
    "        with httpx.Client(**self._client_kwargs) as httpx_client:\n",
    "            client = HttpClient(httpx_client=httpx_client)\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request(client, '/v2/forecast', payload)\n",
    "            else:\n",
    "                payloads = self._partition_series(payload, num_partitions, h)\n",
    "                resp = self._make_partitioned_requests(client, '/v2/forecast', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        out = ufp.make_future_dataframe(\n",
    "            uids=processed.uids,\n",
    "            last_times=type(processed.uids)(processed.last_times),\n",
    "            freq=freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        if resp['intervals'] is not None:\n",
    "            intervals_df = type(df)(\n",
    "                {\n",
    "                    f'TimeGPT-{k}': v for k, v in resp['intervals'].items()\n",
    "                }\n",
    "            )\n",
    "            out = ufp.horizontal_concat([out, intervals_df])\n",
    "        if resp['weights_x'] is not None:\n",
    "            self.weights_x = type(df)({\n",
    "                'features': x_cols,\n",
    "                'weights': resp['weights_x'],\n",
    "            })\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bc225-6ae3-4305-b002-ade5abbc145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from fastcore.test import test_warns\n",
    "from utilsforecast.data import generate_series\n",
    "\n",
    "from nixtla import NixtlaClient as NixtlaV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e4819-4a57-49b9-8ef0-38497d9f8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_series(\n",
    "    4, min_length=40, max_length=200, n_static_features=2, static_as_categorical=False\n",
    ")\n",
    "horizon = 20\n",
    "level = [80, 97.5]\n",
    "series['unique_id'] = series['unique_id'].astype('int64')\n",
    "valid = series.groupby('unique_id', observed=True).tail(horizon)\n",
    "X_df = valid.drop(columns='y')\n",
    "train = series.drop(valid.index)\n",
    "train_pl = pl.from_pandas(train)\n",
    "X_df_pl = pl.from_pandas(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ff47f-5bbe-4219-b90f-794acc9f9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = NixtlaClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef60dc2-34d0-4acf-b64f-d828daf027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resv2_pd = client.forecast(df=train, X_df=X_df, h=horizon, freq=\"D\", level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc288a9-66b5-4746-883f-158bfb71beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_warns(lambda: client.forecast(df=train, h=horizon, freq=\"D\", level=level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd08c9-2b2c-46ec-b77a-ed5b8dbac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resv2_pl = client.forecast(df=train_pl, X_df=X_df_pl, h=horizon, freq=\"1d\", level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(\n",
    "    client.forecast(df=series[['unique_id', 'ds', 'y']], h=horizon, freq=\"D\")['TimeGPT'].to_numpy(),\n",
    "    client.forecast(df=series[['unique_id', 'ds', 'y']], h=horizon, freq=\"D\", num_partitions=4)['TimeGPT'].to_numpy(),\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bbbc61-57d9-4c4e-9d2f-02b31c17a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(resv2_pd, resv2_pl.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e5db3-afde-4d55-80b5-18336e217623",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_client = NixtlaV1()\n",
    "resv1 = v1_client.forecast(df=train, X_df=X_df, h=horizon, freq=\"D\", level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d303843-4145-443f-866b-bd9bc61c4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(resv2_pd.astype({'ds': 'str'}), resv1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
