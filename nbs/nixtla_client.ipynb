{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nixtla_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import functools\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from functools import partial, wraps\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry, stop_after_attempt, \n",
    "    wait_fixed, stop_after_delay,\n",
    "    RetryCallState, \n",
    "    retry_if_exception, \n",
    "    retry_if_not_exception_type,\n",
    ")\n",
    "from utilsforecast.preprocessing import fill_gaps\n",
    "from utilsforecast.processing import (\n",
    "    backtest_splits,\n",
    "    drop_index_if_pandas,\n",
    "    join,\n",
    "    maybe_compute_sort_indices,\n",
    "    take_rows,\n",
    "    vertical_concat,\n",
    ")\n",
    "\n",
    "from nixtla.client import Nixtla\n",
    "from nixtla.core import ApiError\n",
    "from nixtla.types.multi_series_anomaly import MultiSeriesAnomaly\n",
    "from nixtla.types.multi_series_forecast import MultiSeriesForecast\n",
    "from nixtla.types.multi_series_insample_forecast import MultiSeriesInsampleForecast\n",
    "from nixtla.types.single_series_forecast import SingleSeriesForecast\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "main_logger = logging.getLogger(__name__)\n",
    "httpx_logger = logging.getLogger('httpx')\n",
    "httpx_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "import warnings\n",
    "from itertools import product\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastcore.test import test_eq, test_fail, test_warns\n",
    "from nbdev.showdoc import show_doc\n",
    "from tqdm import TqdmExperimentalWarning\n",
    "\n",
    "load_dotenv()\n",
    "logging.getLogger('statsforecast').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def deprecated_argument(old_name, new_name):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if old_name in kwargs:\n",
    "                warnings.warn(f\"`'{old_name}'` is deprecated; use `'{new_name}'` instead.\", FutureWarning)\n",
    "                if new_name in kwargs:\n",
    "                    raise TypeError(f\"{new_name} argument duplicated\")\n",
    "                kwargs[new_name] = kwargs.pop(old_name)\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def deprecated_method(new_method):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            warnings.warn(\n",
    "                f\"Method `{func.__name__}` is deprecated; \"\n",
    "                f\"use `{new_method}` instead.\", \n",
    "                FutureWarning,\n",
    "            )\n",
    "            return getattr(self, new_method)(*args, **kwargs)\n",
    "        wrapper.__doc__ = func.__doc__\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "deprecated_fewshot_steps = deprecated_argument(\"fewshot_steps\", \"finetune_steps\")\n",
    "deprecated_fewshot_loss = deprecated_argument(\"fewshot_loss\", \"finetune_loss\")\n",
    "deprecated_token = deprecated_argument(\"token\", \"api_key\")\n",
    "deprecated_environment = deprecated_argument(\"environment\", \"base_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "use_validate_api_key = deprecated_method(new_method=\"validate_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "date_features_by_freq = {\n",
    "    # Daily frequencies\n",
    "    'B': ['year', 'month', 'day', 'weekday'],\n",
    "    'C': ['year', 'month', 'day', 'weekday'],\n",
    "    'D': ['year', 'month', 'day', 'weekday'],\n",
    "    # Weekly\n",
    "    'W': ['year', 'week', 'weekday'],\n",
    "    # Monthly\n",
    "    'M': ['year', 'month'],\n",
    "    'SM': ['year', 'month', 'day'],\n",
    "    'BM': ['year', 'month'],\n",
    "    'CBM': ['year', 'month'],\n",
    "    'MS': ['year', 'month'],\n",
    "    'SMS': ['year', 'month', 'day'],\n",
    "    'BMS': ['year', 'month'],\n",
    "    'CBMS': ['year', 'month'],\n",
    "    # Quarterly\n",
    "    'Q': ['year', 'quarter'],\n",
    "    'BQ': ['year', 'quarter'],\n",
    "    'QS': ['year', 'quarter'],\n",
    "    'BQS': ['year', 'quarter'],\n",
    "    # Yearly\n",
    "    'A': ['year'],\n",
    "    'Y': ['year'],\n",
    "    'BA': ['year'],\n",
    "    'BY': ['year'],\n",
    "    'AS': ['year'],\n",
    "    'YS': ['year'],\n",
    "    'BAS': ['year'],\n",
    "    'BYS': ['year'],\n",
    "    # Hourly\n",
    "    'BH': ['year', 'month', 'day', 'hour', 'weekday'],\n",
    "    'H': ['year', 'month', 'day', 'hour'],\n",
    "    # Minutely\n",
    "    'T': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    'min': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    # Secondly\n",
    "    'S': ['year', 'month', 'day', 'hour', 'minute', 'second'],\n",
    "    # Milliseconds\n",
    "    'L': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    'ms': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    # Microseconds\n",
    "    'U': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    'us': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    # Nanoseconds\n",
    "    'N': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _NixtlaClientModel:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            client: Nixtla,\n",
    "            h: int,\n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            freq: str = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            quantiles: Optional[List[float]] = None,\n",
    "            finetune_steps: int = 0,\n",
    "            finetune_loss: str = 'default',\n",
    "            clean_ex_first: bool = True,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            model: str = 'timegpt-1',\n",
    "            max_retries: int = 6,\n",
    "            retry_interval: int = 10,\n",
    "            max_wait_time: int = 6 * 60,\n",
    "        ):\n",
    "        self.client = client\n",
    "        self.h = h\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.base_freq = freq\n",
    "        self.level, self.quantiles = self._prepare_level_and_quantiles(level, quantiles)\n",
    "        self.finetune_steps = finetune_steps\n",
    "        self.finetune_loss = finetune_loss\n",
    "        self.clean_ex_first = clean_ex_first\n",
    "        self.date_features = date_features\n",
    "        self.date_features_to_one_hot = date_features_to_one_hot\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_interval = retry_interval\n",
    "        self.max_wait_time = max_wait_time\n",
    "        # variables defined by each flow\n",
    "        self.weights_x: pd.DataFrame = None\n",
    "        self.freq: str = self.base_freq\n",
    "        self.drop_uid: bool = False\n",
    "        self.input_size: int\n",
    "        self.model_horizon: int\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_level_and_quantiles(\n",
    "        level: Optional[List[Union[int, float]]], \n",
    "        quantiles: Optional[List[float]],\n",
    "    ):\n",
    "        if (level is not None) and (quantiles is not None):\n",
    "            raise Exception(\n",
    "                \"you should include `level` or `quantiles` but not both.\"\n",
    "            )\n",
    "        if quantiles is None:\n",
    "            return level, quantiles\n",
    "        # we recover level from quantiles\n",
    "        if not all(0 < q < 1 for q in quantiles):\n",
    "            raise Exception(\n",
    "                \"`quantiles` should lie between 0 and 1\"\n",
    "            )\n",
    "        level = [abs(int(100 - 200 * q)) for q in quantiles]\n",
    "        return level, quantiles\n",
    "        \n",
    "\n",
    "    def _retry_strategy(self):\n",
    "        def after_retry(retry_state: RetryCallState):\n",
    "            \"\"\"Called after each retry attempt.\"\"\"\n",
    "            main_logger.info(f\"Attempt {retry_state.attempt_number} failed...\")\n",
    "        # we want to retry when:\n",
    "        # there is no ApiError \n",
    "        # there is an ApiError with string body\n",
    "        def is_api_error_with_text_body(exception):\n",
    "            if isinstance(exception, ApiError):\n",
    "                if isinstance(exception.body, str):\n",
    "                    return True\n",
    "            return False\n",
    "        return retry(\n",
    "            stop=(stop_after_attempt(self.max_retries) | stop_after_delay(self.max_wait_time)),\n",
    "            wait=wait_fixed(self.retry_interval),\n",
    "            reraise=True,\n",
    "            after=after_retry,\n",
    "            retry=retry_if_exception(is_api_error_with_text_body) | retry_if_not_exception_type(ApiError),\n",
    "        )\n",
    "\n",
    "    def _call_api(self, method, request):\n",
    "        response = self._retry_strategy()(method)(request=request)\n",
    "        if 'data' in response:\n",
    "            response = response['data']\n",
    "        return response\n",
    "\n",
    "    def transform_inputs(self, df: pd.DataFrame, X_df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        main_logger.info(\"Validating inputs...\")\n",
    "        if self.base_freq is None and hasattr(df.index, \"freq\"):\n",
    "            inferred_freq = df.index.freq\n",
    "            if inferred_freq is not None:\n",
    "                inferred_freq = inferred_freq.rule_code\n",
    "                main_logger.info(f\"Inferred freq: {inferred_freq}\")\n",
    "            self.freq = inferred_freq\n",
    "            time_col = df.index.name if df.index.name else \"ds\"\n",
    "            self.time_col = time_col\n",
    "            df.index.name = time_col\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            self.freq = self.base_freq\n",
    "        renamer = {\n",
    "            self.id_col: \"unique_id\",\n",
    "            self.time_col: \"ds\",\n",
    "            self.target_col: \"y\",\n",
    "        }\n",
    "        df = df.rename(columns=renamer)\n",
    "        if df.dtypes.ds != \"object\":\n",
    "            df[\"ds\"] = df[\"ds\"].astype(str)\n",
    "        if \"unique_id\" not in df.columns:\n",
    "            # Insert unique_id column\n",
    "            df = df.assign(unique_id=\"ts_0\")\n",
    "            self.drop_uid = True\n",
    "        if X_df is not None:\n",
    "            X_df = X_df.copy()\n",
    "            X_df = X_df.rename(columns=renamer)\n",
    "            if \"unique_id\" not in X_df.columns:\n",
    "                X_df = X_df.assign(unique_id=\"ts_0\")\n",
    "            if X_df.dtypes.ds != \"object\":\n",
    "                X_df[\"ds\"] = X_df[\"ds\"].astype(str)\n",
    "        return df, X_df\n",
    "\n",
    "    def transform_outputs(self, fcst_df: pd.DataFrame, level_to_quantiles: bool = False):\n",
    "        renamer = {\n",
    "            'unique_id': self.id_col,\n",
    "            'ds': self.time_col,\n",
    "            'y': self.target_col,\n",
    "        }\n",
    "        if self.drop_uid:\n",
    "            fcst_df = fcst_df.drop(columns='unique_id')\n",
    "        fcst_df = fcst_df.rename(columns=renamer)\n",
    "        # transfom levels to quantiles if needed\n",
    "        if level_to_quantiles and self.quantiles is not None:\n",
    "            cols = [col for col in fcst_df.columns if ('-lo-' not in col) and ('-hi-' not in col)]\n",
    "            for q in sorted(self.quantiles):\n",
    "                if q == 0.5:\n",
    "                    col = 'TimeGPT'\n",
    "                else:\n",
    "                    lv = int(100 - 200 * q)\n",
    "                    hi_or_lo = 'lo' if lv > 0 else 'hi'\n",
    "                    lv = abs(lv)\n",
    "                    col = f\"TimeGPT-{hi_or_lo}-{lv}\"\n",
    "                q_col = f\"TimeGPT-q-{int(q * 100)}\"\n",
    "                fcst_df[q_col] = fcst_df[col].values\n",
    "                cols.append(q_col)\n",
    "            fcst_df = fcst_df[cols]\n",
    "        return fcst_df\n",
    "\n",
    "    def infer_freq(self, df: pd.DataFrame):\n",
    "        # special freqs that need to be checked\n",
    "        # for example to ensure 'W'-> 'W-MON'\n",
    "        special_freqs = ['W', 'M', 'Q', 'Y', 'A']\n",
    "        if self.freq is None or self.freq in special_freqs:\n",
    "            unique_id = df.iloc[0]['unique_id']\n",
    "            df_id = df.query('unique_id == @unique_id')\n",
    "            inferred_freq = pd.infer_freq(df_id['ds'].sort_values())\n",
    "            if inferred_freq is None:\n",
    "                raise Exception(\n",
    "                    'Could not infer frequency of ds column. This could be due to '\n",
    "                    'inconsistent intervals. Please check your data for missing, '\n",
    "                    'duplicated or irregular timestamps'\n",
    "                )\n",
    "            if self.freq is not None:\n",
    "                # check we have the same base frequency\n",
    "                # except when we have yearly frequency (A, and Y means the same)\n",
    "                if (self.freq != inferred_freq[0] and self.freq != 'Y') or (self.freq == 'Y' and inferred_freq[0] != 'A'):\n",
    "                    raise Exception(f'Failed to infer special date, inferred freq {inferred_freq}')\n",
    "            main_logger.info(f'Inferred freq: {inferred_freq}')\n",
    "            self.freq = inferred_freq\n",
    "\n",
    "    def resample_dataframe(self, df: pd.DataFrame):\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['ds'].dtype):\n",
    "            df = df.copy(deep=False)\n",
    "            df['ds'] = pd.to_datetime(df['ds'])\n",
    "        resampled_df = fill_gaps(\n",
    "            df,\n",
    "            freq=self.freq,\n",
    "            start='per_serie',\n",
    "            end='per_serie',\n",
    "            id_col='unique_id',\n",
    "            time_col='ds',\n",
    "        )\n",
    "        numeric_cols = resampled_df.columns.drop(['unique_id', 'ds'])\n",
    "        resampled_df[numeric_cols] = (\n",
    "            resampled_df\n",
    "            .groupby('unique_id', observed=True)\n",
    "            [numeric_cols]\n",
    "            .bfill()\n",
    "        )\n",
    "        resampled_df['ds'] = resampled_df['ds'].astype(str)\n",
    "        return resampled_df\n",
    "\n",
    "    def make_future_dataframe(self, df: pd.DataFrame, reconvert: bool = True):\n",
    "        last_dates = df.groupby('unique_id')['ds'].max()\n",
    "        def _future_date_range(last_date):\n",
    "            future_dates = pd.date_range(last_date, freq=self.freq, periods=self.h + 1)\n",
    "            future_dates = future_dates[-self.h:]\n",
    "            return future_dates\n",
    "        future_df = last_dates.apply(_future_date_range).reset_index()\n",
    "        future_df = future_df.explode('ds').reset_index(drop=True)\n",
    "        if reconvert and df.dtypes['ds'] == 'object':\n",
    "            # avoid date 000\n",
    "            future_df['ds'] = future_df['ds'].astype(str)\n",
    "        return future_df\n",
    "\n",
    "    def compute_date_feature(self, dates, feature):\n",
    "        if callable(feature):\n",
    "            feat_name = feature.__name__\n",
    "            feat_vals = feature(dates)\n",
    "        else:\n",
    "            feat_name = feature\n",
    "            if feature in (\"week\", \"weekofyear\"):\n",
    "                dates = dates.isocalendar()\n",
    "            feat_vals = getattr(dates, feature)\n",
    "        if not isinstance(feat_vals, pd.DataFrame):\n",
    "            vals = np.asarray(feat_vals)\n",
    "            feat_vals = pd.DataFrame({feat_name: vals})\n",
    "        feat_vals['ds'] = dates\n",
    "        return feat_vals\n",
    "\n",
    "    def add_date_features( \n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            X_df: Optional[pd.DataFrame],\n",
    "        ):\n",
    "        # df contains exogenous variables\n",
    "        # X_df are the future values of the exogenous variables\n",
    "        # construct dates\n",
    "        train_dates = df['ds'].unique().tolist()\n",
    "        # if we dont have future exogenos variables\n",
    "        # we need to compute the future dates\n",
    "        if (self.h is not None) and X_df is None:\n",
    "            X_df = self.make_future_dataframe(df=df)\n",
    "            future_dates = X_df['ds'].unique().tolist()\n",
    "        elif X_df is not None:\n",
    "            future_dates = X_df['ds'].unique().tolist()\n",
    "        else:\n",
    "            future_dates = []\n",
    "        dates = pd.DatetimeIndex(np.unique(train_dates + future_dates).tolist())\n",
    "        date_features_df = pd.DataFrame({'ds': dates})\n",
    "        for feature in self.date_features:\n",
    "            feat_df = self.compute_date_feature(dates, feature)\n",
    "            date_features_df = date_features_df.merge(feat_df, on=['ds'], how='left')\n",
    "        if df.dtypes['ds'] == 'object':\n",
    "            date_features_df['ds'] = date_features_df['ds'].astype(str)\n",
    "        if self.date_features_to_one_hot is not None:\n",
    "            date_features_df = pd.get_dummies(\n",
    "                date_features_df, \n",
    "                columns=self.date_features_to_one_hot, \n",
    "                dtype=int,\n",
    "            )\n",
    "        # remove duplicated columns if any\n",
    "        date_features_df = date_features_df.drop(\n",
    "            columns=[col for col in date_features_df.columns if col in df.columns and col not in ['unique_id', 'ds']]\n",
    "        )\n",
    "        # add date features to df\n",
    "        df = df.merge(date_features_df, on='ds', how='left')\n",
    "        # add date features to X_df\n",
    "        if X_df is not None:\n",
    "            X_df = X_df.merge(date_features_df, on='ds', how='left')\n",
    "        return df, X_df\n",
    "\n",
    "    def preprocess_X_df(self, X_df: pd.DataFrame):\n",
    "        if X_df.isna().any().any():\n",
    "            raise Exception('Some of your exogenous variables contain NA, please check')\n",
    "        X_df = X_df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        X_df = self.resample_dataframe(X_df)\n",
    "        return X_df\n",
    "\n",
    "    def preprocess_dataframes(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            X_df: Optional[pd.DataFrame],\n",
    "        ):\n",
    "        self.infer_freq(df=df)\n",
    "        \"\"\"Returns Y_df and X_df dataframes in the structure expected by the endpoints.\"\"\"\n",
    "        # add date features logic\n",
    "        if isinstance(self.date_features, bool):\n",
    "            if self.date_features:\n",
    "                self.date_features = date_features_by_freq.get(self.freq)\n",
    "                if self.date_features is None:\n",
    "                    warnings.warn(\n",
    "                        f'Non default date features for {self.freq} '\n",
    "                        'please pass a list of date features'\n",
    "                    )\n",
    "            else:\n",
    "                self.date_features = None\n",
    "                \n",
    "        if self.date_features is not None:\n",
    "            if isinstance(self.date_features_to_one_hot, bool):\n",
    "                if self.date_features_to_one_hot:\n",
    "                    self.date_features_to_one_hot = [feat for feat in self.date_features if not callable(feat)]\n",
    "                    self.date_features_to_one_hot = None if not self.date_features_to_one_hot else self.date_features_to_one_hot\n",
    "                else:\n",
    "                    self.date_features_to_one_hot = None\n",
    "            df, X_df = self.add_date_features(df=df, X_df=X_df)\n",
    "        y_cols = ['unique_id', 'ds', 'y']\n",
    "        Y_df = df[y_cols]\n",
    "        if Y_df['y'].isna().any():\n",
    "            raise Exception('Your target variable contains NA, please check')\n",
    "        # Azul: efficient this code\n",
    "        # and think about returning dates that are not in the training set\n",
    "        Y_df = self.resample_dataframe(Y_df)\n",
    "        x_cols = []\n",
    "        if X_df is not None:\n",
    "            x_cols = X_df.columns.drop(['unique_id', 'ds']).to_list()\n",
    "            if not all(col in df.columns for col in x_cols):\n",
    "                raise Exception(\n",
    "                    'You must include the exogenous variables in the `df` object, '\n",
    "                    f'exogenous variables {\",\".join(x_cols)}'\n",
    "                )\n",
    "            if (self.h is not None) and (len(X_df) != df['unique_id'].nunique() * self.h):\n",
    "                raise Exception(\n",
    "                    f'You have to pass the {self.h} future values of your '\n",
    "                    'exogenous variables for each time series'\n",
    "                )\n",
    "            X_df_history = df[['unique_id', 'ds'] + x_cols]\n",
    "            X_df = pd.concat([X_df_history, X_df])\n",
    "            X_df = self.preprocess_X_df(X_df)\n",
    "        elif (X_df is None) and (self.h is None) and (len(y_cols) < df.shape[1]):\n",
    "            # case for just insample, \n",
    "            # we dont need h\n",
    "            X_df = df.drop(columns='y')\n",
    "            x_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()\n",
    "            X_df = self.preprocess_X_df(X_df)\n",
    "        return Y_df, X_df, x_cols\n",
    "\n",
    "    def dataframes_to_dict(self, Y_df: pd.DataFrame, X_df: pd.DataFrame):\n",
    "        to_dict_args = {'orient': 'split'}\n",
    "        if 'index' in inspect.signature(pd.DataFrame.to_dict).parameters:\n",
    "            to_dict_args['index'] = False\n",
    "        y = Y_df.to_dict(**to_dict_args)\n",
    "        x = X_df.to_dict(**to_dict_args) if X_df is not None else None\n",
    "        # A: I'm aware that sel.x_cols exists, but\n",
    "        # I want to be sure that we are logging the correct\n",
    "        # x cols :kiss:\n",
    "        if x:\n",
    "            x_cols = [col for col in x['columns'] if col not in ['unique_id', 'ds']]\n",
    "            main_logger.info(f'Using the following exogenous variables: {\", \".join(x_cols)}')\n",
    "        return y, x\n",
    "    \n",
    "    def set_model_params(self):\n",
    "        model_params = self._call_api(\n",
    "            self.client.model_params,\n",
    "            SingleSeriesForecast(freq=self.freq, model=self.model),\n",
    "        )\n",
    "        model_params = model_params['detail']\n",
    "        self.input_size, self.model_horizon = model_params['input_size'], model_params['horizon']\n",
    "    \n",
    "    def validate_input_size(self, Y_df: pd.DataFrame):\n",
    "        min_history = Y_df.groupby('unique_id', observed=True).size().min()\n",
    "        if min_history < self.input_size + self.model_horizon:\n",
    "            raise Exception(\n",
    "                'Your time series data is too short '\n",
    "\n",
    "                'Please be sure that your unique time series contain '\n",
    "                f'at least {self.input_size + self.model_horizon} observations'\n",
    "            )\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        X_df: Optional[pd.DataFrame] = None,\n",
    "        add_history: bool = False,\n",
    "    ):\n",
    "        df, X_df = self.transform_inputs(df=df, X_df=X_df)\n",
    "        main_logger.info('Preprocessing dataframes...')\n",
    "        Y_df, X_df, x_cols = self.preprocess_dataframes(df=df, X_df=X_df)\n",
    "        self.set_model_params()\n",
    "        if self.h > self.model_horizon:\n",
    "            main_logger.warning(\n",
    "                'The specified horizon \"h\" exceeds the model horizon. '\n",
    "                'This may lead to less accurate forecasts. '\n",
    "                'Please consider using a smaller horizon.'\n",
    "            )\n",
    "        # restrict input if\n",
    "        #  we dont want to fine-tune\n",
    "        #  we dont have exogenous regressors\n",
    "        #  no add history\n",
    "        restrict_input = self.finetune_steps == 0 and X_df is None and not add_history\n",
    "        if restrict_input:\n",
    "            main_logger.info('Restricting input...')\n",
    "            if self.level is not None:\n",
    "                # add sufficient info to compute\n",
    "                # conformal interval\n",
    "                # @AzulGarza\n",
    "                #  this is an old opinionated decision\n",
    "                #  about reducing the data sent to the api\n",
    "                #  to reduce latency when\n",
    "                #  a user passes level. since currently the model\n",
    "                #  uses conformal prediction, we can change a minimum\n",
    "                #  amount of data if the series are too large\n",
    "                new_input_size = 3 * self.input_size + max(self.model_horizon, self.h)\n",
    "            else:\n",
    "                # we only want to forecast\n",
    "                new_input_size = self.input_size\n",
    "            Y_df = Y_df.groupby(\"unique_id\").tail(new_input_size)\n",
    "            if X_df is not None:\n",
    "                X_df = X_df.groupby(\"unique_id\").tail(\n",
    "                    new_input_size + self.h\n",
    "                )  # history plus exogenous\n",
    "        if self.finetune_steps > 0 or self.level is not None:\n",
    "            self.validate_input_size(Y_df=Y_df)\n",
    "        y, x = self.dataframes_to_dict(Y_df, X_df)\n",
    "        main_logger.info('Calling Forecast Endpoint...')\n",
    "        payload = MultiSeriesForecast(\n",
    "            y=y,\n",
    "            x=x,\n",
    "            fh=self.h,\n",
    "            freq=self.freq,\n",
    "            level=self.level,\n",
    "            finetune_steps=self.finetune_steps,\n",
    "            finetune_loss=self.finetune_loss,\n",
    "            clean_ex_first=self.clean_ex_first,\n",
    "            model=self.model,\n",
    "        )\n",
    "        response_timegpt = self._call_api(\n",
    "            self.client.forecast_multi_series,\n",
    "            payload,\n",
    "        )\n",
    "        if 'weights_x' in response_timegpt:\n",
    "            self.weights_x = pd.DataFrame({\n",
    "                'features': x_cols,\n",
    "                'weights': response_timegpt['weights_x'],\n",
    "            })\n",
    "        fcst_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "        if add_history:\n",
    "            main_logger.info('Calling Historical Forecast Endpoint...')\n",
    "            self.validate_input_size(Y_df=Y_df)\n",
    "            response_timegpt = self._call_api(\n",
    "                self.client.historic_forecast_multi_series,\n",
    "                MultiSeriesInsampleForecast(\n",
    "                    y=y,\n",
    "                    x=x,\n",
    "                    freq=self.freq,\n",
    "                    level=self.level,\n",
    "                    clean_ex_first=self.clean_ex_first,\n",
    "                    model=self.model,\n",
    "                ),\n",
    "            )\n",
    "            fitted_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "            fitted_df = fitted_df.drop(columns='y')\n",
    "            fcst_df = pd.concat([fitted_df, fcst_df]).sort_values(['unique_id', 'ds'])\n",
    "        fcst_df = self.transform_outputs(fcst_df, level_to_quantiles=True)\n",
    "        return fcst_df\n",
    "\n",
    "    def detect_anomalies(self, df: pd.DataFrame):\n",
    "        # Azul\n",
    "        # Remember the input X_df is the FUTURE ex vars\n",
    "        # there is a misleading notation here\n",
    "        # because X_df inputs in the following methods\n",
    "        # returns X_df outputs that means something different\n",
    "        # ie X_df = [X_df_history, X_df]\n",
    "        # exogenous variables are passed after df \n",
    "        df, _ = self.transform_inputs(df=df, X_df=None)\n",
    "        main_logger.info('Preprocessing dataframes...')\n",
    "        Y_df, X_df, x_cols = self.preprocess_dataframes(df=df, X_df=None)\n",
    "        main_logger.info('Calling Anomaly Detector Endpoint...')\n",
    "        y, x = self.dataframes_to_dict(Y_df, X_df)\n",
    "        response_timegpt = self._call_api(\n",
    "            self.client.anomaly_detection_multi_series,\n",
    "            MultiSeriesAnomaly(\n",
    "                y=y,\n",
    "                x=x,\n",
    "                freq=self.freq,\n",
    "                level=[self.level] if (isinstance(self.level, int) or isinstance(self.level, float)) else [self.level[0]],\n",
    "                clean_ex_first=self.clean_ex_first,\n",
    "                model=self.model,\n",
    "            ),\n",
    "        )\n",
    "        if 'weights_x' in response_timegpt:\n",
    "            self.weights_x = pd.DataFrame({\n",
    "                'features': x_cols,\n",
    "                'weights': response_timegpt['weights_x'],\n",
    "            })\n",
    "        anomalies_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "        anomalies_df = anomalies_df.drop(columns='y')\n",
    "        anomalies_df = self.transform_outputs(anomalies_df)\n",
    "        return anomalies_df\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        n_windows: int = 1,\n",
    "        step_size: Optional[int] = None,\n",
    "    ):\n",
    "        # A: see `transform_inputs`\n",
    "        # the code always will return X_df=None\n",
    "        # if X_df=None\n",
    "        df, _ = self.transform_inputs(df=df, X_df=None)\n",
    "        self.infer_freq(df)\n",
    "        df['ds'] = pd.to_datetime(df['ds'])\n",
    "        # mlforecast cv code\n",
    "        results = []\n",
    "        sort_idxs = maybe_compute_sort_indices(df, \"unique_id\", \"ds\")\n",
    "        if sort_idxs is not None:\n",
    "            df = take_rows(df, sort_idxs)\n",
    "        splits = backtest_splits(\n",
    "            df,\n",
    "            n_windows=n_windows,\n",
    "            h=self.h,\n",
    "            id_col=\"unique_id\",\n",
    "            time_col=\"ds\",\n",
    "            freq=pd.tseries.frequencies.to_offset(self.freq),\n",
    "            step_size=self.h if step_size is None else step_size,\n",
    "        )\n",
    "        for i_window, (cutoffs, train, valid) in enumerate(splits):\n",
    "            if len(valid.columns) > 3:\n",
    "                # if we have uid, ds, y + exogenous vars\n",
    "                train_future = valid.drop(columns='y')\n",
    "            else:\n",
    "                train_future = None\n",
    "            y_pred = self.forecast(\n",
    "                df=train,\n",
    "                X_df=train_future,\n",
    "            )\n",
    "            y_pred, _ = self.transform_inputs(df=y_pred, X_df=None)\n",
    "            y_pred = join(y_pred, cutoffs, on=\"unique_id\", how=\"left\")\n",
    "            y_pred[\"ds\"] = pd.to_datetime(y_pred[\"ds\"])\n",
    "            result = join(\n",
    "                valid[[\"unique_id\", \"ds\", \"y\"]],\n",
    "                y_pred,\n",
    "                on=[\"unique_id\", \"ds\"],\n",
    "            )\n",
    "            if result.shape[0] < valid.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Cross validation result produced less results than expected. \"\n",
    "                    \"Please verify that the frequency parameter (freq) matches your series' \"\n",
    "                    \"and that there aren't any missing periods.\"\n",
    "                )\n",
    "            results.append(result)\n",
    "        out = vertical_concat(results)\n",
    "        out = drop_index_if_pandas(out)\n",
    "        first_out_cols = [\"unique_id\", \"ds\", \"cutoff\", \"y\"]\n",
    "        remaining_cols = [c for c in out.columns if c not in first_out_cols]\n",
    "        fcst_cv_df = out[first_out_cols + remaining_cols]\n",
    "        fcst_cv_df[\"ds\"] = fcst_cv_df[\"ds\"].astype(str)\n",
    "        fcst_cv_df = self.transform_outputs(fcst_cv_df)\n",
    "        return fcst_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def validate_model_parameter(func):\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if 'model' in kwargs:\n",
    "            model = kwargs['model']\n",
    "            rename_models_dict = {\n",
    "                'short-horizon': 'timegpt-1',\n",
    "                'long-horizon': 'timegpt-1-long-horizon',\n",
    "            }\n",
    "            if model in rename_models_dict.keys():\n",
    "                new_model = rename_models_dict[model]\n",
    "                warnings.warn(f\"'{model}' is deprecated; use '{new_model}' instead.\", FutureWarning)\n",
    "                model = new_model\n",
    "            if model not in self.supported_models:\n",
    "                raise ValueError(\n",
    "                    f'unsupported model: {kwargs[\"model\"]} '\n",
    "                    f'supported models: {\", \".join(self.supported_models)}'\n",
    "                )\n",
    "        return func(self, *args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def remove_unused_categories(df: pd.DataFrame, col: str):\n",
    "    \"\"\"Check if col exists in df and if it is a category column.\n",
    "    In that case, it removes the unused levels.\"\"\"\n",
    "    if df is not None and col in df:\n",
    "        if df[col].dtype == 'category':\n",
    "            df = df.copy()\n",
    "            df[col] = df[col].cat.remove_unused_categories()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def partition_by_uid(func):\n",
    "    def wrapper(self, num_partitions, **kwargs):\n",
    "        if num_partitions is None or num_partitions == 1:\n",
    "            return func(self, **kwargs, num_partitions=1)\n",
    "        df = kwargs.pop('df')\n",
    "        X_df = kwargs.pop('X_df', None)\n",
    "        id_col = kwargs.pop('id_col')\n",
    "        uids = df['unique_id'].unique()\n",
    "        df_lock = Lock()\n",
    "        X_df_lock = Lock()\n",
    "\n",
    "        def partition_by_uid_single(uids_split, func, df, X_df, id_col, **kwargs):\n",
    "            with df_lock:\n",
    "                df_uids = df.query('unique_id in @uids_split')\n",
    "            if X_df is not None:\n",
    "                with X_df_lock:\n",
    "                    X_df_uids = X_df.query('unique_id in @uids_split')\n",
    "            else:\n",
    "                X_df_uids = None\n",
    "            df_uids = remove_unused_categories(df_uids, col=id_col)\n",
    "            X_df_uids = remove_unused_categories(X_df_uids, col=id_col)\n",
    "            kwargs_uids = {'df': df_uids, **kwargs}\n",
    "            if X_df_uids is not None:\n",
    "                kwargs_uids['X_df'] = X_df_uids\n",
    "\n",
    "            kwargs_uids['id_col'] = id_col\n",
    "            results_uids = func(self, **kwargs_uids, num_partitions=1)    \n",
    "\n",
    "            return results_uids\n",
    "\n",
    "        fpartition_by_uid_single = partial(partition_by_uid_single, \n",
    "                                           func=func, \n",
    "                                           df=df, \n",
    "                                           X_df=X_df, \n",
    "                                           id_col=id_col, \n",
    "                                           **kwargs)\n",
    "\n",
    "        num_processes = min(num_partitions, cpu_count())\n",
    "        uids_splits = np.array_split(uids, num_processes)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
    "            results_uids = [\n",
    "                executor.submit(fpartition_by_uid_single, uids_split)\n",
    "                for uids_split in uids_splits\n",
    "            ]\n",
    "        \n",
    "        results_df = pd.concat([result_df.result() for result_df in results_uids]).reset_index(drop=True)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    return wrapper\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _NixtlaClient:\n",
    "    \"\"\"\n",
    "    A class used to interact with Nixtla API.\n",
    "    \"\"\"\n",
    "    @deprecated_token\n",
    "    @deprecated_environment\n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: Optional[str] = None, \n",
    "        base_url: Optional[str] = None,\n",
    "        max_retries: int = 6,\n",
    "        retry_interval: int = 10,\n",
    "        max_wait_time: int = 6 * 60,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the NixtlaClient object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_key : str, (default=None)\n",
    "            The authorization api_key interacts with the Nixtla API.\n",
    "            If not provided, it will be inferred by the NIXTLA_API_KEY environment variable.\n",
    "        base_url : str, (default=None)\n",
    "            Custom base_url. Pass only if provided.\n",
    "        max_retries : int, (default=6)\n",
    "            The maximum number of attempts to make when calling the API before giving up. \n",
    "            It defines how many times the client will retry the API call if it fails. \n",
    "            Default value is 6, indicating the client will attempt the API call up to 6 times in total\n",
    "        retry_interval : int, (default=10)\n",
    "            The interval in seconds between consecutive retry attempts. \n",
    "            This is the waiting period before the client tries to call the API again after a failed attempt. \n",
    "            Default value is 10 seconds, meaning the client waits for 10 seconds between retries.\n",
    "        max_wait_time : int, (default=360)\n",
    "            The maximum total time in seconds that the client will spend on all retry attempts before giving up. \n",
    "            This sets an upper limit on the cumulative waiting time for all retry attempts. \n",
    "            If this time is exceeded, the client will stop retrying and raise an exception. \n",
    "            Default value is 360 seconds, meaning the client will cease retrying if the total time \n",
    "            spent on retries exceeds 360 seconds. \n",
    "            The client throws a ReadTimeout error after 60 seconds of inactivity. If you want to \n",
    "            catch these errors, use max_wait_time >> 60. \n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            timegpt_token = os.environ.get(\"TIMEGPT_TOKEN\")\n",
    "            if timegpt_token is not None:\n",
    "                warnings.warn(\n",
    "                    f\"`TIMEGPT_TOKEN` environment variable is deprecated; \"\n",
    "                    \"use `NIXTLA_API_KEY` instead.\", \n",
    "                    FutureWarning,\n",
    "                )\n",
    "            api_key = os.environ.get(\"NIXTLA_API_KEY\", timegpt_token)\n",
    "        if api_key is None:\n",
    "            raise Exception(\n",
    "                'The api_key must be set either by passing `api_key` '\n",
    "                'or by setting the `NIXTLA_API_KEY` environment variable.'\n",
    "            )\n",
    "        if base_url is None:\n",
    "            base_url = os.environ.get(\n",
    "                \"NIXTLA_BASE_URL\", \n",
    "                \"https://dashboard.nixtla.io/api\",\n",
    "            )\n",
    "        self.client = Nixtla(base_url=base_url, token=api_key)\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_interval = retry_interval\n",
    "        self.max_wait_time = max_wait_time\n",
    "        self.supported_models = ['timegpt-1', 'timegpt-1-long-horizon'] \n",
    "        # custom attr\n",
    "        self.weights_x: pd.DataFrame = None\n",
    "\n",
    "    @use_validate_api_key\n",
    "    def validate_token(self):\n",
    "        \"\"\"this is deprecated in favor of validate_api_key\"\"\"\n",
    "        pass\n",
    "\n",
    "    def validate_api_key(self, log: bool = True) -> bool:\n",
    "        \"\"\"Returns True if your api_key is valid.\"\"\"\n",
    "        valid = False\n",
    "        try:\n",
    "            validation = self.client.validate_token()\n",
    "        except:\n",
    "            validation = dict()\n",
    "        if 'message' in validation:\n",
    "            if validation['message'] == 'success':\n",
    "                valid = True\n",
    "        elif 'detail' in validation:\n",
    "            if 'Forecasting! :)' in validation['detail']:\n",
    "                valid = True\n",
    "        if 'support' in validation and log:\n",
    "            main_logger.info(f'Happy Forecasting! :), {validation[\"support\"]}')\n",
    "        return valid\n",
    "\n",
    "    def _uids_to_categorical(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        X_df: Optional[pd.DataFrame],\n",
    "        id_col: str,\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.CategoricalDtype]]:\n",
    "        if id_col not in df:\n",
    "            return df, X_df, None\n",
    "        df = df.copy(deep=False)\n",
    "        if isinstance(df[id_col].dtype, pd.CategoricalDtype):\n",
    "            dtype = df[id_col].dtype\n",
    "        else:\n",
    "            dtype = pd.CategoricalDtype(categories=df[id_col].unique())\n",
    "            df[id_col] = df[id_col].astype(dtype)\n",
    "        df[id_col] = df[id_col].cat.codes\n",
    "        if X_df is not None:\n",
    "            X_df = X_df.copy(deep=False)\n",
    "            if X_df[id_col].dtype != dtype:\n",
    "                X_df[id_col] = X_df[id_col].astype(dtype)\n",
    "            X_df[id_col] = X_df[id_col].cat.codes\n",
    "        return df, X_df, dtype\n",
    "\n",
    "    def _restore_uids(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        dtype: Optional[pd.CategoricalDtype],\n",
    "        id_col: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        if id_col not in df:\n",
    "            return df\n",
    "        assert dtype is not None\n",
    "        df = df.copy(deep=False)\n",
    "        code2cat = dict(enumerate(dtype.categories))\n",
    "        df[id_col] = df[id_col].map(code2cat)\n",
    "        return df\n",
    "\n",
    "    @validate_model_parameter\n",
    "    @partition_by_uid\n",
    "    def _forecast(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        h: int,\n",
    "        freq: Optional[str] = None,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[pd.DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        quantiles: Optional[List[float]] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        finetune_loss: str = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        validate_api_key: bool = False,\n",
    "        add_history: bool = False,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        model: str = 'timegpt-1',\n",
    "        num_partitions: int = 1,\n",
    "    ):\n",
    "        if validate_api_key and not self.validate_api_key(log=False):\n",
    "            raise Exception(\n",
    "                'API Key not valid, please email ops@nixtla.io'\n",
    "            )\n",
    "        nixtla_client_model = _NixtlaClientModel(\n",
    "            client=self.client,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            freq=freq,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            finetune_steps=finetune_steps,\n",
    "            finetune_loss=finetune_loss,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            model=model,\n",
    "            max_retries=self.max_retries,\n",
    "            retry_interval=self.retry_interval,\n",
    "            max_wait_time=self.max_wait_time,   \n",
    "        )\n",
    "        df, X_df, uids_dtype = self._uids_to_categorical(df=df, X_df=X_df, id_col=id_col)\n",
    "        fcst_df = nixtla_client_model.forecast(df=df, X_df=X_df, add_history=add_history)\n",
    "        fcst_df = self._restore_uids(fcst_df, dtype=uids_dtype, id_col=id_col)\n",
    "        self.weights_x = nixtla_client_model.weights_x\n",
    "        return fcst_df\n",
    "\n",
    "    @validate_model_parameter\n",
    "    @partition_by_uid\n",
    "    def _detect_anomalies(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        freq: Optional[str] = None,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Union[int, float] = 99,\n",
    "        clean_ex_first: bool = True,\n",
    "        validate_api_key: bool = False,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        model: str = 'timegpt-1',\n",
    "        num_partitions: int = 1,\n",
    "    ):\n",
    "        if validate_api_key and not self.validate_api_key(log=False):\n",
    "            raise Exception(\n",
    "                'API Key not valid, please email ops@nixtla.io'\n",
    "            )\n",
    "        nixtla_client_model = _NixtlaClientModel(\n",
    "            client=self.client,\n",
    "            h=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            freq=freq,\n",
    "            level=level,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            model=model,\n",
    "            max_retries=self.max_retries,\n",
    "            retry_interval=self.retry_interval,\n",
    "            max_wait_time=self.max_wait_time,\n",
    "        )\n",
    "        df, X_df, uids_dtype = self._uids_to_categorical(df=df, X_df=None, id_col=id_col)\n",
    "        anomalies_df = nixtla_client_model.detect_anomalies(df=df)\n",
    "        anomalies_df = self._restore_uids(anomalies_df, dtype=uids_dtype, id_col=id_col)\n",
    "        self.weights_x = nixtla_client_model.weights_x\n",
    "        return anomalies_df\n",
    "\n",
    "    @validate_model_parameter\n",
    "    @partition_by_uid\n",
    "    def _cross_validation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        h: int,\n",
    "        freq: Optional[str] = None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        quantiles: Optional[List[float]] = None,\n",
    "        validate_api_key: bool = False,\n",
    "        n_windows: int = 1,\n",
    "        step_size: Optional[int] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        finetune_loss: str = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        model: str = 'timegpt-1',\n",
    "        num_partitions: int = 1,\n",
    "    ):\n",
    "        if validate_api_key and not self.validate_api_key(log=False):\n",
    "            raise Exception(\n",
    "                'API Key not valid, please email ops@nixtla.io'\n",
    "            )\n",
    "        nixtla_client_model = _NixtlaClientModel(\n",
    "            client=self.client,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            freq=freq,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            finetune_steps=finetune_steps,\n",
    "            finetune_loss=finetune_loss,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            model=model,\n",
    "            max_retries=self.max_retries,\n",
    "            retry_interval=self.retry_interval,\n",
    "            max_wait_time=self.max_wait_time,\n",
    "        )\n",
    "        df, X_df, uids_dtype = self._uids_to_categorical(df=df, X_df=None, id_col=id_col)\n",
    "        cv_df = nixtla_client_model.cross_validation(df=df, n_windows=n_windows, step_size=step_size)\n",
    "        cv_df = self._restore_uids(cv_df, dtype=uids_dtype, id_col=id_col)\n",
    "        self.weights_x = nixtla_client_model.weights_x\n",
    "        return cv_df\n",
    "    \n",
    "    def plot(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        forecasts_df: Optional[pd.DataFrame] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        unique_ids: Union[Optional[List[str]], np.ndarray] = None,\n",
    "        plot_random: bool = True,\n",
    "        models: Optional[List[str]] = None,\n",
    "        level: Optional[List[float]] = None,\n",
    "        max_insample_length: Optional[int] = None,\n",
    "        plot_anomalies: bool = False,\n",
    "        engine: str = 'matplotlib',\n",
    "        resampler_kwargs: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"Plot forecasts and insample values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        forecasts_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and models.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        unique_ids : List[str], optional (default=None)\n",
    "            Time Series to plot.\n",
    "            If None, time series are selected randomly.\n",
    "        plot_random : bool (default=True)\n",
    "            Select time series to plot randomly.\n",
    "        models : List[str], optional (default=None)\n",
    "            List of models to plot.\n",
    "        level : List[float], optional (default=None)\n",
    "            List of prediction intervals to plot if paseed.\n",
    "        max_insample_length : int, optional (default=None)\n",
    "            Max number of train/insample observations to be plotted.\n",
    "        plot_anomalies : bool (default=False)\n",
    "            Plot anomalies for each prediction interval.\n",
    "        engine : str (default='plotly')\n",
    "            Library used to plot. 'plotly', 'plotly-resampler' or 'matplotlib'.\n",
    "        resampler_kwargs : dict\n",
    "            Kwargs to be passed to plotly-resampler constructor.\n",
    "            For further custumization (\"show_dash\") call the method,\n",
    "            store the plotting object and add the extra arguments to\n",
    "            its `show_dash` method.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from utilsforecast.plotting import plot_series\n",
    "        except ModuleNotFoundError:\n",
    "            raise Exception(\n",
    "                'You have to install additional dependencies to use this method, '\n",
    "                'please install them using `pip install \"nixtla[plotting]\"`'\n",
    "            )\n",
    "        df = df.copy()\n",
    "        if id_col not in df:\n",
    "            df[id_col] = 'ts_0'\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "        if forecasts_df is not None:\n",
    "            forecasts_df = forecasts_df.copy()\n",
    "            if id_col not in forecasts_df:\n",
    "                forecasts_df[id_col] = 'ts_0'\n",
    "            forecasts_df[time_col] = pd.to_datetime(forecasts_df[time_col])\n",
    "            if 'anomaly' in forecasts_df:\n",
    "                # special case to plot outputs\n",
    "                # from detect_anomalies\n",
    "                forecasts_df = forecasts_df.drop(columns='anomaly')\n",
    "                cols = forecasts_df.columns\n",
    "                cols = cols[cols.str.contains('TimeGPT-lo-')]\n",
    "                level = cols.str.replace('TimeGPT-lo-', '')[0]\n",
    "                level = float(level) if '.' in level else int(level)\n",
    "                level = [level]\n",
    "                plot_anomalies = True\n",
    "                models = ['TimeGPT']\n",
    "                forecasts_df = df.merge(forecasts_df, how='left')\n",
    "                df = df.groupby('unique_id').head(1)\n",
    "                # prevent double plotting\n",
    "                df.loc[:, target_col] = np.nan\n",
    "        return plot_series(\n",
    "            df=df,\n",
    "            forecasts_df=forecasts_df,\n",
    "            ids=unique_ids,\n",
    "            plot_random=plot_random,\n",
    "            models=models,\n",
    "            level=level,\n",
    "            max_insample_length=max_insample_length,\n",
    "            plot_anomalies=plot_anomalies,\n",
    "            engine=engine,\n",
    "            resampler_kwargs=resampler_kwargs,\n",
    "            palette=\"tab20b\",\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class NixtlaClient(_NixtlaClient):\n",
    "\n",
    "    def _instantiate_distributed_nixtla_client(self):\n",
    "        from nixtla.distributed.nixtla_client import _DistributedNixtlaClient\n",
    "        dist_nixtla_client = _DistributedNixtlaClient(\n",
    "            api_key=self.client._client_wrapper._token,\n",
    "            base_url=self.client._client_wrapper._base_url,\n",
    "            max_retries=self.max_retries,\n",
    "            retry_interval=self.retry_interval,\n",
    "            max_wait_time=self.max_wait_time,\n",
    "        )\n",
    "        return dist_nixtla_client\n",
    "\n",
    "    @deprecated_fewshot_loss\n",
    "    @deprecated_fewshot_steps\n",
    "    def forecast(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            h: int,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            X_df: Optional[pd.DataFrame] = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            quantiles: Optional[List[float]] = None,\n",
    "            finetune_steps: int = 0,\n",
    "            finetune_loss: str = 'default',\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_api_key: bool = False,\n",
    "            add_history: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            model: str = 'timegpt-1',\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        quantiles : List[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the\n",
    "            new data.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before \n",
    "            sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._forecast(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        else:\n",
    "            dist_nixtla_client = self._instantiate_distributed_nixtla_client()\n",
    "            return dist_nixtla_client.forecast(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "            \n",
    "    def detect_anomalies(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            level: Union[int, float] = 99,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_api_key: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            model: str = 'timegpt-1',\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ):\n",
    "        \"\"\"Detect anomalies in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before \n",
    "            sending requests.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        anomalies_df : pandas.DataFrame\n",
    "            DataFrame with anomalies flagged with 1 detected by TimeGPT.\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._detect_anomalies(\n",
    "                df=df,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        else:\n",
    "            dist_nixtla_client = self._instantiate_distributed_nixtla_client()\n",
    "            return dist_nixtla_client.detect_anomalies(\n",
    "                df=df,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "\n",
    "    @deprecated_fewshot_loss\n",
    "    @deprecated_fewshot_steps\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        h: int,\n",
    "        freq: Optional[str] = None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        quantiles: Optional[List[float]] = None,\n",
    "        validate_api_key: bool = False,\n",
    "        n_windows: int = 1,\n",
    "        step_size: Optional[int] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        finetune_loss: str = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        model: str = 'timegpt-1',\n",
    "        num_partitions: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Perform cross validation in your time series using TimeGPT.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for prediction intervals.\n",
    "        quantiles : List[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names..\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before\n",
    "            sending requests.\n",
    "        n_windows : int (defaul=1)\n",
    "            Number of windows to evaluate.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the\n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cv_df : pandas.DataFrame\n",
    "            DataFrame with cross validation forecasts.\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._cross_validation(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        else:\n",
    "            dist_nixtla_client = self._instantiate_distributed_nixtla_client()\n",
    "            return dist_nixtla_client.cross_validation(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TimeGPT(NixtlaClient):\n",
    "    \"\"\"\n",
    "    Class `TimeGPT` is deprecated; use `NixtlaClient` instead.\n",
    "\n",
    "    This class is deprecated and may be removed in future releases.\n",
    "    Please use `NixtlaClient` instead.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        warnings.warn(\n",
    "            \"Class `TimeGPT` is deprecated; use `NixtlaClient` instead.\", \n",
    "            FutureWarning,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test warns timegpt deprecation\n",
    "test_warns(\n",
    "    lambda: TimeGPT(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.__init__, title_level=2, name='NixtlaClient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeGPT, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "@contextmanager\n",
    "def delete_env_var(key):\n",
    "    original_value = os.environ.get(key)\n",
    "    rm = False\n",
    "    if key in os.environ:\n",
    "        del os.environ[key]\n",
    "        rm = True\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if rm:\n",
    "            os.environ[key] = original_value\n",
    "# test api_key fail\n",
    "with delete_env_var('NIXTLA_API_KEY'), delete_env_var('TIMEGPT_TOKEN'):\n",
    "    test_fail(\n",
    "        lambda: NixtlaClient(),\n",
    "        contains='NIXTLA_API_KEY',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nixtla_client = NixtlaClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test token and environment deprecation\n",
    "test_warns(\n",
    "    lambda: NixtlaClient(token='token'),\n",
    ")\n",
    "test_warns(\n",
    "    lambda: NixtlaClient(environment='token'),\n",
    ")\n",
    "test_warns(\n",
    "    lambda: NixtlaClient(token='token', environment='token'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.validate_api_key, title_level=2, name='NixtlaClient.validate_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.validate_token, title_level=4, name='NixtlaClient.validate_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test validate_token deprecation\n",
    "test_eq(\n",
    "    nixtla_client.validate_api_key(),\n",
    "    nixtla_client.validate_token(),\n",
    ")\n",
    "\n",
    "_nixtla_client = NixtlaClient(api_key=\"invalid\")\n",
    "test_eq(\n",
    "    _nixtla_client.validate_api_key(),\n",
    "    _nixtla_client.validate_token(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "_nixtla_client = NixtlaClient(\n",
    "    api_key=os.environ['NIXTLA_API_KEY_CUSTOM'], \n",
    "    base_url=os.environ['NIXTLA_BASE_URL_CUSTOM'],\n",
    ")\n",
    "_nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "@contextmanager\n",
    "def add_env_var(key, val):\n",
    "    original_value = os.environ.get(key)\n",
    "    to_rm = False \n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = val\n",
    "        to_rm = True\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if to_rm: \n",
    "            del os.environ[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test TIMEGPT_TOKEN deprecation\n",
    "with delete_env_var(\"TIMEGPT_TOKEN\"), delete_env_var(\"NIXTLA_API_KEY\"), add_env_var(\"TIMEGPT_TOKEN\", \"token\"):\n",
    "    test_warns(lambda: NixtlaClient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    lambda: NixtlaClient(api_key='transphobic').forecast(df=pd.DataFrame(), h=None, validate_api_key=True),\n",
    "    contains='nixtla'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test input_size\n",
    "test_eq(\n",
    "    nixtla_client.client.model_params(request=SingleSeriesForecast(freq='D'))['data']['detail'],\n",
    "    {'input_size': 28, 'horizon': 7},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to make forecasts! Let's import an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test model change\n",
    "df_test = df.copy()\n",
    "df_test.columns = [\"ds\", \"y\"]\n",
    "test_warns(\n",
    "    lambda: nixtla_client.forecast(df_test, finetune_steps=2, h=12, model=\"short-horizon\"),\n",
    ")\n",
    "test_warns(\n",
    "    lambda: nixtla_client.forecast(df_test, finetune_steps=2, h=12, model=\"long-horizon\"),\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    nixtla_client.forecast(df_test, h=12, model=\"short-horizon\"),\n",
    "    nixtla_client.forecast(df_test, h=12),\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    nixtla_client.forecast(df_test, h=12, model=\"timegpt-1-long-horizon\"),\n",
    "    nixtla_client.forecast(df_test, h=12, model=\"long-horizon\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test fewshot deprecation\n",
    "test_warns(\n",
    "    lambda: nixtla_client.forecast(df_test, fewshot_steps=2, h=12),\n",
    ")\n",
    "test_warns(\n",
    "    lambda: nixtla_client.forecast(df_test, fewshot_steps=2, finetune_loss=\"mse\", h=12),\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    nixtla_client.forecast(df_test, fewshot_steps=2, h=12),\n",
    "    nixtla_client.forecast(df_test, finetune_steps=2, h=12),\n",
    "    atol=1,\n",
    "    rtol=0,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    nixtla_client.forecast(df_test, fewshot_steps=2, fewshot_loss=\"mse\", h=12),\n",
    "    nixtla_client.forecast(df_test, finetune_steps=2, finetune_loss=\"mse\", h=12),\n",
    "    atol=1,\n",
    "    rtol=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from typing import Callable\n",
    "from utilsforecast.data import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test date_features with multiple series\n",
    "# and different ends\n",
    "test_series = generate_series(n_series=2, min_length=5, max_length=20)\n",
    "h = 12\n",
    "fcst_test_series = nixtla_client.forecast(test_series, h=12, date_features=['dayofweek'])\n",
    "uids = test_series['unique_id']\n",
    "for uid in uids:\n",
    "    test_eq(\n",
    "        fcst_test_series.query('unique_id == @uid')['ds'].values,\n",
    "        pd.date_range(periods=h + 1, start=test_series.query('unique_id == @uid')['ds'].max())[1:].astype(str),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test resample with timestamps at non standard cuts\n",
    "custom_dates = pd.date_range('2000-01-01 00:04:00', freq='5min', periods=100)\n",
    "custom_df = pd.DataFrame(\n",
    "    {\n",
    "        'unique_id': np.repeat(np.array([0, 1]), 50),\n",
    "        'ds': custom_dates,\n",
    "        'y': np.arange(100),\n",
    "    }\n",
    ")\n",
    "# drop second row from each serie\n",
    "custom_df = custom_df.drop([1, 51])\n",
    "model = _NixtlaClientModel(\n",
    "    client=nixtla_client,\n",
    "    h=1,\n",
    "    freq='5min'\n",
    ")\n",
    "resampled_df = model.resample_dataframe(custom_df)\n",
    "# we do a backfill so the second row must've got the value of the third row\n",
    "assert resampled_df.loc[1, 'y'] == resampled_df.loc[2, 'y']\n",
    "assert resampled_df.loc[51, 'y'] == resampled_df.loc[52, 'y']\n",
    "pd.testing.assert_series_equal(\n",
    "    resampled_df['ds'],\n",
    "    custom_dates.to_series(index=resampled_df.index, name='ds').astype(str),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test quantiles\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(\n",
    "        df=df, \n",
    "        h=12, \n",
    "        time_col='timestamp', \n",
    "        target_col='value', \n",
    "        level=[80], \n",
    "        quantiles=[0.2, 0.3]\n",
    "    ),\n",
    "    contains='not both'\n",
    ")\n",
    "test_qls = list(np.arange(0.1, 1, 0.1))\n",
    "exp_q_cols = [f\"TimeGPT-q-{int(100 * q)}\" for q in test_qls]\n",
    "def test_method_qls(method, **kwargs):\n",
    "    df_qls = method(\n",
    "        df=df, \n",
    "        h=12, \n",
    "        time_col='timestamp', \n",
    "        target_col='value', \n",
    "        quantiles=test_qls,\n",
    "        **kwargs\n",
    "    )\n",
    "    assert all(col in df_qls.columns for col in exp_q_cols)\n",
    "    # test monotonicity of quantiles\n",
    "    df_qls.apply(lambda x: x.is_monotonic_increasing, axis=1).sum() == len(exp_q_cols)\n",
    "test_method_qls(nixtla_client.forecast)\n",
    "test_method_qls(nixtla_client.forecast, add_history=True)\n",
    "test_method_qls(nixtla_client.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test num partitions\n",
    "# we need to be sure that we can recover the same results\n",
    "# using a for loop\n",
    "# A: be aware that num partitons can produce different results\n",
    "# when used finetune_steps\n",
    "def test_num_partitions_same_results(method: Callable, num_partitions: int, **kwargs):\n",
    "    res_partitioned = method(**kwargs, num_partitions=num_partitions)\n",
    "    res_no_partitioned = method(**kwargs, num_partitions=1)\n",
    "    sort_by = ['unique_id', 'ds']\n",
    "    if 'cutoff' in res_partitioned:\n",
    "        sort_by.extend(['cutoff'])\n",
    "    pd.testing.assert_frame_equal(\n",
    "        res_partitioned.sort_values(sort_by).reset_index(drop=True), \n",
    "        res_no_partitioned.sort_values(sort_by).reset_index(drop=True),\n",
    "        rtol=1e-2,\n",
    "        atol=1e-2,\n",
    "    )\n",
    "    \n",
    "freqs = {'D': 7, 'W-THU': 52, 'Q-DEC': 8, '15T': 4 * 24 * 7}\n",
    "for freq, h in freqs.items():\n",
    "    df_freq = generate_series(\n",
    "        10, \n",
    "        min_length=500 if freq != '15T' else 1_200, \n",
    "        max_length=550 if freq != '15T' else 2_000,\n",
    "    )\n",
    "    #df_freq['y'] = df_freq['y'].astype(np.float32)\n",
    "    df_freq['ds'] = df_freq.groupby('unique_id')['ds'].transform(\n",
    "        lambda x: pd.date_range(periods=len(x), freq=freq, end='2023-01-01')\n",
    "    )\n",
    "    min_size = df_freq.groupby('unique_id').size().min()\n",
    "    test_num_partitions_same_results(\n",
    "        nixtla_client.detect_anomalies,\n",
    "        level=98,\n",
    "        df=df_freq,\n",
    "        num_partitions=2,\n",
    "    )\n",
    "    test_num_partitions_same_results(\n",
    "        nixtla_client.cross_validation,\n",
    "        h=7,\n",
    "        n_windows=2,\n",
    "        df=df_freq,\n",
    "        num_partitions=2,\n",
    "    )\n",
    "    test_num_partitions_same_results(\n",
    "        nixtla_client.forecast,\n",
    "        df=df_freq,\n",
    "        h=7,\n",
    "        add_history=True,\n",
    "        num_partitions=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from itertools import product\n",
    "from time import time, sleep\n",
    "from unittest.mock import patch\n",
    "\n",
    "from requests.exceptions import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from httpx import ReadTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_retry_behavior(side_effect, max_retries=5, retry_interval=5, max_wait_time=40, should_retry=True, sleep_seconds=5):\n",
    "    mock_nixtla_client = NixtlaClient(\n",
    "        max_retries=max_retries, \n",
    "        retry_interval=retry_interval, \n",
    "        max_wait_time=max_wait_time,\n",
    "    )\n",
    "    init_time = time()\n",
    "    with patch('nixtla.client.Nixtla.forecast_multi_series', side_effect=side_effect):\n",
    "        test_fail(\n",
    "            lambda: mock_nixtla_client.forecast(df=df, h=12, time_col='timestamp', target_col='value'),\n",
    "        )\n",
    "    total_mock_time = time() - init_time\n",
    "    if should_retry:\n",
    "        approx_expected_time = min((max_retries - 1) * retry_interval, max_wait_time)\n",
    "        upper_expected_time = min(max_retries * retry_interval, max_wait_time)\n",
    "        assert total_mock_time >= approx_expected_time, \"It is not retrying as expected\"\n",
    "        # preprocessing time before the first api call should be less than 60 seconds\n",
    "        assert total_mock_time - upper_expected_time - (max_retries - 1) * sleep_seconds <= sleep_seconds\n",
    "    else:\n",
    "        assert total_mock_time <= max_wait_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we want the api to retry in these cases\n",
    "def raise_api_error_with_text(*args, **kwargs):\n",
    "    raise ApiError(\n",
    "        status_code=503, \n",
    "        body=\"\"\"\n",
    "        <html><head>\n",
    "        <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
    "        <title>503 Server Error</title>\n",
    "        </head>\n",
    "        <body text=#000000 bgcolor=#ffffff>\n",
    "        <h1>Error: Server Error</h1>\n",
    "        <h2>The service you requested is not available at this time.<p>Service error -27.</h2>\n",
    "        <h2></h2>\n",
    "        </body></html>\n",
    "        \"\"\")\n",
    "test_retry_behavior(raise_api_error_with_text, retry_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# we want the api to not retry in these cases\n",
    "# here A is assuming that the endpoint responds always\n",
    "# with a json\n",
    "def raise_api_error_with_json(*args, **kwargs):\n",
    "    raise ApiError(\n",
    "        status_code=503, \n",
    "        body=dict(detail='Please use numbers'),\n",
    "    )\n",
    "test_retry_behavior(raise_api_error_with_json, should_retry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test resilience of api calls\n",
    "\n",
    "def raise_read_timeout_error(*args, **kwargs):\n",
    "    print(f'raising ReadTimeout error after {sleep_seconds} seconds')\n",
    "    sleep(sleep_seconds)\n",
    "    raise ReadTimeout\n",
    "    \n",
    "def raise_http_error(*args, **kwargs):\n",
    "    print('raising HTTP error')\n",
    "    raise HTTPError(response=dict(status_code=503))\n",
    "    \n",
    "combs = [\n",
    "    (2, 5, 30),\n",
    "    (10, 1, 5),\n",
    "]\n",
    "side_effects = [raise_read_timeout_error, raise_http_error]\n",
    "\n",
    "for (max_retries, retry_interval, max_wait_time), side_effect in product(combs, side_effects):\n",
    "    test_retry_behavior(\n",
    "        max_retries=max_retries, \n",
    "        retry_interval=retry_interval, \n",
    "        max_wait_time=max_wait_time, \n",
    "        side_effect=side_effect,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test pass dataframe with index\n",
    "df_ds_index = df.set_index('timestamp')\n",
    "df_ds_index.index = pd.DatetimeIndex(df_ds_index.index, freq='MS')\n",
    "fcst_inferred_df_index = nixtla_client.forecast(df_ds_index, h=10, time_col='timestamp', target_col='value')\n",
    "anom_inferred_df_index = nixtla_client.detect_anomalies(df_ds_index, time_col='timestamp', target_col='value')\n",
    "fcst_inferred_df = nixtla_client.forecast(df, h=10, time_col='timestamp', target_col='value')\n",
    "anom_inferred_df = nixtla_client.detect_anomalies(df, time_col='timestamp', target_col='value')\n",
    "pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df)\n",
    "pd.testing.assert_frame_equal(anom_inferred_df_index, anom_inferred_df)\n",
    "for freq in ['Y', 'W-MON', 'Q-DEC', 'H']:\n",
    "    df_ds_index.index = pd.date_range(end='2023-01-01', periods=len(df), freq=freq)\n",
    "    df_ds_index.index.name = 'timestamp'\n",
    "    df_test = df_ds_index.reset_index()\n",
    "    fcst_inferred_df_index = nixtla_client.forecast(df_ds_index, h=10, time_col='timestamp', target_col='value')\n",
    "    fcst_inferred_df = nixtla_client.forecast(df_test, h=10, time_col='timestamp', target_col='value')\n",
    "    pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.plot, name='NixtlaClient.plot', title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nixtla_client.plot(df, time_col='timestamp', target_col='value', engine='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.forecast, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test we recover the same <mean> forecasts\n",
    "# with and without restricting input\n",
    "# (add_history)\n",
    "\n",
    "def test_equal_fcsts_add_history(**kwargs):\n",
    "    fcst_no_rest_df = nixtla_client.forecast(**kwargs, add_history=True)\n",
    "    fcst_no_rest_df = fcst_no_rest_df.groupby('unique_id').tail(kwargs['h']).reset_index(drop=True)\n",
    "    fcst_rest_df = nixtla_client.forecast(**kwargs)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_no_rest_df,\n",
    "        fcst_rest_df,\n",
    "    )\n",
    "    return fcst_rest_df\n",
    "\n",
    "freqs = {'D': 7, 'W-THU': 52, 'Q-DEC': 8, '15T': 4 * 24 * 7}\n",
    "for freq, h in freqs.items():\n",
    "    df_freq = generate_series(\n",
    "        10, \n",
    "        min_length=500 if freq != '15T' else 1_200, \n",
    "        max_length=550 if freq != '15T' else 2_000,\n",
    "    )\n",
    "    df_freq['ds'] = df_freq.groupby('unique_id', observed=True)['ds'].transform(\n",
    "        lambda x: pd.date_range(periods=len(x), freq=freq, end='2023-01-01')\n",
    "    )\n",
    "    kwargs = dict(\n",
    "        df=df_freq,\n",
    "        h=h,\n",
    "    )\n",
    "    fcst_1_df = test_equal_fcsts_add_history(**{**kwargs, 'model': 'timegpt-1'})\n",
    "    fcst_2_df = test_equal_fcsts_add_history(**{**kwargs, 'model': 'timegpt-1-long-horizon'})\n",
    "    test_fail(\n",
    "        lambda: pd.testing.assert_frame_equal(fcst_1_df, fcst_2_df),\n",
    "        contains='(column name=\"TimeGPT\") are different',\n",
    "    )\n",
    "    # add test num_partitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test same results custom url\n",
    "nixtla_client_custom = NixtlaClient(\n",
    "    api_key=os.environ['NIXTLA_API_KEY_CUSTOM'], \n",
    "    base_url=os.environ['NIXTLA_BASE_URL_CUSTOM'],\n",
    ")\n",
    "# forecast method\n",
    "fcst_kwargs = dict(\n",
    "    df=df, \n",
    "    h=12, \n",
    "    level=[90, 95], \n",
    "    add_history=True, \n",
    "    time_col='timestamp', \n",
    "    target_col='value',\n",
    ")\n",
    "fcst_df = nixtla_client.forecast(**fcst_kwargs)\n",
    "fcst_df_custom = nixtla_client_custom.forecast(**fcst_kwargs)\n",
    "pd.testing.assert_frame_equal(\n",
    "    fcst_df,\n",
    "    fcst_df_custom,\n",
    ")\n",
    "# anomalies method\n",
    "anomalies_kwargs = dict(\n",
    "    df=df, \n",
    "    level=99,\n",
    "    time_col='timestamp', \n",
    "    target_col='value',\n",
    ")\n",
    "anomalies_df = nixtla_client.detect_anomalies(**anomalies_kwargs)\n",
    "anomalies_df_custom = nixtla_client_custom.detect_anomalies(**anomalies_kwargs)\n",
    "pd.testing.assert_frame_equal(\n",
    "    anomalies_df,\n",
    "    anomalies_df_custom,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test different results for different models\n",
    "fcst_kwargs['model'] = 'timegpt-1'\n",
    "fcst_timegpt_1 = nixtla_client.forecast(**fcst_kwargs)\n",
    "fcst_kwargs['model'] = 'timegpt-1-long-horizon'\n",
    "fcst_timegpt_long = nixtla_client.forecast(**fcst_kwargs)\n",
    "test_fail(\n",
    "    lambda: pd.testing.assert_frame_equal(fcst_timegpt_1[['TimeGPT']], fcst_timegpt_long[['TimeGPT']]),\n",
    "    contains='(column name=\"TimeGPT\") are different'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test different results for different models\n",
    "# anomalies\n",
    "anomalies_kwargs['model'] = 'timegpt-1'\n",
    "anomalies_timegpt_1 = nixtla_client.detect_anomalies(**anomalies_kwargs)\n",
    "anomalies_kwargs['model'] = 'timegpt-1-long-horizon'\n",
    "anomalies_timegpt_long = nixtla_client.detect_anomalies(**anomalies_kwargs)\n",
    "test_fail(\n",
    "    lambda: pd.testing.assert_frame_equal(anomalies_timegpt_1[['TimeGPT']], anomalies_timegpt_long[['TimeGPT']]),\n",
    "    contains='(column name=\"TimeGPT\") are different'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unsupported model\n",
    "fcst_kwargs['model'] = 'a-model'\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(**fcst_kwargs),\n",
    "    contains='unsupported model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test unsupported model\n",
    "anomalies_kwargs['model'] = 'my-awesome-model'\n",
    "test_fail(\n",
    "    lambda: nixtla_client.detect_anomalies(**anomalies_kwargs),\n",
    "    contains='unsupported model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test make future dataframe for one series\n",
    "df_ = df.rename(columns={'timestamp': 'ds', 'value': 'y'})\n",
    "df_.insert(0, 'unique_id', 'AirPassengers')\n",
    "df_actual_future = df_.tail(12)[['unique_id', 'ds']]\n",
    "df_history = df_.drop(df_actual_future.index)\n",
    "df_future = _NixtlaClientModel(client=nixtla_client.client, h=12, freq='MS').make_future_dataframe(df_history)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_actual_future.reset_index(drop=True),\n",
    "    df_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features\n",
    "date_features = ['year', 'month']\n",
    "df_date_features, future_df = _NixtlaClientModel(\n",
    "    client=nixtla_client.client,\n",
    "    h=12, \n",
    "    freq='MS', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=None,\n",
    ").add_date_features(df_,  X_df=None)\n",
    "assert all(col in df_date_features for col in date_features)\n",
    "assert all(col in future_df for col in date_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.cross_validation, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# cross validation tests\n",
    "df_copy = df_.copy()\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_copy,\n",
    "    df_,\n",
    ")\n",
    "df_test = df_.groupby('unique_id').tail(12)\n",
    "df_train = df_.drop(df_test.index)\n",
    "hyps = [\n",
    "    # finetune steps is unstable due\n",
    "    # to numerical reasons\n",
    "    # dict(finetune_steps=2),\n",
    "    dict(),\n",
    "    dict(clean_ex_first=False),\n",
    "    dict(date_features=['month']),\n",
    "    dict(level=[80, 90]),\n",
    "    #dict(level=[80, 90], finetune_steps=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test exogenous variables cv\n",
    "df_ex_ = df_.copy()\n",
    "df_ex_['exogenous_var'] = df_ex_['y'] + np.random.normal(size=len(df_ex_))\n",
    "x_df_test = df_test.drop(columns='y').merge(df_ex_.drop(columns='y'))\n",
    "for hyp in hyps:\n",
    "    main_logger.info(f'Hyperparameters: {hyp}')\n",
    "    main_logger.info('\\n\\nPerforming forecast\\n')\n",
    "    fcst_test = nixtla_client.forecast(df_train.merge(df_ex_.drop(columns='y')), h=12, X_df=x_df_test, **hyp)\n",
    "    fcst_test = df_test[['unique_id', 'ds', 'y']].merge(fcst_test)\n",
    "    fcst_test = fcst_test.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    main_logger.info('\\n\\nPerforming Cross validation\\n')\n",
    "    fcst_cv = nixtla_client.cross_validation(df_ex_, h=12, **hyp)\n",
    "    fcst_cv = fcst_cv.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    main_logger.info('\\n\\nVerify difference\\n')\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "        #rtol=1e-2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for hyp in hyps:\n",
    "    fcst_test = nixtla_client.forecast(df_train, h=12, **hyp)\n",
    "    fcst_test = df_test[['unique_id', 'ds', 'y']].merge(fcst_test)\n",
    "    fcst_test = fcst_test.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    fcst_cv = nixtla_client.cross_validation(df_, h=12, **hyp)\n",
    "    fcst_cv = fcst_cv.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "        rtol=1e-2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for hyp in hyps:\n",
    "    fcst_test = nixtla_client.forecast(df_train, h=12, **hyp)\n",
    "    fcst_test.insert(2, 'y', df_test['y'].values)\n",
    "    fcst_test = fcst_test.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    fcst_cv = nixtla_client.cross_validation(df_, h=12, **hyp)\n",
    "    fcst_cv = fcst_cv.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "        rtol=1e-2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nixtla.date_features import SpecialDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add callables\n",
    "date_features = [SpecialDates({'first_dates': ['2021-01-1'], 'second_dates': ['2021-01-01']})]\n",
    "df_daily = df_.copy()\n",
    "df_daily['ds'] = pd.date_range(end='2021-01-01', periods=len(df_daily))\n",
    "df_date_features, future_df = _NixtlaClientModel(\n",
    "    client=nixtla_client.client,\n",
    "    h=12, \n",
    "    freq='D', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=None,\n",
    ").add_date_features(df_,  X_df=None)\n",
    "assert all(col in df_date_features for col in ['first_dates', 'second_dates'])\n",
    "assert all(col in future_df for col in ['first_dates', 'second_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features one hot encoded\n",
    "date_features = ['year', 'month']\n",
    "date_features_to_one_hot = ['month']\n",
    "df_date_features, future_df = _NixtlaClientModel(\n",
    "    client=nixtla_client.client,\n",
    "    h=12, \n",
    "    freq='D', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=date_features_to_one_hot,\n",
    ").add_date_features(df_,  X_df=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test future dataframe for multiple series\n",
    "df_ = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-with-ex-vars.csv')\n",
    "df_actual_future = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-future-ex-vars.csv')\n",
    "df_future = _NixtlaClientModel(\n",
    "        client=nixtla_client.client, \n",
    "        h=24, \n",
    "        freq='H',\n",
    "    ).make_future_dataframe(df_[['unique_id', 'ds', 'y']])\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_actual_future[['unique_id', 'ds']],\n",
    "    df_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pass dataframe with index\n",
    "df_ds_index = df_.set_index('ds')[['unique_id', 'y']]\n",
    "df_ds_index.index = pd.DatetimeIndex(df_ds_index.index)\n",
    "fcst_inferred_df_index = nixtla_client.forecast(df_ds_index, h=10)\n",
    "anom_inferred_df_index = nixtla_client.detect_anomalies(df_ds_index)\n",
    "fcst_inferred_df = nixtla_client.forecast(df_[['ds', 'unique_id', 'y']], h=10)\n",
    "anom_inferred_df = nixtla_client.detect_anomalies(df_[['ds', 'unique_id', 'y']])\n",
    "pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df, atol=1e-3)\n",
    "pd.testing.assert_frame_equal(anom_inferred_df_index, anom_inferred_df, atol=1e-3)\n",
    "df_ds_index = df_ds_index.groupby('unique_id').tail(80)\n",
    "for freq in ['Y', 'W-MON', 'Q-DEC', 'H']:\n",
    "    df_ds_index.index = np.concatenate(\n",
    "        df_ds_index['unique_id'].nunique() * [pd.date_range(end='2023-01-01', periods=80, freq=freq)]\n",
    "    )\n",
    "    df_ds_index.index.name = 'ds'\n",
    "    fcst_inferred_df_index = nixtla_client.forecast(df_ds_index, h=10)\n",
    "    df_test = df_ds_index.reset_index()\n",
    "    fcst_inferred_df = nixtla_client.forecast(df_test, h=10)\n",
    "    pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features with exogenous variables \n",
    "# and multiple series\n",
    "date_features = ['year', 'month']\n",
    "df_date_features, future_df = _NixtlaClientModel(\n",
    "    client=nixtla_client.client,\n",
    "    h=24, \n",
    "    freq='H', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=None,\n",
    ").add_date_features(df_,  X_df=df_actual_future)\n",
    "assert all(col in df_date_features for col in date_features)\n",
    "assert all(col in future_df for col in date_features)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_date_features[df_.columns],\n",
    "    df_,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    future_df[df_actual_future.columns],\n",
    "    df_actual_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features one hot with exogenous variables \n",
    "# and multiple series\n",
    "date_features = ['month', 'day']\n",
    "df_date_features, future_df = _NixtlaClientModel(\n",
    "    client=nixtla_client.client,\n",
    "    h=24, \n",
    "    freq='H', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=date_features,\n",
    ").add_date_features(df_,  X_df=df_actual_future)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_date_features[df_.columns],\n",
    "    df_,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    future_df[df_actual_future.columns],\n",
    "    df_actual_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test warning horizon too long\n",
    "nixtla_client.forecast(df=df.tail(3), h=100, time_col='timestamp', target_col='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with add_history\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', add_history=True),\n",
    "    contains='be sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with finetunning\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', finetune_steps=10, finetune_loss='mae'),\n",
    "    contains='be sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with level\n",
    "test_fail(\n",
    "    lambda: nixtla_client.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', level=[80, 90]),\n",
    "    contains='be sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test custom url\n",
    "# same results\n",
    "_timegpt_fcst_df = _nixtla_client.forecast(df=df, h=12, time_col='timestamp', target_col='value')\n",
    "timegpt_fcst_df = nixtla_client.forecast(df=df, h=12, time_col='timestamp', target_col='value')\n",
    "pd.testing.assert_frame_equal(\n",
    "    _timegpt_fcst_df,\n",
    "    timegpt_fcst_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test using index as time_col\n",
    "# same results\n",
    "df_test = df.copy()\n",
    "df_test[\"timestamp\"] = pd.to_datetime(df_test[\"timestamp\"])\n",
    "df_test.set_index(df_test[\"timestamp\"], inplace=True)\n",
    "df_test.drop(columns=\"timestamp\", inplace=True)\n",
    "\n",
    "# Using user_provided time_col and freq\n",
    "timegpt_anomalies_df_1 = nixtla_client.detect_anomalies(df, time_col='timestamp', target_col='value', freq= 'M')\n",
    "# Infer time_col and freq from index\n",
    "timegpt_anomalies_df_2 = nixtla_client.detect_anomalies(df_test, time_col='timestamp', target_col='value')\n",
    "\n",
    "pd.testing.assert_frame_equal(\n",
    "    timegpt_anomalies_df_1,\n",
    "    timegpt_anomalies_df_2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NixtlaClient.detect_anomalies, title_level=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
