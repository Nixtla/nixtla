{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nixtla_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datetime\n",
    "from enum import Enum\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections.abc import Sequence\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    overload,\n",
    ")\n",
    "\n",
    "import annotated_types\n",
    "import httpcore\n",
    "import httpx\n",
    "import numpy as np\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "import zstandard as zstd\n",
    "from pydantic import BaseModel\n",
    "from tenacity import (\n",
    "    RetryCallState,\n",
    "    retry,\n",
    "    retry_if_exception,\n",
    "    stop_after_attempt,\n",
    "    stop_after_delay,\n",
    "    wait_fixed,\n",
    ")\n",
    "from utilsforecast.compat import DFType, DataFrame, pl_DataFrame\n",
    "from utilsforecast.feature_engineering import _add_time_features, time_features\n",
    "from utilsforecast.preprocessing import fill_gaps, id_time_grid\n",
    "from utilsforecast.validation import ensure_time_dtype, validate_format\n",
    "from utilsforecast.processing import ensure_sorted\n",
    "if TYPE_CHECKING:\n",
    "    try:\n",
    "        from fugue import AnyDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        import plotly\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        import triad\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from polars import DataFrame as PolarsDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from dask.dataframe import DataFrame as DaskDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        from ray.data import Dataset as RayDataset\n",
    "    except ModuleNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "AnyDFType = TypeVar(\n",
    "    \"AnyDFType\",\n",
    "    \"DaskDataFrame\",\n",
    "    pd.DataFrame,\n",
    "    \"PolarsDataFrame\",\n",
    "    \"RayDataset\",\n",
    "    \"SparkDataFrame\",\n",
    ")\n",
    "DistributedDFType = TypeVar(\n",
    "    \"DistributedDFType\",\n",
    "    \"DaskDataFrame\",\n",
    "    \"RayDataset\",\n",
    "    \"SparkDataFrame\",\n",
    ")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('httpx').setLevel(logging.ERROR)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_PositiveInt = Annotated[int, annotated_types.Gt(0)]\n",
    "_NonNegativeInt = Annotated[int, annotated_types.Ge(0)]\n",
    "_Loss = Literal[\"default\", \"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]\n",
    "_Model = Literal[\"azureai\", \"timegpt-1\", \"timegpt-1-long-horizon\"]\n",
    "_FinetuneDepth = Literal[1, 2, 3, 4, 5]\n",
    "_Freq = Union[str, int, pd.offsets.BaseOffset]\n",
    "_FreqType = TypeVar(\"_FreqType\", str, int, pd.offsets.BaseOffset)\n",
    "_ThresholdMethod = Literal[\"univariate\", \"multivariate\"]\n",
    "\n",
    "class FinetunedModel(BaseModel, extra='allow'):  # type: ignore\n",
    "    id: str\n",
    "    created_at: datetime.datetime\n",
    "    created_by: str\n",
    "    base_model_id: str\n",
    "    steps: int\n",
    "    depth: int\n",
    "    loss: _Loss\n",
    "    model: _Model\n",
    "    freq: str\n",
    "\n",
    "_date_features_by_freq = {\n",
    "    # Daily frequencies\n",
    "    'B': ['year', 'month', 'day', 'weekday'],\n",
    "    'C': ['year', 'month', 'day', 'weekday'],\n",
    "    'D': ['year', 'month', 'day', 'weekday'],\n",
    "    # Weekly\n",
    "    'W': ['year', 'week', 'weekday'],\n",
    "    # Monthly\n",
    "    'M': ['year', 'month'],\n",
    "    'SM': ['year', 'month', 'day'],\n",
    "    'BM': ['year', 'month'],\n",
    "    'CBM': ['year', 'month'],\n",
    "    'MS': ['year', 'month'],\n",
    "    'SMS': ['year', 'month', 'day'],\n",
    "    'BMS': ['year', 'month'],\n",
    "    'CBMS': ['year', 'month'],\n",
    "    # Quarterly\n",
    "    'Q': ['year', 'quarter'],\n",
    "    'BQ': ['year', 'quarter'],\n",
    "    'QS': ['year', 'quarter'],\n",
    "    'BQS': ['year', 'quarter'],\n",
    "    # Yearly\n",
    "    'A': ['year'],\n",
    "    'Y': ['year'],\n",
    "    'BA': ['year'],\n",
    "    'BY': ['year'],\n",
    "    'AS': ['year'],\n",
    "    'YS': ['year'],\n",
    "    'BAS': ['year'],\n",
    "    'BYS': ['year'],\n",
    "    # Hourly\n",
    "    'BH': ['year', 'month', 'day', 'hour', 'weekday'],\n",
    "    'H': ['year', 'month', 'day', 'hour'],\n",
    "    # Minutely\n",
    "    'T': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    'min': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    # Secondly\n",
    "    'S': ['year', 'month', 'day', 'hour', 'minute', 'second'],\n",
    "    # Milliseconds\n",
    "    'L': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    'ms': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    # Microseconds\n",
    "    'U': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    'us': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    # Nanoseconds\n",
    "    'N': []\n",
    "}\n",
    "\n",
    "def _retry_strategy(max_retries: int, retry_interval: int, max_wait_time: int):\n",
    "    def should_retry(exc: Exception) -> bool:\n",
    "        retriable_exceptions = (\n",
    "            ConnectionResetError,\n",
    "            httpcore.ConnectError,\n",
    "            httpcore.RemoteProtocolError,\n",
    "            httpx.ConnectTimeout,\n",
    "            httpx.ReadError,\n",
    "            httpx.RemoteProtocolError,\n",
    "            httpx.ReadTimeout,\n",
    "            httpx.PoolTimeout,\n",
    "            httpx.WriteError,\n",
    "            httpx.WriteTimeout,\n",
    "        )\n",
    "        retriable_codes = [408, 409, 429, 502, 503, 504]\n",
    "        return (\n",
    "            isinstance(exc, retriable_exceptions)\n",
    "            or (isinstance(exc, ApiError) and exc.status_code in retriable_codes)\n",
    "        )\n",
    "\n",
    "    def after_retry(retry_state: RetryCallState) -> None:\n",
    "        error = retry_state.outcome.exception()\n",
    "        logger.error(\n",
    "            f\"Attempt {retry_state.attempt_number} failed with error: {error}\"\n",
    "        )\n",
    "\n",
    "    return retry(\n",
    "        retry=retry_if_exception(should_retry),\n",
    "        wait=wait_fixed(retry_interval),\n",
    "        after=after_retry,\n",
    "        stop=stop_after_attempt(max_retries) | stop_after_delay(max_wait_time),\n",
    "        reraise=True,\n",
    "    )\n",
    "\n",
    "def _maybe_infer_freq(\n",
    "    df: DataFrame,\n",
    "    freq: Optional[_FreqType],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    ") -> _FreqType:\n",
    "    if freq is not None:\n",
    "        return freq\n",
    "    if isinstance(df, pl_DataFrame):\n",
    "        raise ValueError(\n",
    "            \"Cannot infer frequency for a polars DataFrame, please set the \"\n",
    "            \"`freq` argument to a valid polars offset.\\nYou can find them at \"\n",
    "            \"https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.Expr.dt.offset_by.html\"\n",
    "        )\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    sizes = df[id_col].value_counts(sort=True)\n",
    "    times = df.loc[df[id_col] == sizes.index[0], time_col].sort_values()\n",
    "    if times.dt.tz is not None:\n",
    "        times = times.dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    inferred_freq = pd.infer_freq(times.values)\n",
    "    if inferred_freq is None:\n",
    "        raise RuntimeError(\n",
    "            'Could not infer the frequency of the time column. This could be due '\n",
    "            'to inconsistent intervals. Please check your data for missing, '\n",
    "            'duplicated or irregular timestamps'\n",
    "        )\n",
    "    logger.info(f'Inferred freq: {inferred_freq}')\n",
    "    return inferred_freq\n",
    "\n",
    "def _standardize_freq(freq: _Freq, processed: ufp.ProcessedDF) -> str:\n",
    "    if isinstance(freq, str):\n",
    "        # polars uses 'mo' for months, all other strings are compatible with pandas\n",
    "        freq = freq.replace('mo', 'MS')\n",
    "    elif isinstance(freq, pd.offsets.BaseOffset):\n",
    "        freq = freq.freqstr\n",
    "    elif isinstance(freq, int):\n",
    "        freq = 'MS'\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"`freq` must be a string, int or pandas offset, got {type(freq).__name__}\"\n",
    "        )\n",
    "    return freq\n",
    "\n",
    "def _array_tails(\n",
    "    x: np.ndarray,\n",
    "    indptr: np.ndarray,\n",
    "    out_sizes: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    if (out_sizes > np.diff(indptr)).any():\n",
    "        raise ValueError('out_sizes must be at most the original sizes.')\n",
    "    idxs = np.hstack(\n",
    "        [\n",
    "            np.arange(end - size, end)\n",
    "            for end, size in zip(indptr[1:], out_sizes)\n",
    "        ]\n",
    "    )\n",
    "    return x[idxs]\n",
    "\n",
    "def _tail(proc: ufp.ProcessedDF, n: int) -> ufp.ProcessedDF:\n",
    "    new_sizes = np.minimum(np.diff(proc.indptr), n)\n",
    "    new_indptr = np.append(0, new_sizes.cumsum())\n",
    "    new_data = _array_tails(proc.data, proc.indptr, new_sizes)\n",
    "    return ufp.ProcessedDF(\n",
    "        uids=proc.uids,\n",
    "        last_times=proc.last_times,\n",
    "        data=new_data,\n",
    "        indptr=new_indptr,\n",
    "        sort_idxs=None,\n",
    "    )\n",
    "\n",
    "def _partition_series(\n",
    "    payload: dict[str, Any], n_part: int, h: int\n",
    ") -> list[dict[str, Any]]:\n",
    "    parts = []\n",
    "    series = payload.pop(\"series\")\n",
    "    n_series = len(series[\"sizes\"])\n",
    "    n_part = min(n_part, n_series)\n",
    "    series_per_part = math.ceil(n_series / n_part)\n",
    "    prev_size = 0\n",
    "    for i in range(0, n_series, series_per_part):\n",
    "        sizes = series[\"sizes\"][i : i + series_per_part]\n",
    "        curr_size = sum(sizes)\n",
    "        part_idxs = slice(prev_size, prev_size + curr_size)\n",
    "        prev_size += curr_size\n",
    "        part_series = {\n",
    "            \"y\": series[\"y\"][part_idxs],\n",
    "            \"sizes\": sizes,\n",
    "        }\n",
    "        if series[\"X\"] is None:\n",
    "            part_series[\"X\"] = None\n",
    "            if h > 0:\n",
    "                part_series[\"X_future\"] = None\n",
    "        else:\n",
    "            part_series[\"X\"] = [x[part_idxs] for x in series[\"X\"]]\n",
    "            if h > 0:\n",
    "                part_series[\"X_future\"] = [\n",
    "                    x[i * h : (i + series_per_part) * h] for x in series[\"X_future\"]\n",
    "                ]\n",
    "        parts.append({\"series\": part_series, **payload})\n",
    "    return parts\n",
    "\n",
    "def _maybe_add_date_features(\n",
    "    df: DFType,\n",
    "    X_df: Optional[DFType],\n",
    "    features: Union[bool, Sequence[Union[str, Callable]]],\n",
    "    one_hot: Union[bool, list[str]],\n",
    "    freq: _Freq,\n",
    "    h: int,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    ") -> tuple[DFType, Optional[DFType]]:\n",
    "    if not features or not isinstance(freq, str):\n",
    "        return df, X_df\n",
    "    if isinstance(features, list):\n",
    "        date_features: Sequence[Union[str, Callable]] = features\n",
    "    else:\n",
    "        date_features = _date_features_by_freq.get(freq, [])\n",
    "        if not date_features:\n",
    "            warnings.warn(\n",
    "                f'Non default date features for {freq} '\n",
    "                'please provide a list of date features'\n",
    "            )\n",
    "    # add features\n",
    "    if X_df is None:\n",
    "        df, X_df = time_features(\n",
    "            df=df,\n",
    "            freq=freq,\n",
    "            features=date_features,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "    else:\n",
    "        df = _add_time_features(df, features=date_features, time_col=time_col)\n",
    "        X_df = _add_time_features(X_df, features=date_features,time_col=time_col)\n",
    "    # one hot\n",
    "    if isinstance(one_hot, list):\n",
    "        features_one_hot = one_hot\n",
    "    elif one_hot:\n",
    "        features_one_hot = [f for f in date_features if not callable(f)]\n",
    "    else:\n",
    "        features_one_hot = []\n",
    "    if features_one_hot:\n",
    "        X_df = ufp.assign_columns(X_df, target_col, 0)\n",
    "        full_df = ufp.vertical_concat([df, X_df])\n",
    "        if isinstance(full_df, pd.DataFrame):\n",
    "            full_df = pd.get_dummies(\n",
    "                full_df, columns=features_one_hot, dtype='float32'\n",
    "            )\n",
    "        else:\n",
    "            full_df = full_df.to_dummies(columns=features_one_hot)\n",
    "        df = ufp.take_rows(full_df, slice(0, df.shape[0]))\n",
    "        X_df = ufp.take_rows(full_df, slice(df.shape[0], full_df.shape[0]))\n",
    "        X_df = ufp.drop_columns(X_df, target_col)\n",
    "        X_df = ufp.drop_index_if_pandas(X_df)\n",
    "    if h == 0:\n",
    "        # time_features returns an empty df, we use it as None here\n",
    "        X_df = None\n",
    "    return df, X_df\n",
    "\n",
    "def _validate_exog(\n",
    "    df: DFType,\n",
    "    X_df: Optional[DFType],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    hist_exog: Optional[list[str]],\n",
    ") -> tuple[DFType, Optional[DFType]]:\n",
    "    base_cols = {id_col, time_col, target_col}\n",
    "    exogs = [c for c in df.columns if c not in base_cols]\n",
    "    if hist_exog is None:\n",
    "        hist_exog = []\n",
    "    if X_df is None:\n",
    "        # all exogs must be historic\n",
    "        ignored_exogs = [c for c in exogs if c not in hist_exog]\n",
    "        if ignored_exogs:\n",
    "            warnings.warn(\n",
    "                f\"`df` contains the following exogenous features: {ignored_exogs}, \"\n",
    "                \"but `X_df` was not provided and they were not declared in `hist_exog_list`. \"\n",
    "                \"They will be ignored.\"\n",
    "            )\n",
    "        exogs = [c for c in exogs if c in hist_exog]\n",
    "        df = df[[id_col, time_col, target_col, *exogs]]\n",
    "        return df, None\n",
    "\n",
    "    # exogs in df that weren't declared as historic nor future\n",
    "    futr_exog = [c for c in X_df.columns if c not in base_cols]\n",
    "    declared_exogs = {*hist_exog, *futr_exog}\n",
    "    ignored_exogs = [c for c in exogs if c not in declared_exogs]\n",
    "    if ignored_exogs:\n",
    "        warnings.warn(\n",
    "            f\"`df` contains the following exogenous features: {ignored_exogs}, \"\n",
    "            \"but they were not found in `X_df` nor declared in `hist_exog_list`. \"\n",
    "            \"They will be ignored.\"\n",
    "        )\n",
    "\n",
    "    # future exogenous are provided in X_df that are not in df\n",
    "    missing_futr = set(futr_exog) - set(exogs)\n",
    "    if missing_futr:\n",
    "        raise ValueError(\n",
    "            \"The following exogenous features are present in `X_df` \"\n",
    "            f\"but not in `df`: {missing_futr}.\"\n",
    "        )\n",
    "\n",
    "    # features are provided through X_df but declared as historic\n",
    "    futr_and_hist = set(futr_exog) & set(hist_exog)\n",
    "    if futr_and_hist:\n",
    "        warnings.warn(\n",
    "            \"The following features were declared as historic but found in `X_df`: \"\n",
    "            f\"{futr_and_hist}, they will be considered as historic.\"\n",
    "        )\n",
    "        futr_exog = [f for f in futr_exog if f not in hist_exog]\n",
    "\n",
    "    # Make sure df and X_df are in right order\n",
    "    df = df[[id_col, time_col, target_col, *futr_exog, *hist_exog]]\n",
    "    X_df = X_df[[id_col, time_col, *futr_exog]]\n",
    "\n",
    "    return df, X_df\n",
    "\n",
    "def _validate_input_size(\n",
    "    processed: ufp.ProcessedDF,\n",
    "    model_input_size: int,\n",
    "    model_horizon: int,\n",
    ") -> None:\n",
    "    min_size = np.diff(processed.indptr).min().item()\n",
    "    if min_size < model_input_size + model_horizon:\n",
    "        raise ValueError(\n",
    "            'Some series are too short. '\n",
    "            'Please make sure that each series contains '\n",
    "            f'at least {model_input_size + model_horizon} observations.'\n",
    "        )\n",
    "\n",
    "def _prepare_level_and_quantiles(\n",
    "    level: Optional[list[Union[int, float]]],\n",
    "    quantiles: Optional[list[float]],\n",
    ") -> tuple[Optional[list[Union[int, float]]], Optional[list[float]]]:\n",
    "    if level is not None and quantiles is not None:\n",
    "        raise ValueError(\n",
    "            \"You should provide `level` or `quantiles`, but not both.\"\n",
    "        )\n",
    "    if quantiles is None:\n",
    "        return level, quantiles\n",
    "    # we recover level from quantiles\n",
    "    if not all(0 < q < 1 for q in quantiles):\n",
    "        raise ValueError(\"`quantiles` should be floats between 0 and 1.\")\n",
    "    level = [abs(int(100 - 200 * q)) for q in quantiles]\n",
    "    return level, quantiles\n",
    "\n",
    "def _maybe_convert_level_to_quantiles(\n",
    "    df: DFType,\n",
    "    quantiles: Optional[list[float]],\n",
    ") -> DFType:\n",
    "    if quantiles is None:\n",
    "        return df\n",
    "    out_cols = [c for c in df.columns if '-lo-' not in c and '-hi-' not in c]\n",
    "    df = ufp.copy_if_pandas(df, deep=False)\n",
    "    for q in sorted(quantiles):\n",
    "        if q == 0.5:\n",
    "            col = 'TimeGPT'\n",
    "        else:\n",
    "            lv = int(100 - 200 * q)\n",
    "            hi_or_lo = 'lo' if lv > 0 else 'hi'\n",
    "            lv = abs(lv)\n",
    "            col = f\"TimeGPT-{hi_or_lo}-{lv}\"\n",
    "        q_col = f\"TimeGPT-q-{int(q * 100)}\"\n",
    "        df = ufp.assign_columns(df, q_col, df[col])\n",
    "        out_cols.append(q_col)\n",
    "    return df[out_cols]\n",
    "\n",
    "def _preprocess(\n",
    "    df: DFType,\n",
    "    X_df: Optional[DFType],\n",
    "    h: int,\n",
    "    freq: str,\n",
    "    date_features: Union[bool, Sequence[Union[str, Callable]]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    ") -> tuple[ufp.ProcessedDF, Optional[DFType], list[str], Optional[list[str]]]:\n",
    "    df, X_df = _maybe_add_date_features(\n",
    "        df=df,\n",
    "        X_df=X_df,\n",
    "        features=date_features,\n",
    "        one_hot=date_features_to_one_hot,\n",
    "        freq=freq,\n",
    "        h=h,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "    )\n",
    "    processed = ufp.process_df(\n",
    "        df=df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "    )\n",
    "    if X_df is not None and X_df.shape[1] > 2:\n",
    "        X_df = ensure_time_dtype(X_df, time_col=time_col)\n",
    "        processed_X = ufp.process_df(\n",
    "            df=X_df, id_col=id_col, time_col=time_col, target_col=None,\n",
    "        )\n",
    "        X_future = processed_X.data.T\n",
    "        futr_cols = [c for c in X_df.columns if c not in (id_col, time_col)]\n",
    "    else:\n",
    "        X_future = None\n",
    "        futr_cols = None\n",
    "    x_cols = [c for c in df.columns if c not in (id_col, time_col, target_col)]\n",
    "    return processed, X_future, x_cols, futr_cols\n",
    "\n",
    "def _forecast_payload_to_in_sample(payload):\n",
    "    in_sample_payload = {\n",
    "        k: v\n",
    "        for k, v in payload.items()\n",
    "        if k not in ('h', 'finetune_steps', 'finetune_loss')\n",
    "    }\n",
    "    del in_sample_payload['series']['X_future']\n",
    "    return in_sample_payload\n",
    "\n",
    "def _maybe_add_intervals(\n",
    "    df: DFType,\n",
    "    intervals: Optional[dict[str, list[float]]],\n",
    ") -> DFType:\n",
    "    if intervals is None:\n",
    "        return df\n",
    "    first_key = next(iter(intervals), None)\n",
    "    if first_key is None or intervals[first_key] is None:\n",
    "        return df\n",
    "    intervals_df = type(df)(\n",
    "        {f'TimeGPT-{k}': intervals[k] for k in sorted(intervals.keys())}\n",
    "    )\n",
    "    return ufp.horizontal_concat([df, intervals_df])\n",
    "\n",
    "def _maybe_drop_id(df: DFType, id_col: str, drop: bool) -> DFType:\n",
    "    if drop:\n",
    "        df = ufp.drop_columns(df, id_col)\n",
    "    return df\n",
    "\n",
    "def _parse_in_sample_output(\n",
    "    in_sample_output: dict[str, Union[list[float], dict[str, list[float]]]],\n",
    "    df: DataFrame,\n",
    "    processed: ufp.ProcessedDF,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    ") -> DataFrame:\n",
    "    times = df[time_col].to_numpy()\n",
    "    targets = df[target_col].to_numpy()\n",
    "    if processed.sort_idxs is not None:\n",
    "        times = times[processed.sort_idxs]\n",
    "        targets = targets[processed.sort_idxs]\n",
    "    times = _array_tails(\n",
    "        times, processed.indptr, in_sample_output['sizes']\n",
    "    )\n",
    "    targets = _array_tails(\n",
    "        targets, processed.indptr, in_sample_output['sizes']\n",
    "    )\n",
    "    uids = ufp.repeat(processed.uids, in_sample_output['sizes'])\n",
    "    out = type(df)(\n",
    "        {\n",
    "            id_col: uids,\n",
    "            time_col: times,\n",
    "            target_col: targets,\n",
    "            'TimeGPT': in_sample_output['mean'],\n",
    "        }\n",
    "    )\n",
    "    return _maybe_add_intervals(out, in_sample_output['intervals'])  # type: ignore\n",
    "\n",
    "def _restrict_input_samples(level, input_size, model_horizon, h) -> int:\n",
    "    if level is not None:\n",
    "        # add sufficient info to compute\n",
    "        # conformal interval\n",
    "        # @AzulGarza\n",
    "        #  this is an old opinionated decision\n",
    "        #  about reducing the data sent to the api\n",
    "        #  to reduce latency when\n",
    "        #  a user passes level. since currently the model\n",
    "        #  uses conformal prediction, we can change a minimum\n",
    "        #  amount of data if the series are too large\n",
    "        new_input_size = 3 * input_size + max(model_horizon, h)\n",
    "    else:\n",
    "        # we only want to forecast\n",
    "        new_input_size = input_size\n",
    "    return new_input_size\n",
    "\n",
    "def _extract_target_array(df: DataFrame, target_col: str) -> np.ndarray:\n",
    "    # in pandas<2.2 to_numpy can lead to an object array if\n",
    "    # the type is a pandas nullable type, e.g. pd.Float64Dtype\n",
    "    # we thus use the dtype's type as the target dtype\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        target_dtype = df.dtypes[target_col].type\n",
    "        targets = df[target_col].to_numpy(dtype=target_dtype)\n",
    "    else:\n",
    "        targets = df[target_col].to_numpy()\n",
    "    return targets\n",
    "\n",
    "def _process_exog_features(\n",
    "    processed_data: np.ndarray,\n",
    "    x_cols: list[str],\n",
    "    hist_exog_list: Optional[list[str]] = None\n",
    ") -> tuple[Optional[np.ndarray], Optional[list[int]]]:\n",
    "    X = None\n",
    "    hist_exog = None\n",
    "    if processed_data.shape[1] > 1:\n",
    "        X = processed_data[:, 1:].T\n",
    "        if hist_exog_list is None:\n",
    "            futr_exog = x_cols\n",
    "        else:\n",
    "            missing_hist: set[str] = set(hist_exog_list) - set(x_cols)\n",
    "            if missing_hist:\n",
    "                raise ValueError(\n",
    "                    \"The following exogenous features were declared as historic \"\n",
    "                    f\"but were not found in `df`: {missing_hist}.\"\n",
    "                )\n",
    "            futr_exog = [c for c in x_cols if c not in hist_exog_list]\n",
    "            # match the forecast method order [future, historic]\n",
    "            fcst_features_order = futr_exog + hist_exog_list\n",
    "            x_idxs = [x_cols.index(c) for c in fcst_features_order]\n",
    "            X = X[x_idxs]\n",
    "            hist_exog = [fcst_features_order.index(c) for c in hist_exog_list]\n",
    "        if futr_exog and logger:\n",
    "            logger.info(f'Using future exogenous features: {futr_exog}')\n",
    "        if hist_exog_list and logger:\n",
    "            logger.info(f'Using historical exogenous features: {hist_exog_list}')\n",
    "\n",
    "    return X, hist_exog\n",
    "\n",
    "def _model_in_list(model:str, model_list: tuple[Any]) -> bool:\n",
    "    for m in model_list:\n",
    "        if isinstance(m, str):\n",
    "            if m == model:\n",
    "                return True\n",
    "        elif isinstance(m, re.Pattern):\n",
    "            if m.fullmatch(model):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class AuditDataSeverity(Enum):\n",
    "    \"\"\"Enum class to indicate audit data severity levels\"\"\"\n",
    "    FAIL = \"Fail\"  # Indicates a critical issue that requires immediate attention\n",
    "    CASE_SPECIFIC = \"Case Specific\"  # Indicates an issue that may be acceptable in specific contexts\n",
    "    PASS = \"Pass\"  # Indicates that the data is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def _audit_duplicate_rows(\n",
    "    df: AnyDFType,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    ") -> tuple[AuditDataSeverity, AnyDFType]:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        duplicates = df.duplicated(subset=[id_col, time_col], keep=False)\n",
    "        if duplicates.any():\n",
    "            return AuditDataSeverity.FAIL, df[duplicates]\n",
    "        return AuditDataSeverity.PASS, pd.DataFrame()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type {type(df)} is not supported yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit Missing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def _audit_missing_dates(\n",
    "    df: AnyDFType,\n",
    "    freq: _Freq,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    start: Union[str, int, datetime.date, datetime.datetime] = \"per_serie\",\n",
    "    end: Union[str, int, datetime.date, datetime.datetime] = \"global\",\n",
    ") -> tuple[AuditDataSeverity, AnyDFType]:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Fill gaps in data\n",
    "        # Convert time_col to datetime if it's string/object type\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "        df_complete = fill_gaps(df, freq=freq, id_col=id_col, time_col=time_col, start=start, end=end)\n",
    "\n",
    "        # Find missing dates by comparing df_complete with df\n",
    "        df_missing = pd.merge(df_complete, df, on=[id_col, time_col], how='outer', indicator=True)\n",
    "        df_missing = df_missing.query(\"_merge == 'left_only'\")[[id_col, time_col]]\n",
    "        if len(df_missing) > 0:\n",
    "            return AuditDataSeverity.FAIL, df_missing\n",
    "        return AuditDataSeverity.PASS, pd.DataFrame()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type {type(df)} is not supported yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit Categorical Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def _audit_categorical_variables(\n",
    "    df: AnyDFType,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    ") -> tuple[AuditDataSeverity, AnyDFType]:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Check categorical variables in df except id_col and time_col\n",
    "        categorical_cols = (\n",
    "            df\n",
    "            .select_dtypes(include=[\"category\", \"object\"])\n",
    "            .columns\n",
    "            .drop([id_col, time_col], errors=\"ignore\")\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        if categorical_cols:\n",
    "            return AuditDataSeverity.FAIL, df[categorical_cols]\n",
    "        return AuditDataSeverity.PASS, pd.DataFrame()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type {type(df)} is not supported yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit Leading Zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "def _audit_leading_zeros(\n",
    "    df: pd.DataFrame,\n",
    "    id_col: str = 'unique_id',\n",
    "    time_col: str = 'ds',\n",
    "    target_col: str = 'y', \n",
    ") -> tuple[AuditDataSeverity, pd.DataFrame]:\n",
    "    df = ensure_sorted(df, id_col=id_col, time_col=time_col)\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        group_info = df.groupby(id_col).agg(\n",
    "            first_index=(target_col, lambda s: s.index[0]),\n",
    "            first_nonzero_index=(target_col, lambda s: s.ne(0).idxmax() if s.ne(0).any() else s.index[0])\n",
    "        )\n",
    "        leading_zeros_df = group_info[group_info['first_index'] != group_info['first_nonzero_index']].reset_index()\n",
    "        if len(leading_zeros_df) > 0:\n",
    "            return AuditDataSeverity.CASE_SPECIFIC, leading_zeros_df\n",
    "        return AuditDataSeverity.PASS, pd.DataFrame()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type {type(df)} is not supported yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit Negative Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _audit_negative_values(\n",
    "    df: AnyDFType,\n",
    "    target_col: str = 'y',\n",
    ") -> tuple[AuditDataSeverity, AnyDFType]:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        negative_values = df.loc[df[target_col] < 0]\n",
    "        if len(negative_values) > 0:\n",
    "            return AuditDataSeverity.CASE_SPECIFIC, negative_values\n",
    "        return AuditDataSeverity.PASS, pd.DataFrame()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type {type(df)} is not supported yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ApiError(Exception):\n",
    "    status_code: Optional[int]\n",
    "    body: Any\n",
    "\n",
    "    def __init__(self, *, status_code: Optional[int] = None, body: Optional[Any] = None):\n",
    "        self.status_code = status_code\n",
    "        self.body = body\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"status_code: {self.status_code}, body: {self.body}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NixtlaClient:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        timeout: Optional[int] = 60,\n",
    "        max_retries: int = 6,\n",
    "        retry_interval: int = 10,\n",
    "        max_wait_time: int = 6 * 60,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Client to interact with the Nixtla API.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_key : str, optional (default=None)\n",
    "            The authorization api_key interacts with the Nixtla API.\n",
    "            If not provided, will use the NIXTLA_API_KEY environment variable.\n",
    "        base_url : str, optional (default=None)\n",
    "            Custom base_url.\n",
    "            If not provided, will use the NIXTLA_BASE_URL environment variable.\n",
    "        timeout : int, optional (default=60)\n",
    "            Request timeout in seconds. Set this to `None` to disable it.\n",
    "        max_retries : int (default=6)\n",
    "            The maximum number of attempts to make when calling the API before giving up.\n",
    "            It defines how many times the client will retry the API call if it fails.\n",
    "            Default value is 6, indicating the client will attempt the API call up to 6 times in total\n",
    "        retry_interval : int (default=10)\n",
    "            The interval in seconds between consecutive retry attempts.\n",
    "            This is the waiting period before the client tries to call the API again after a failed attempt.\n",
    "            Default value is 10 seconds, meaning the client waits for 10 seconds between retries.\n",
    "        max_wait_time : int (default=360)\n",
    "            The maximum total time in seconds that the client will spend on all retry attempts before giving up.\n",
    "            This sets an upper limit on the cumulative waiting time for all retry attempts.\n",
    "            If this time is exceeded, the client will stop retrying and raise an exception.\n",
    "            Default value is 360 seconds, meaning the client will cease retrying if the total time\n",
    "            spent on retries exceeds 360 seconds.\n",
    "            The client throws a ReadTimeout error after 60 seconds of inactivity. If you want to\n",
    "            catch these errors, use max_wait_time >> 60.\n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            api_key = os.environ['NIXTLA_API_KEY']\n",
    "        if base_url is None:\n",
    "            base_url = os.getenv('NIXTLA_BASE_URL', 'https://api.nixtla.io')\n",
    "        self._client_kwargs = {\n",
    "            'base_url': base_url,\n",
    "            'headers': {\n",
    "                'Authorization': f'Bearer {api_key}',\n",
    "                'Content-Type': 'application/json',\n",
    "            },\n",
    "            'timeout': timeout,\n",
    "        }\n",
    "        self._retry_strategy = _retry_strategy(\n",
    "            max_retries=max_retries, retry_interval=retry_interval, max_wait_time=max_wait_time\n",
    "        )\n",
    "        self._model_params: dict[tuple[str, str], tuple[int, int]] = {}\n",
    "        self._is_azure = 'ai.azure' in base_url\n",
    "        self.supported_models:list[Any] = [re.compile('^timegpt-.+$'), 'azureai']\n",
    "\n",
    "    def _make_request(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        payload: dict[str, Any],\n",
    "        multithreaded_compress: bool,\n",
    "    ) -> dict[str, Any]:\n",
    "        def ensure_contiguous_if_array(x):\n",
    "            if not isinstance(x, np.ndarray):\n",
    "                return x\n",
    "            if np.issubdtype(x.dtype, np.floating):\n",
    "                x = np.nan_to_num(\n",
    "                    np.ascontiguousarray(x, dtype=np.float32),\n",
    "                    nan=np.nan,\n",
    "                    posinf=np.finfo(np.float32).max,\n",
    "                    neginf=np.finfo(np.float32).min,\n",
    "                    copy=False,\n",
    "                )\n",
    "            else:\n",
    "                x = np.ascontiguousarray(x)\n",
    "            return x\n",
    "\n",
    "        def ensure_contiguous_arrays(d: dict[str, Any]) -> None:\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, np.ndarray):\n",
    "                    d[k] = ensure_contiguous_if_array(v)\n",
    "                elif isinstance(v, list):\n",
    "                    d[k] = [ensure_contiguous_if_array(x) for x in v]\n",
    "                elif isinstance(v, dict):\n",
    "                    ensure_contiguous_arrays(v)\n",
    "\n",
    "        ensure_contiguous_arrays(payload)\n",
    "        content = orjson.dumps(payload, option=orjson.OPT_SERIALIZE_NUMPY)\n",
    "        content_size_mb = len(content) / 2**20\n",
    "        if content_size_mb > 200:\n",
    "            raise ValueError(f'The payload is too large. Set num_partitions={math.ceil(content_size_mb / 200)}')\n",
    "        headers = {}\n",
    "        if content_size_mb > 1:\n",
    "            threads = -1 if multithreaded_compress else 0\n",
    "            content = zstd.ZstdCompressor(level=1, threads=threads).compress(content)\n",
    "            headers['content-encoding'] = 'zstd'\n",
    "        resp = client.post(url=endpoint, content=content, headers=headers)\n",
    "        try:\n",
    "            resp_body = orjson.loads(resp.content)\n",
    "        except orjson.JSONDecodeError:\n",
    "            raise ApiError(\n",
    "                status_code=resp.status_code,\n",
    "                body=f'Could not parse JSON: {resp.content}',\n",
    "            )\n",
    "        if resp.status_code != 200:\n",
    "            raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        if 'data' in resp_body:\n",
    "            resp_body = resp_body['data']\n",
    "        return resp_body\n",
    "\n",
    "    def _make_request_with_retries(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        payload: dict[str, Any],\n",
    "        multithreaded_compress: bool = True,\n",
    "    ) -> dict[str, Any]:\n",
    "        return self._retry_strategy(self._make_request)(\n",
    "            client=client,\n",
    "            endpoint=endpoint,\n",
    "            payload=payload,\n",
    "            multithreaded_compress=multithreaded_compress,\n",
    "        )\n",
    "\n",
    "    def _get_request(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        params: Optional[dict[str, Any]] = None,\n",
    "    ) -> dict[str, Any]:\n",
    "        resp = client.get(endpoint, params=params)\n",
    "        resp_body = resp.json()\n",
    "        if resp.status_code != 200:\n",
    "            raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        return resp_body\n",
    "\n",
    "    def _make_partitioned_requests(\n",
    "        self,\n",
    "        client: httpx.Client,\n",
    "        endpoint: str,\n",
    "        payloads: list[dict[str, Any]],\n",
    "    ) -> dict[str, Any]:\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        num_partitions = len(payloads)\n",
    "        results: list[dict[str, Any]] = [{} for _ in range(num_partitions)]\n",
    "        max_workers = min(10, num_partitions)\n",
    "        with ThreadPoolExecutor(max_workers) as executor:\n",
    "            future2pos = {\n",
    "                executor.submit(\n",
    "                    self._make_request_with_retries,\n",
    "                    client=client,\n",
    "                    endpoint=endpoint,\n",
    "                    payload=payload,\n",
    "                    multithreaded_compress=False,\n",
    "                ): i\n",
    "                for i, payload in enumerate(payloads)\n",
    "            }\n",
    "            for future in tqdm(as_completed(future2pos), total=len(future2pos)):\n",
    "                pos = future2pos[future]\n",
    "                results[pos] = future.result()\n",
    "        resp = {\"mean\": np.hstack([res[\"mean\"] for res in results])}\n",
    "        first_res = results[0]\n",
    "        for k in ('sizes', 'anomaly'):\n",
    "            if k in first_res:\n",
    "                resp[k] = np.hstack([res[k] for res in results])\n",
    "        if 'idxs' in first_res:\n",
    "            offsets = [0] + [sum(p['series']['sizes']) for p in payloads[:-1]]\n",
    "            resp['idxs'] = np.hstack(\n",
    "                [\n",
    "                    np.array(res['idxs'], dtype=np.int64) + offset\n",
    "                    for res, offset in zip(results, offsets)\n",
    "                ]\n",
    "            )\n",
    "        if 'anomaly_score' in first_res:\n",
    "            resp['anomaly_score'] = np.hstack([res['anomaly_score'] for res in results])\n",
    "        if first_res[\"intervals\"] is None:\n",
    "            resp[\"intervals\"] = None\n",
    "        else:\n",
    "            resp[\"intervals\"] = {}\n",
    "            for k in first_res[\"intervals\"].keys():\n",
    "                resp[\"intervals\"][k] = np.hstack(\n",
    "                    [res[\"intervals\"][k] for res in results]\n",
    "                )\n",
    "        if \"weights_x\" not in first_res or first_res[\"weights_x\"] is None:\n",
    "            resp[\"weights_x\"] = None\n",
    "        else:\n",
    "            resp[\"weights_x\"] = [res[\"weights_x\"] for res in results]\n",
    "        if \"feature_contributions\" not in first_res or first_res[\"feature_contributions\"] is None:\n",
    "            resp[\"feature_contributions\"] = None\n",
    "        else:\n",
    "            resp[\"feature_contributions\"] = np.vstack([\n",
    "                np.stack(res[\"feature_contributions\"], axis=1) for res in results\n",
    "            ]).T\n",
    "        return resp\n",
    "\n",
    "    def _maybe_override_model(self, model: _Model) -> _Model:\n",
    "        if self._is_azure and model != 'azureai':\n",
    "            warnings.warn(\"Azure endpoint detected, setting `model` to 'azureai'.\")\n",
    "            model = 'azureai'\n",
    "        return model\n",
    "    \n",
    "    def _make_client(self, **kwargs: Any) -> httpx.Client:\n",
    "        return httpx.Client(**kwargs)\n",
    "\n",
    "    def _get_model_params(self, model: _Model, freq: str) -> tuple[int, int]:\n",
    "        key = (model, freq)\n",
    "        if key not in self._model_params:\n",
    "            logger.info('Querying model metadata...')\n",
    "            payload = {'model': model, 'freq': freq}\n",
    "            with self._make_client(**self._client_kwargs) as client:\n",
    "                if self._is_azure:\n",
    "                    resp_body = self._make_request_with_retries(\n",
    "                        client, 'model_params', payload\n",
    "                    )\n",
    "                else:\n",
    "                    resp_body = self._retry_strategy(self._get_request)(\n",
    "                        client, '/model_params', payload\n",
    "                    )\n",
    "            params = resp_body['detail']\n",
    "            self._model_params[key] = (params['input_size'], params['horizon'])\n",
    "        return self._model_params[key]\n",
    "\n",
    "    def _maybe_assign_weights(\n",
    "        self,\n",
    "        weights: Optional[Union[list[float], list[list[float]]]],\n",
    "        df: DataFrame,\n",
    "        x_cols: list[str],\n",
    "    ) -> None:\n",
    "        if weights is None:\n",
    "            return\n",
    "        if isinstance(weights[0], list):\n",
    "            self.weights_x = [\n",
    "                type(df)({'features': x_cols, 'weights': w}) for w in weights\n",
    "            ]\n",
    "        else:\n",
    "            self.weights_x = type(df)(\n",
    "                {'features': x_cols, 'weights': weights}\n",
    "            )\n",
    "\n",
    "    def _maybe_assign_feature_contributions(\n",
    "        self,\n",
    "        expected_contributions: bool,\n",
    "        resp: dict[str, Any],\n",
    "        x_cols: list[str],\n",
    "        out_df: DataFrame,\n",
    "        insample_feat_contributions: Optional[list[list[float]]],\n",
    "    ) -> None:\n",
    "        if not expected_contributions:\n",
    "            return\n",
    "        if 'feature_contributions' not in resp:\n",
    "            if self._is_azure:\n",
    "                warnings.warn(\n",
    "                    \"feature_contributions aren't implemented in Azure yet.\"\n",
    "                )\n",
    "                return\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    'feature_contributions expected in response but not found'\n",
    "                )\n",
    "        feature_contributions = resp['feature_contributions']\n",
    "        if feature_contributions is None:\n",
    "            return\n",
    "        shap_cols = x_cols + [\"base_value\"]\n",
    "        shap_df = type(out_df)(dict(zip(shap_cols, feature_contributions)))\n",
    "        if insample_feat_contributions is not None:\n",
    "            insample_shap_df = type(out_df)(\n",
    "                dict(zip(shap_cols, insample_feat_contributions))\n",
    "            )\n",
    "            shap_df = ufp.vertical_concat([insample_shap_df, shap_df])\n",
    "        self.feature_contributions = ufp.horizontal_concat([out_df, shap_df])\n",
    "\n",
    "    def _run_validations(\n",
    "        self,\n",
    "        df: DFType,\n",
    "        X_df: Optional[DFType],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        model: _Model,\n",
    "        validate_api_key: bool,\n",
    "        freq: Optional[_FreqType],\n",
    "    ) -> tuple[DFType, Optional[DFType], bool, _FreqType]:\n",
    "        if validate_api_key and not self.validate_api_key(log=False):\n",
    "            raise Exception('API Key not valid, please email support@nixtla.io')\n",
    "        if not _model_in_list(model, tuple(self.supported_models)):\n",
    "            raise ValueError(\n",
    "                f'unsupported model: {model}.'\n",
    "            )\n",
    "        drop_id = id_col not in df.columns\n",
    "        if drop_id:\n",
    "            df = ufp.copy_if_pandas(df, deep=False)\n",
    "            df = ufp.assign_columns(df, id_col, 0)\n",
    "            if X_df is not None:\n",
    "                X_df = ufp.copy_if_pandas(X_df, deep=False)\n",
    "                X_df = ufp.assign_columns(X_df, id_col, 0)\n",
    "        if (\n",
    "            isinstance(df, pd.DataFrame)\n",
    "            and time_col not in df\n",
    "            and pd.api.types.is_datetime64_any_dtype(df.index)\n",
    "        ):\n",
    "            df.index.name = time_col\n",
    "            df = df.reset_index()\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "        validate_format(df=df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "        if ufp.is_nan_or_none(df[target_col]).any():\n",
    "            raise ValueError(f'Target column ({target_col}) cannot contain missing values.')\n",
    "        freq = _maybe_infer_freq(df, freq=freq, id_col=id_col, time_col=time_col)\n",
    "        if isinstance(freq, (str, int)):\n",
    "            expected_ids_times = id_time_grid(\n",
    "                df,\n",
    "                freq=freq,\n",
    "                start=\"per_serie\",\n",
    "                end=\"per_serie\",\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "            )\n",
    "            freq_ok = len(df) == len(expected_ids_times)\n",
    "        elif isinstance(freq, pd.offsets.BaseOffset):\n",
    "            times_by_id = df.groupby(id_col, observed=True)[time_col].agg(['min', 'max', 'size'])\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "                expected_ends = times_by_id['min'] + freq * (times_by_id['size'] - 1)\n",
    "            freq_ok = (expected_ends == times_by_id['max']).all()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`freq` should be a string, integer or pandas offset, \"\n",
    "                f\"got {type(freq).__name__}.\"\n",
    "            )\n",
    "        if not freq_ok:\n",
    "            raise ValueError(\n",
    "                \"Series contain missing or duplicate timestamps, or the timestamps \"\n",
    "                \"do not match the provided frequency.\\n\"\n",
    "                \"Please make sure that all series have a single observation from the first \"\n",
    "                \"to the last timestamp and that the provided frequency matches the timestamps'.\\n\"\n",
    "                \"You can refer to https://docs.nixtla.io/docs/tutorials-missing_values \"\n",
    "                \"for an end to end example.\"\n",
    "            )\n",
    "        return df, X_df, drop_id, freq\n",
    "\n",
    "    def validate_api_key(self, log: bool = True) -> bool:\n",
    "        \"\"\"Check API key status.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log : bool (default=True)\n",
    "            Show the endpoint's response.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Whether API key is valid.\"\"\"\n",
    "        if self._is_azure:\n",
    "            raise NotImplementedError(\n",
    "                'validate_api_key is not implemented for Azure deployments, '\n",
    "                'you can try using the forecasting methods directly.'\n",
    "            )\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            resp = client.get(\"/validate_api_key\")\n",
    "            body = resp.json()\n",
    "        if log:\n",
    "            logger.info(body[\"detail\"])\n",
    "        return resp.status_code == 200\n",
    "\n",
    "    def usage(self) -> dict[str, dict[str, int]]:\n",
    "        \"\"\"Query consumed requests and limits\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Consumed requests and limits by minute and month.\"\"\"\n",
    "        if self._is_azure:\n",
    "            raise NotImplementedError('usage is not implemented for Azure deployments')\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            return self._get_request(client, '/usage')\n",
    "\n",
    "    def finetune(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        freq: Optional[_Freq] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        finetune_steps: _NonNegativeInt = 10,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        output_model_id: Optional[str] = None,\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        model: _Model = 'timegpt-1',\n",
    "    ) -> str:\n",
    "        \"\"\"Fine-tune TimeGPT to your series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        finetune_steps : int (default=10)\n",
    "            Number of steps used to finetune learning TimeGPT in the new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        output_model_id : str, optional(default=None)\n",
    "            ID to assign to the fine-tuned model. If `None`, an UUID is used.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use as base.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`.\n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting\n",
    "            if you want to predict more than one seasonal\n",
    "            period given the frequency of your data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ID of the fine-tuned model\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            raise ValueError(\"Can only fine-tune on pandas or polars dataframes.\")\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, X_df, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=False,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, *_ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=False,\n",
    "            date_features_to_one_hot=False,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        _validate_input_size(processed, model_input_size, model_horizon)\n",
    "        logger.info('Calling Fine-tune Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "            },\n",
    "            'model': model,\n",
    "            'freq': standard_freq,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'output_model_id': output_model_id,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "        }\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            resp = self._make_request_with_retries(client, 'v2/finetune', payload)\n",
    "        return resp['finetuned_model_id']\n",
    "\n",
    "    @overload\n",
    "    def finetuned_models(self, as_df: Literal[False]) -> list[FinetunedModel]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def finetuned_models(self, as_df: Literal[True]) -> pd.DataFrame:\n",
    "        ...\n",
    "\n",
    "    def finetuned_models(\n",
    "        self,\n",
    "        as_df: bool = False,\n",
    "    ) -> Union[list[FinetunedModel], pd.DataFrame]:\n",
    "        \"\"\"List fine-tuned models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        as_df : bool\n",
    "            Return the fine-tuned models as a pandas dataframe\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of FinetunedModel\n",
    "            List of available fine-tuned models.\"\"\"\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            resp_body = self._get_request(client, '/v2/finetuned_models')\n",
    "        models = [FinetunedModel(**m) for m in resp_body['finetuned_models']]\n",
    "        if as_df:\n",
    "            models = pd.DataFrame([m.model_dump() for m in models])\n",
    "        return models\n",
    "\n",
    "    def finetuned_model(self, finetuned_model_id: str) -> FinetunedModel:\n",
    "        \"\"\"Get fine-tuned model metadata\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        finetuned_model_id : str\n",
    "            ID of the fine-tuned model to get metadata from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        FinetunedModel\n",
    "            Fine-tuned model metadata.\"\"\"\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            resp_body = self._get_request(\n",
    "                client, f'/v2/finetuned_models/{finetuned_model_id}'\n",
    "            )\n",
    "        return FinetunedModel(**resp_body)\n",
    "\n",
    "    def delete_finetuned_model(self, finetuned_model_id: str) -> bool:\n",
    "        \"\"\"Delete a previously fine-tuned model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        finetuned_model_id : str\n",
    "            ID of the fine-tuned model to be deleted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Whether delete was successful.\"\"\"\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            resp = client.delete(\n",
    "                f\"/v2/finetuned_models/{finetuned_model_id}\",\n",
    "                headers={'accept-encoding': 'identity'},\n",
    "            )\n",
    "        return resp.status_code == 204\n",
    "\n",
    "    def _distributed_forecast(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        X_df: Optional[DistributedDFType],\n",
    "        level: Optional[list[Union[int, float]]],\n",
    "        quantiles: Optional[list[float]],\n",
    "        finetune_steps: _NonNegativeInt,\n",
    "        finetune_depth: _FinetuneDepth,\n",
    "        finetune_loss: _Loss,\n",
    "        finetuned_model_id: Optional[str],\n",
    "        clean_ex_first: bool,\n",
    "        hist_exog_list: Optional[list[str]],\n",
    "        validate_api_key: bool,\n",
    "        add_history: bool,\n",
    "        date_features: Union[bool, list[Union[str, Callable]]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        num_partitions: Optional[int],\n",
    "        feature_contributions: bool,\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "\n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='forecast',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        if X_df is not None:\n",
    "            def format_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "                return df.assign(_in_sample=True)\n",
    "\n",
    "            def format_X_df(\n",
    "                X_df: pd.DataFrame,\n",
    "                target_col: str,\n",
    "                df_cols: list[str],\n",
    "            ) -> pd.DataFrame:\n",
    "                return X_df.assign(**{'_in_sample': False, target_col: 0.0})[df_cols]\n",
    "\n",
    "            df = fa.transform(df, format_df, schema='*,_in_sample:bool')\n",
    "            X_df = fa.transform(\n",
    "                X_df,\n",
    "                format_X_df,\n",
    "                schema=fa.get_schema(df),\n",
    "                params={'target_col': target_col, 'df_cols': fa.get_column_names(df)},\n",
    "            )\n",
    "            df = fa.union(df, X_df)\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_forecast_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=None,\n",
    "                feature_contributions=feature_contributions,\n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[AnyDFType] = None,\n",
    "        level: Optional[list[Union[int, float]]] = None,\n",
    "        quantiles: Optional[list[float]] = None,\n",
    "        finetune_steps: _NonNegativeInt = 0,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        clean_ex_first: bool = True,\n",
    "        hist_exog_list: Optional[list[str]] = None,\n",
    "        validate_api_key: bool = False,\n",
    "        add_history: bool = False,\n",
    "        date_features: Union[bool, list[Union[str, Callable]]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "        feature_contributions: bool = False\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : list[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        quantiles : list[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the\n",
    "            new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        hist_exog_list : list of str, optional (default=None)\n",
    "            Column names of the historical exogenous features.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the\n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`.\n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting\n",
    "            if you want to predict more than one seasonal\n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        feature_contributions: bool (default=False)\n",
    "            Compute SHAP values\n",
    "            Gives access to computed SHAP values to explain the impact\n",
    "            of features on the final predictions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset.\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_forecast(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "                feature_contributions=feature_contributions,\n",
    "            )\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        self.__dict__.pop('feature_contributions', None)\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, X_df, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=validate_api_key,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "        df, X_df = _validate_exog(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            hist_exog=hist_exog_list,\n",
    "        )\n",
    "        level, quantiles = _prepare_level_and_quantiles(level, quantiles)\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, X_future, x_cols, futr_cols = _preprocess(\n",
    "            df=df,\n",
    "            X_df=X_df,\n",
    "            h=h,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        if finetune_steps > 0 or add_history:\n",
    "            _validate_input_size(processed, model_input_size, model_horizon)\n",
    "        if h > model_horizon:\n",
    "            logger.warning(\n",
    "                'The specified horizon \"h\" exceeds the model horizon, '\n",
    "                'this may lead to less accurate forecasts. '\n",
    "                'Please consider using a smaller horizon.'\n",
    "            )\n",
    "        restrict_input = finetune_steps == 0 and not x_cols and not add_history\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            processed = _tail(processed, new_input_size)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T\n",
    "            if futr_cols is not None:\n",
    "                logger.info(f'Using future exogenous features: {futr_cols}')\n",
    "            if hist_exog_list:\n",
    "                logger.info(f'Using historical exogenous features: {hist_exog_list}')\n",
    "        else:\n",
    "            X = None\n",
    "\n",
    "        logger.info('Calling Forecast Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "                'X': X,\n",
    "                'X_future': X_future,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "            'feature_contributions': feature_contributions and X is not None,\n",
    "        }\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            insample_feat_contributions = None\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(client, 'v2/forecast', payload)\n",
    "                if add_history:\n",
    "                    in_sample_payload = _forecast_payload_to_in_sample(payload)\n",
    "                    logger.info('Calling Historical Forecast Endpoint...')\n",
    "                    in_sample_resp = self._make_request_with_retries(\n",
    "                        client, 'v2/historic_forecast', in_sample_payload\n",
    "                    )\n",
    "                    insample_feat_contributions = in_sample_resp.get(\n",
    "                        'feature_contributions', None\n",
    "                    )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/forecast', payloads)\n",
    "                if add_history:\n",
    "                    in_sample_payloads = [\n",
    "                        _forecast_payload_to_in_sample(p) for p in payloads\n",
    "                    ]\n",
    "                    logger.info('Calling Historical Forecast Endpoint...')\n",
    "                    in_sample_resp = self._make_partitioned_requests(\n",
    "                        client, 'v2/historic_forecast', in_sample_payloads\n",
    "                    )\n",
    "                    insample_feat_contributions = in_sample_resp.get(\n",
    "                        'feature_contributions', None\n",
    "                    )\n",
    "\n",
    "        # assemble result\n",
    "        out = ufp.make_future_dataframe(\n",
    "            uids=processed.uids,\n",
    "            last_times=type(processed.uids)(processed.last_times),\n",
    "            freq=freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        out = _maybe_add_intervals(out, resp['intervals'])\n",
    "        if add_history:\n",
    "            in_sample_df = _parse_in_sample_output(\n",
    "                in_sample_output=in_sample_resp,\n",
    "                df=df,\n",
    "                processed=processed,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "            in_sample_df = ufp.drop_columns(in_sample_df, target_col)\n",
    "            out = ufp.vertical_concat([in_sample_df, out])\n",
    "        out = _maybe_convert_level_to_quantiles(out, quantiles)\n",
    "        self._maybe_assign_feature_contributions(\n",
    "            expected_contributions=feature_contributions,\n",
    "            resp=resp,\n",
    "            x_cols=x_cols,\n",
    "            out_df=out[[id_col, time_col, 'TimeGPT']],\n",
    "            insample_feat_contributions=insample_feat_contributions,\n",
    "        )\n",
    "        if add_history:\n",
    "            sort_idxs = ufp.maybe_compute_sort_indices(out, id_col=id_col, time_col=time_col)\n",
    "            if sort_idxs is not None:\n",
    "                out = ufp.take_rows(out, sort_idxs)\n",
    "                out = ufp.drop_index_if_pandas(out)\n",
    "                if hasattr(self, 'feature_contributions'):\n",
    "                    self.feature_contributions = ufp.take_rows(self.feature_contributions, sort_idxs)\n",
    "                    self.feature_contributions = ufp.drop_index_if_pandas(self.feature_contributions)\n",
    "        out = _maybe_drop_id(df=out, id_col=id_col, drop=drop_id)\n",
    "        self._maybe_assign_weights(weights=resp['weights_x'], df=df, x_cols=x_cols)\n",
    "        return out\n",
    "\n",
    "    def _distributed_detect_anomalies(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        level: Union[int, float],\n",
    "        finetuned_model_id: Optional[str],\n",
    "        clean_ex_first: bool,\n",
    "        validate_api_key: bool,\n",
    "        date_features: Union[bool, list[str]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        num_partitions: Optional[int],\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "\n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='detect_anomalies',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=None,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_detect_anomalies_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=None,\n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def detect_anomalies(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        freq: Optional[_Freq] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Union[int, float] = 99,\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        clean_ex_first: bool = True,\n",
    "        validate_api_key: bool = False,\n",
    "        date_features: Union[bool, list[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"Detect anomalies in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before sending requests.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the\n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`.\n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting\n",
    "            if you want to predict more than one seasonal\n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset.\n",
    "            DataFrame with anomalies flagged by TimeGPT.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_detect_anomalies(\n",
    "                df=df,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, _, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=validate_api_key,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, _, x_cols, _ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T\n",
    "            logger.info(f'Using the following exogenous features: {x_cols}')\n",
    "        else:\n",
    "            X = None\n",
    "\n",
    "        logger.info('Calling Anomaly Detector Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "                'X': X,\n",
    "            },\n",
    "            'model': model,\n",
    "            'freq': standard_freq,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "        }\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(\n",
    "                    client, 'v2/anomaly_detection', payload\n",
    "                )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h=0)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/anomaly_detection', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        out = _parse_in_sample_output(\n",
    "            in_sample_output=resp,\n",
    "            df=df,\n",
    "            processed=processed,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'anomaly', resp['anomaly'])\n",
    "        out = _maybe_drop_id(df=out, id_col=id_col, drop=drop_id)\n",
    "        self._maybe_assign_weights(weights=resp[\"weights_x\"], df=df, x_cols=x_cols)\n",
    "        return out\n",
    "\n",
    "    def _distributed_detect_anomalies_online(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        h: _PositiveInt,\n",
    "        detection_size: _PositiveInt,\n",
    "        threshold_method: _ThresholdMethod,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        level: Union[int, float],\n",
    "        clean_ex_first: bool,\n",
    "        step_size: Optional[_PositiveInt],\n",
    "        finetune_steps: _NonNegativeInt,\n",
    "        finetune_depth: _FinetuneDepth,\n",
    "        finetune_loss: _Loss,\n",
    "        hist_exog_list: Optional[list[str]],\n",
    "        date_features: Union[bool, list[str]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        refit: bool,\n",
    "        num_partitions: Optional[int],\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "\n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='detect_anomalies_online',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=None,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_detect_anomalies_online_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                h=h,\n",
    "                detection_size=detection_size,\n",
    "                threshold_method=threshold_method,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetune_depth=finetune_depth,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                refit=refit,\n",
    "                num_partitions=None,\n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def detect_anomalies_online(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        h: _PositiveInt,\n",
    "        detection_size: _PositiveInt,\n",
    "        threshold_method: _ThresholdMethod = 'univariate',\n",
    "        freq: Optional[_Freq] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Union[int, float] = 99,\n",
    "        clean_ex_first: bool = True,\n",
    "        step_size: Optional[_PositiveInt] = None,\n",
    "        finetune_steps: _NonNegativeInt = 0,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        hist_exog_list: Optional[list[str]] = None,\n",
    "        date_features: Union[bool, list[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        refit: bool = False,\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"\n",
    "        Online anomaly detection in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        detection_size : int\n",
    "            The length of the sequence where anomalies will be detected starting from the end of the dataset.\n",
    "        threshold_method : str, optional (default='univariate')\n",
    "            The method used to calculate the intervals for anomaly detection.\n",
    "            Use `univariate` to flag anomalies independently for each series in the dataset.\n",
    "            Use `multivariate` to have a global threshold across all series in the dataset. For this method, all series\n",
    "            must have the same length.\n",
    "        freq : str, optional\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str, optional (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str, optional (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str, optional (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float, optional (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        clean_ex_first : bool, optional (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        hist_exog_list : list of str, optional (default=None)\n",
    "            Column names of the historical exogenous features.\n",
    "        date_features : bool or list of str, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True, automatically adds most used date features for the frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str, optional (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are one-hot encoded by default.\n",
    "        model : str, optional (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`.\n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting if you want to predict more than one seasonal\n",
    "            period given the frequency of your data.\n",
    "        refit : bool, optional (default=False)\n",
    "            Fine-tune the model in each window. If False, only fine-tunes on the first window.\n",
    "            Only used if finetune_steps > 0.e\n",
    "        num_partitions : int, optional (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal to the available parallel resources in distributed environments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset\n",
    "            DataFrame with anomalies flagged by TimeGPT.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_detect_anomalies_online(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                detection_size=detection_size,\n",
    "                threshold_method=threshold_method,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                refit=refit,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        if threshold_method == \"multivariate\" and num_partitions is not None and num_partitions > 1:\n",
    "            raise ValueError(\n",
    "                \"Cannot use more than 1 partition for multivariate anomaly detection. \"\n",
    "                \"Either set threshold_method to univariate \"\n",
    "                \"or set num_partitions to None.\"\n",
    "            )\n",
    "        self.__dict__.pop('weights_x', None)\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, _, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=False,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, _, x_cols, _ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        targets = _extract_target_array(df, target_col)\n",
    "        times = df[time_col].to_numpy()\n",
    "        if processed.sort_idxs is not None:\n",
    "            targets = targets[processed.sort_idxs]\n",
    "            times = times[processed.sort_idxs]\n",
    "        X, hist_exog = _process_exog_features(processed.data, x_cols, hist_exog_list)\n",
    "        sizes = np.diff(processed.indptr)\n",
    "        if np.all(sizes <= 6 * detection_size):\n",
    "            logger.warn('Detection size is large. Using the entire series to compute the anomaly threshold...')\n",
    "        logger.info('Calling Online Anomaly Detector Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0],\n",
    "                'sizes': sizes,\n",
    "                'X': X,\n",
    "            },\n",
    "            'h': h,\n",
    "            'detection_size': detection_size,\n",
    "            'threshold_method': threshold_method,\n",
    "            'model': model,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'step_size': step_size,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'refit': refit,\n",
    "            'hist_exog': hist_exog,\n",
    "        }\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(\n",
    "                    client, 'v2/online_anomaly_detection', payload\n",
    "                )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h=0)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/online_anomaly_detection', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        idxs = np.array(resp['idxs'], dtype=np.int64)\n",
    "        sizes = np.array(resp['sizes'], dtype=np.int64)\n",
    "        out = type(df)(\n",
    "            {\n",
    "                id_col: ufp.repeat(processed.uids, sizes),\n",
    "                time_col: times[idxs],\n",
    "                target_col: targets[idxs],\n",
    "            }\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        out = ufp.assign_columns(out, 'anomaly', resp['anomaly'])\n",
    "        out = ufp.assign_columns(out, 'anomaly_score', resp['anomaly_score'])\n",
    "        if threshold_method == 'multivariate':\n",
    "            out = ufp.assign_columns(out, 'accumulated_anomaly_score', resp['accumulated_anomaly_score'])\n",
    "        return _maybe_add_intervals(out, resp['intervals'])\n",
    "\n",
    "    def _distributed_cross_validation(\n",
    "        self,\n",
    "        df: DistributedDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq],\n",
    "        id_col: str,\n",
    "        time_col: str,\n",
    "        target_col: str,\n",
    "        level: Optional[list[Union[int, float]]],\n",
    "        quantiles: Optional[list[float]],\n",
    "        validate_api_key: bool,\n",
    "        n_windows: _PositiveInt,\n",
    "        step_size: Optional[_PositiveInt],\n",
    "        finetune_steps: _NonNegativeInt,\n",
    "        finetune_depth: _FinetuneDepth,\n",
    "        finetune_loss: _Loss,\n",
    "        finetuned_model_id: Optional[str],\n",
    "        refit: bool,\n",
    "        clean_ex_first: bool,\n",
    "        hist_exog_list: Optional[list[str]],\n",
    "        date_features: Union[bool, Sequence[Union[str, Callable]]],\n",
    "        date_features_to_one_hot: Union[bool, list[str]],\n",
    "        model: _Model,\n",
    "        num_partitions: Optional[int],\n",
    "    ) -> DistributedDFType:\n",
    "        import fugue.api as fa\n",
    "\n",
    "        schema, partition_config = _distributed_setup(\n",
    "            df=df,\n",
    "            method='cross_validation',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            num_partitions=num_partitions,\n",
    "        )\n",
    "        result_df = fa.transform(\n",
    "            df,\n",
    "            using=_cross_validation_wrapper,\n",
    "            schema=schema,\n",
    "            params=dict(\n",
    "                client=self,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                validate_api_key=validate_api_key,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                refit=refit,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=None,\n",
    "            ),\n",
    "            partition=partition_config,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        h: _PositiveInt,\n",
    "        freq: Optional[_Freq] = None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        level: Optional[list[Union[int, float]]] = None,\n",
    "        quantiles: Optional[list[float]] = None,\n",
    "        validate_api_key: bool = False,\n",
    "        n_windows: _PositiveInt = 1,\n",
    "        step_size: Optional[_PositiveInt] = None,\n",
    "        finetune_steps: _NonNegativeInt = 0,\n",
    "        finetune_depth: _FinetuneDepth = 1,\n",
    "        finetune_loss: _Loss = 'default',\n",
    "        finetuned_model_id: Optional[str] = None,\n",
    "        refit: bool = True,\n",
    "        clean_ex_first: bool = True,\n",
    "        hist_exog_list: Optional[list[str]] = None,\n",
    "        date_features: Union[bool, list[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, list[str]] = False,\n",
    "        model: _Model = 'timegpt-1',\n",
    "        num_partitions: Optional[_PositiveInt] = None,\n",
    "    ) -> AnyDFType:\n",
    "        \"\"\"Perform cross validation in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str, int or pandas offset, optional (default=None).\n",
    "            Frequency of the timestamps. If `None`, it will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for prediction intervals.\n",
    "        quantiles : list[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before sending requests.\n",
    "        n_windows : int (defaul=1)\n",
    "            Number of windows to evaluate.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        finetune_depth : int (default=1)\n",
    "            The depth of the finetuning. Uses a scale from 1 to 5, where 1 means little finetuning,\n",
    "            and 5 means that the entire model is finetuned.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        finetuned_model_id : str, optional(default=None)\n",
    "            ID of previously fine-tuned model to use.\n",
    "        refit : bool (default=True)\n",
    "            Fine-tune the model in each window. If `False`, only fine-tunes on the first window.\n",
    "            Only used if `finetune_steps` > 0.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts using TimeGPT.\n",
    "        hist_exog_list : list of str, optional (default=None)\n",
    "            Column names of the historical exogenous features.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates.\n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the\n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=False)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`.\n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting\n",
    "            if you want to predict more than one seasonal\n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas, polars, dask or spark DataFrame or ray Dataset.\n",
    "            DataFrame with cross validation forecasts.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, (pd.DataFrame, pl_DataFrame)):\n",
    "            return self._distributed_cross_validation(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                validate_api_key=validate_api_key,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_depth=finetune_depth,\n",
    "                finetune_loss=finetune_loss,\n",
    "                finetuned_model_id=finetuned_model_id,\n",
    "                refit=refit,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                hist_exog_list=hist_exog_list,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        model = self._maybe_override_model(model)\n",
    "        logger.info('Validating inputs...')\n",
    "        df, _, drop_id, freq = self._run_validations(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            validate_api_key=validate_api_key,\n",
    "            model=model,\n",
    "            freq=freq,\n",
    "        )\n",
    "        level, quantiles = _prepare_level_and_quantiles(level, quantiles)\n",
    "        if step_size is None:\n",
    "            step_size = h\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed, _, x_cols, _ = _preprocess(\n",
    "            df=df,\n",
    "            X_df=None,\n",
    "            h=0,\n",
    "            freq=freq,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "        standard_freq = _standardize_freq(freq, processed)\n",
    "        model_input_size, model_horizon = self._get_model_params(model, standard_freq)\n",
    "        targets = _extract_target_array(df, target_col)\n",
    "        times = df[time_col].to_numpy()\n",
    "        if processed.sort_idxs is not None:\n",
    "            targets = targets[processed.sort_idxs]\n",
    "            times = times[processed.sort_idxs]\n",
    "        restrict_input = finetune_steps == 0 and not x_cols\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            new_input_size += h + step_size * (n_windows - 1)\n",
    "            orig_indptr = processed.indptr\n",
    "            processed = _tail(processed, new_input_size)\n",
    "            times = _array_tails(times, orig_indptr, np.diff(processed.indptr))\n",
    "            targets = _array_tails(targets, orig_indptr, np.diff(processed.indptr))\n",
    "        X, hist_exog = _process_exog_features(processed.data, x_cols, hist_exog_list)\n",
    "\n",
    "        logger.info('Calling Cross Validation Endpoint...')\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': targets,\n",
    "                'sizes': np.diff(processed.indptr),\n",
    "                'X': X,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'n_windows': n_windows,\n",
    "            'step_size': step_size,\n",
    "            'freq': standard_freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'hist_exog': hist_exog,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_depth': finetune_depth,\n",
    "            'finetune_loss': finetune_loss,\n",
    "            'finetuned_model_id': finetuned_model_id,\n",
    "            'refit': refit,\n",
    "        }\n",
    "        with self._make_client(**self._client_kwargs) as client:\n",
    "            if num_partitions is None:\n",
    "                resp = self._make_request_with_retries(\n",
    "                    client, 'v2/cross_validation', payload\n",
    "                )\n",
    "            else:\n",
    "                payloads = _partition_series(payload, num_partitions, h=0)\n",
    "                resp = self._make_partitioned_requests(client, 'v2/cross_validation', payloads)\n",
    "\n",
    "        # assemble result\n",
    "        idxs = np.array(resp['idxs'], dtype=np.int64)\n",
    "        sizes = np.array(resp['sizes'], dtype=np.int64)\n",
    "        window_starts = np.arange(0, sizes.sum(), h)\n",
    "        cutoff_idxs = np.repeat(idxs[window_starts] - 1, h)\n",
    "        out = type(df)(\n",
    "            {\n",
    "                id_col: ufp.repeat(processed.uids, sizes),\n",
    "                time_col: times[idxs],\n",
    "                'cutoff': times[cutoff_idxs],\n",
    "                target_col: targets[idxs],\n",
    "            }\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        out = _maybe_add_intervals(out, resp['intervals'])\n",
    "        out = _maybe_drop_id(df=out, id_col=id_col, drop=drop_id)\n",
    "        return _maybe_convert_level_to_quantiles(out, quantiles)\n",
    "\n",
    "    def plot(\n",
    "        self,\n",
    "        df: Optional[DataFrame] = None,\n",
    "        forecasts_df: Optional[DataFrame] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        unique_ids: Union[Optional[list[str]], np.ndarray] = None,\n",
    "        plot_random: bool = True,\n",
    "        max_ids: int = 8,\n",
    "        models: Optional[list[str]] = None,\n",
    "        level: Optional[list[Union[int, float]]] = None,\n",
    "        max_insample_length: Optional[int] = None,\n",
    "        plot_anomalies: bool = False,\n",
    "        engine: Literal['matplotlib', 'plotly', 'plotly-resampler'] = 'matplotlib',\n",
    "        resampler_kwargs: Optional[dict] = None,\n",
    "        ax: Optional[Union[\"plt.Axes\", np.ndarray, \"plotly.graph_objects.Figure\"]] = None,\n",
    "    ):\n",
    "        \"\"\"Plot forecasts and insample values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame, optional (default=None)\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        forecasts_df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and models.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each series.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        unique_ids : list[str], optional (default=None)\n",
    "            Time Series to plot.\n",
    "            If None, time series are selected randomly.\n",
    "        plot_random : bool (default=True)\n",
    "            Select time series to plot randomly.\n",
    "        max_ids : int (default=8)\n",
    "            Maximum number of ids to plot.\n",
    "        models : list[str], optional (default=None)\n",
    "            list of models to plot.\n",
    "        level : list[float], optional (default=None)\n",
    "            list of prediction intervals to plot if paseed.\n",
    "        max_insample_length : int, optional (default=None)\n",
    "            Max number of train/insample observations to be plotted.\n",
    "        plot_anomalies : bool (default=False)\n",
    "            Plot anomalies for each prediction interval.\n",
    "        engine : str (default='matplotlib')\n",
    "            Library used to plot. 'matplotlib', 'plotly' or 'plotly-resampler'.\n",
    "        resampler_kwargs : dict\n",
    "            Kwargs to be passed to plotly-resampler constructor.\n",
    "            For further custumization (\"show_dash\") call the method,\n",
    "            store the plotting object and add the extra arguments to\n",
    "            its `show_dash` method.\n",
    "        ax : matplotlib axes, array of matplotlib axes or plotly Figure, optional (default=None)\n",
    "            Object where plots will be added.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from utilsforecast.plotting import plot_series\n",
    "        except ModuleNotFoundError:\n",
    "            raise Exception(\n",
    "                'You have to install additional dependencies to use this method, '\n",
    "                'please install them using `pip install \"nixtla[plotting]\"`'\n",
    "            )\n",
    "        if df is not None and id_col not in df.columns:\n",
    "            df = ufp.copy_if_pandas(df, deep=False)\n",
    "            df = ufp.assign_columns(df, id_col, 'ts_0')\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "        if forecasts_df is not None:\n",
    "            if id_col not in forecasts_df.columns:\n",
    "                forecasts_df = ufp.copy_if_pandas(forecasts_df, deep=False)\n",
    "                forecasts_df = ufp.assign_columns(forecasts_df, id_col, 'ts_0')\n",
    "            forecasts_df = ensure_time_dtype(forecasts_df, time_col=time_col)\n",
    "            if 'anomaly' in forecasts_df.columns:\n",
    "                # special case to plot outputs\n",
    "                # from detect_anomalies\n",
    "                df = None\n",
    "                forecasts_df = ufp.drop_columns(forecasts_df, 'anomaly')\n",
    "                cols = [\n",
    "                    c.replace('TimeGPT-lo-', '')\n",
    "                    for c in forecasts_df.columns\n",
    "                    if 'TimeGPT-lo-' in c\n",
    "                ]\n",
    "                level = [float(c) if '.' in c else int(c) for c in cols]\n",
    "                plot_anomalies = True\n",
    "                models = ['TimeGPT']\n",
    "        return plot_series(\n",
    "            df=df,\n",
    "            forecasts_df=forecasts_df,\n",
    "            ids=unique_ids,\n",
    "            plot_random=plot_random,\n",
    "            max_ids=max_ids,\n",
    "            models=models,\n",
    "            level=level,\n",
    "            max_insample_length=max_insample_length,\n",
    "            plot_anomalies=plot_anomalies,\n",
    "            engine=engine,\n",
    "            resampler_kwargs=resampler_kwargs,\n",
    "            palette=\"tab20b\",\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def audit_data(\n",
    "        df: AnyDFType,\n",
    "        freq: _Freq,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = 'y',\n",
    "        start: Union[str, int, datetime.date, datetime.datetime] = \"per_serie\",\n",
    "        end: Union[str, int, datetime.date, datetime.datetime] = \"global\",\n",
    "    ) -> tuple[bool, dict[str, DataFrame], dict[str, DataFrame]]:\n",
    "        \"\"\"Audit data quality.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            The dataframe to be audited.\n",
    "        freq : str, int or pandas offset.\n",
    "            Frequency of the timestamps. Must be specified.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str\n",
    "            Column that identifies each series, by default 'unique_id'\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or\n",
    "            integers, by default 'ds'\n",
    "        target_col : str\n",
    "            Column that contains the target, by default 'y'\n",
    "        start : Union[str, int, datetime.date, datetime.datetime], optional\n",
    "            Initial timestamp for the series.\n",
    "                * 'per_serie' uses each series first timestamp\n",
    "                * 'global' uses the first timestamp seen in the data\n",
    "                * Can also be a specific timestamp or integer,\n",
    "                e.g. '2000-01-01', 2000 or datetime(2000, 1, 1)\n",
    "            , by default \"per_serie\"\n",
    "        end : Union[str, int, datetime.date, datetime.datetime], optional\n",
    "            Final timestamp for the series.\n",
    "                * 'per_serie' uses each series last timestamp\n",
    "                * 'global' uses the last timestamp seen in the data\n",
    "                * Can also be a specific timestamp or integer,\n",
    "                e.g. '2000-01-01', 2000 or datetime(2000, 1, 1)\n",
    "            , by default \"global\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[bool, dict[str, DataFrame], dict[str, DataFrame]]\n",
    "            Tuple containing:\n",
    "            - bool: True if all tests pass, False otherwise\n",
    "            - dict: Dictionary mapping test IDs to error DataFrames for failed\n",
    "                    tests or None if the test could not be performed.\n",
    "            - dict: Dictionary mapping test IDs to error DataFrames for\n",
    "                    case-specific tests.\n",
    "\n",
    "            Test IDs:\n",
    "            - D001: Test for duplicate rows\n",
    "            - D002: Test for missing dates\n",
    "            - F001: Test for presence of categorical feature columns\n",
    "            - V001: Test for negative values\n",
    "            - V002: Test for leading zeros\n",
    "\n",
    "        \"\"\"\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "\n",
    "        logger.info(\"Running data quality tests...\")\n",
    "        pass_D001, error_df_D001 = _audit_duplicate_rows(df, id_col, time_col)\n",
    "        pass_D002, error_df_D002 = AuditDataSeverity.FAIL, None\n",
    "        if pass_D001 != AuditDataSeverity.FAIL:\n",
    "            # If data has duplicate rows, missing dates can not be added by fill_gaps.\n",
    "            # Duplicate rows issue needs to be resolved first.\n",
    "            pass_D002, error_df_D002 = _audit_missing_dates(\n",
    "                df, freq, id_col, time_col, start, end\n",
    "            )\n",
    "        pass_F001, error_df_F001 = _audit_categorical_variables(df, id_col, time_col)\n",
    "        pass_V001, error_df_V001 = _audit_negative_values(df, target_col)\n",
    "        pass_V002, error_df_V002 = _audit_leading_zeros(df, id_col, time_col, target_col)\n",
    "\n",
    "        fail_dict, case_specific_dict = {}, {}\n",
    "        test_ids = ['D001', 'D002', \"F001\", \"V001\", \"V002\"]\n",
    "        pass_vals = [pass_D001, pass_D002, pass_F001, pass_V001, pass_V002]\n",
    "        error_dfs = [\n",
    "            error_df_D001,\n",
    "            error_df_D002,\n",
    "            error_df_F001,\n",
    "            error_df_V001,\n",
    "            error_df_V002,\n",
    "        ]\n",
    "        all_pass = True\n",
    "\n",
    "        for test_id, pass_val, error_df in zip(test_ids, pass_vals, error_dfs):\n",
    "            # Only include errors for failed or case specific tests\n",
    "            if pass_val == AuditDataSeverity.FAIL:\n",
    "                all_pass = False\n",
    "                if error_df is not None:\n",
    "                    logger.warning(f\"Failure {test_id} detected with critical severity.\")\n",
    "                else:\n",
    "                    logger.warning(f\"Test {test_id} could not be performed.\")\n",
    "                fail_dict[test_id] = error_df\n",
    "\n",
    "            if pass_val == AuditDataSeverity.CASE_SPECIFIC:\n",
    "                all_pass = False\n",
    "                logger.warning(f\"Failure {test_id} detected which could cause issue depending on the use case.\")\n",
    "                case_specific_dict[test_id] = error_df\n",
    "\n",
    "        if all_pass:\n",
    "            logger.info(\"All checks passed...\")\n",
    "        return all_pass, fail_dict, case_specific_dict\n",
    "\n",
    "    def clean_data(\n",
    "        self,\n",
    "        df: AnyDFType,\n",
    "        fail_dict: dict[str, DataFrame],\n",
    "        case_specific_dict: dict[str, DataFrame],\n",
    "        freq: _Freq,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = 'y',\n",
    "        clean_case_specific: bool = False,\n",
    "        agg_dict: Optional[dict[str, Union[str, Callable]]] = None,\n",
    "    ) -> tuple[AnyDFType, bool, dict[str, DataFrame], dict[str, DataFrame]]:\n",
    "        \"\"\"Clean the data. This should be run after running `audit_data`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : AnyDFType\n",
    "            The dataframe to be cleaned\n",
    "        fail_dict : dict[str, DataFrame]\n",
    "            The failure dictionary from the audit_data method\n",
    "        case_specific_dict : dict[str, DataFrame]\n",
    "            The case specific dictionary from the audit_data method\n",
    "        freq : str, int or pandas offset.\n",
    "            Frequency of the timestamps. Must be specified.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str\n",
    "            Column that identifies each series, by default 'unique_id'\n",
    "        time_col : str\n",
    "            Column that identifies each timestep, its values can be timestamps or\n",
    "            integers, by default 'ds'\n",
    "        target_col : str\n",
    "            Column that contains the target, by default 'y'\n",
    "        clean_case_specific : bool, optional\n",
    "            If True, clean case specific issues, by default False\n",
    "        agg_dict : Optional[dict[str, Union[str, Callable]]], optional\n",
    "            The aggregation methods to use when there are duplicate rows (D001),\n",
    "            by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[AnyDFType, bool, dict[str, DataFrame], dict[str, DataFrame]]\n",
    "            Tuple containing:\n",
    "            - AnyDFType: The cleaned dataframe\n",
    "            - The three outputs from audit_data that are run at the end of cleansing.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Any exceptions during the cleaning process.\n",
    "        \"\"\"\n",
    "        df = ensure_time_dtype(df, time_col=time_col)\n",
    "        logger.info(\"Running data cleansing...\")\n",
    "\n",
    "        if fail_dict:\n",
    "            if \"D001\" in fail_dict:\n",
    "                try:\n",
    "                    logger.info(\"Fixing D001: Cleaning duplicate rows...\")\n",
    "\n",
    "                    if agg_dict is None:\n",
    "                        raise ValueError(\"agg_dict must be provided to resolve D001 failure.\")\n",
    "\n",
    "                    # Get all columns except id_col and time_col\n",
    "                    other_cols = [\n",
    "                        col for col in df.columns if col not in [id_col, time_col]\n",
    "                    ]\n",
    "\n",
    "                    # Verify all columns have aggregation rules\n",
    "                    missing_cols = [col for col in other_cols if col not in agg_dict]\n",
    "                    if missing_cols:\n",
    "                        raise ValueError(\n",
    "                            f\"D001: Missing aggregation rules for columns: {missing_cols}. \"\n",
    "                            \"Please provide aggregation rules for all columns in agg_dict.\"\n",
    "                        )\n",
    "                    df = df.groupby([id_col, time_col], as_index=False).agg(agg_dict)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error cleaning duplicate rows D001: {e}\")\n",
    "            if \"D002\" in fail_dict:\n",
    "                try:\n",
    "                    missing = fail_dict.get(\"D002\")\n",
    "                    if missing is None:\n",
    "                        logger.warning(\n",
    "                            'D002: Missing dates could not be checked by audit_data. '\n",
    "                            'Hence not filling missing dates...'\n",
    "                        )\n",
    "                    else:\n",
    "                        logger.info('Fixing D002: Filling missing dates...')\n",
    "                        df = pd.concat([df, fail_dict.get(\"D002\")])\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error filling missing dates D002: {e}\")\n",
    "\n",
    "        if case_specific_dict and clean_case_specific:\n",
    "            if \"V001\" in case_specific_dict:\n",
    "                try:\n",
    "                    logger.info('Fixing V001: Removing negative values...')\n",
    "                    df.loc[df[target_col] < 0, target_col] = 0\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error removing negative values V001: {e}\")\n",
    "\n",
    "            if \"V002\" in case_specific_dict:\n",
    "                try:\n",
    "                    logger.info('Fixing V002: Removing leading zeros...')\n",
    "                    leading_zeros_df = case_specific_dict[\"V002\"]\n",
    "                    leading_zeros_dict = leading_zeros_df.set_index(id_col)['first_nonzero_index'].to_dict()\n",
    "                    df = df.groupby(id_col, group_keys=False).apply(\n",
    "                        lambda group: group.loc[group.index >= leading_zeros_dict.get(group.name, group.index[0])]\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error removing leading zeros V002: {e}\")\n",
    "\n",
    "        # Run data quality checks on the cleaned data\n",
    "        all_pass, error_dfs, case_specific_dfs = self.audit_data(\n",
    "            df=df, freq=freq, id_col=id_col, time_col=time_col\n",
    "        )\n",
    "\n",
    "        return df, all_pass, error_dfs, case_specific_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _forecast_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    h: _PositiveInt,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[list[Union[int, float]]],\n",
    "    quantiles: Optional[list[float]],\n",
    "    finetune_steps: _NonNegativeInt,\n",
    "    finetune_depth: _FinetuneDepth,\n",
    "    finetune_loss: _Loss,\n",
    "    finetuned_model_id: Optional[str],\n",
    "    clean_ex_first: bool,\n",
    "    hist_exog_list: Optional[list[str]],\n",
    "    validate_api_key: bool,\n",
    "    add_history: bool,\n",
    "    date_features: Union[bool, list[Union[str, Callable]]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    "    feature_contributions: bool,\n",
    ") -> pd.DataFrame:\n",
    "    if '_in_sample' in df:\n",
    "        in_sample_mask = df['_in_sample']\n",
    "        X_df = df.loc[~in_sample_mask].drop(columns=['_in_sample', target_col])\n",
    "        df = df.loc[in_sample_mask].drop(columns='_in_sample')\n",
    "    else:\n",
    "        X_df = None\n",
    "    return client.forecast(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        X_df=X_df,\n",
    "        level=level,\n",
    "        quantiles=quantiles,\n",
    "        finetune_steps=finetune_steps,\n",
    "        finetune_depth=finetune_depth,\n",
    "        finetune_loss=finetune_loss,\n",
    "        finetuned_model_id=finetuned_model_id,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        validate_api_key=validate_api_key,\n",
    "        add_history=add_history,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        num_partitions=num_partitions,\n",
    "        feature_contributions=feature_contributions,\n",
    "    )\n",
    "\n",
    "def _detect_anomalies_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Union[int, float],\n",
    "    finetuned_model_id: Optional[str],\n",
    "    clean_ex_first: bool,\n",
    "    validate_api_key: bool,\n",
    "    date_features: Union[bool, list[str]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    ") -> pd.DataFrame:\n",
    "    return client.detect_anomalies(\n",
    "        df=df,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        finetuned_model_id=finetuned_model_id,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        validate_api_key=validate_api_key,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        num_partitions=num_partitions,\n",
    "    )\n",
    "\n",
    "def _detect_anomalies_online_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    h: _PositiveInt,\n",
    "    detection_size: _PositiveInt,\n",
    "    threshold_method: _ThresholdMethod,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Union[int, float],\n",
    "    clean_ex_first: bool,\n",
    "    step_size: _PositiveInt,\n",
    "    finetune_steps: _NonNegativeInt,\n",
    "    finetune_depth: _FinetuneDepth,\n",
    "    finetune_loss: _Loss,\n",
    "    hist_exog_list: Optional[list[str]],\n",
    "    date_features: Union[bool, list[str]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    refit: bool,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    ") -> pd.DataFrame:\n",
    "    return client.detect_anomalies_online(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        detection_size=detection_size,\n",
    "        threshold_method=threshold_method,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        step_size=step_size,\n",
    "        finetune_steps=finetune_steps,\n",
    "        finetune_depth=finetune_depth,\n",
    "        finetune_loss=finetune_loss,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        refit=refit,\n",
    "        num_partitions=num_partitions,\n",
    "    )\n",
    "\n",
    "def _cross_validation_wrapper(\n",
    "    df: pd.DataFrame,\n",
    "    client: NixtlaClient,\n",
    "    h: _PositiveInt,\n",
    "    freq: Optional[_Freq],\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[list[Union[int, float]]],\n",
    "    quantiles: Optional[list[float]],\n",
    "    validate_api_key: bool,\n",
    "    n_windows: _PositiveInt,\n",
    "    step_size: Optional[_PositiveInt],\n",
    "    finetune_steps: _NonNegativeInt,\n",
    "    finetune_depth: _FinetuneDepth,\n",
    "    finetune_loss: _Loss,\n",
    "    finetuned_model_id: Optional[str],\n",
    "    refit: bool,\n",
    "    clean_ex_first: bool,\n",
    "    hist_exog_list: Optional[list[str]],\n",
    "    date_features: Union[bool, list[str]],\n",
    "    date_features_to_one_hot: Union[bool, list[str]],\n",
    "    model: _Model,\n",
    "    num_partitions: Optional[_PositiveInt],\n",
    ") -> pd.DataFrame:\n",
    "    return client.cross_validation(\n",
    "        df=df,\n",
    "        h=h,\n",
    "        freq=freq,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        quantiles=quantiles,\n",
    "        validate_api_key=validate_api_key,\n",
    "        n_windows=n_windows,\n",
    "        step_size=step_size,\n",
    "        finetune_steps=finetune_steps,\n",
    "        finetune_depth=finetune_depth,\n",
    "        finetune_loss=finetune_loss,\n",
    "        finetuned_model_id=finetuned_model_id,\n",
    "        refit=refit,\n",
    "        clean_ex_first=clean_ex_first,\n",
    "        hist_exog_list=hist_exog_list,\n",
    "        date_features=date_features,\n",
    "        date_features_to_one_hot=date_features_to_one_hot,\n",
    "        model=model,\n",
    "        num_partitions=num_partitions,\n",
    "    )\n",
    "\n",
    "def _get_schema(\n",
    "    df: 'AnyDataFrame',\n",
    "    method: str,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[Union[int, float, list[Union[int, float]]]],\n",
    "    quantiles: Optional[list[float]],\n",
    ") -> 'triad.Schema':\n",
    "    import fugue.api as fa\n",
    "\n",
    "    base_cols = [id_col, time_col]\n",
    "    if method != 'forecast':\n",
    "        base_cols.append(target_col)\n",
    "    schema = fa.get_schema(df).extract(base_cols).copy()\n",
    "    schema.append('TimeGPT:double')\n",
    "    if method == 'detect_anomalies':\n",
    "        schema.append('anomaly:bool')\n",
    "    if method == 'detect_anomalies_online':\n",
    "        schema.append('anomaly:bool')\n",
    "        schema.append('anomaly_score:double')\n",
    "    elif method == 'cross_validation':\n",
    "        schema.append(('cutoff', schema[time_col].type))\n",
    "    if level is not None and quantiles is not None:\n",
    "        raise ValueError(\"You should provide `level` or `quantiles` but not both.\")\n",
    "    if level is not None:\n",
    "        if not isinstance(level, list):\n",
    "            level = [level]\n",
    "        level = sorted(level)\n",
    "        schema.append(\",\".join(f\"TimeGPT-lo-{lv}:double\" for lv in reversed(level)))\n",
    "        schema.append(\",\".join(f\"TimeGPT-hi-{lv}:double\" for lv in level))\n",
    "    if quantiles is not None:\n",
    "        quantiles = sorted(quantiles)\n",
    "        q_cols = [f'TimeGPT-q-{int(q * 100)}:double' for q in quantiles]\n",
    "        schema.append(\",\".join(q_cols))\n",
    "    return schema\n",
    "\n",
    "def _distributed_setup(\n",
    "    df: 'AnyDataFrame',\n",
    "    method: str,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    level: Optional[Union[int, float, list[Union[int, float]]]],\n",
    "    quantiles: Optional[list[float]],\n",
    "    num_partitions: Optional[int],\n",
    ") -> tuple['triad.Schema', dict[str, Any]]:\n",
    "    from fugue.execution import infer_execution_engine\n",
    "\n",
    "    if infer_execution_engine([df]) is None:\n",
    "        raise ValueError(\n",
    "            f'Could not infer execution engine for type {type(df).__name__}. '\n",
    "            'Expected a spark or dask DataFrame or a ray Dataset.'\n",
    "        )\n",
    "    schema = _get_schema(\n",
    "        df=df,\n",
    "        method=method,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        level=level,\n",
    "        quantiles=quantiles,\n",
    "    )\n",
    "    partition_config: dict[str, Any] = dict(by=id_col, algo='coarse')\n",
    "    if num_partitions is not None:\n",
    "        partition_config['num'] = num_partitions\n",
    "    return schema, partition_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_kwargs = {\n",
    "    \"freq\": \"D\",\n",
    "    \"id_col\": 'unique_id',\n",
    "    \"time_col\": 'ds'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate rows and missing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leading zeros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to make forecasts! Let's import an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ray"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
