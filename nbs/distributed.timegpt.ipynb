{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distributed.timegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "from fugue import transform, DataFrame, FugueWorkflow, ExecutionEngine\n",
    "from fugue.collections.yielded import Yielded\n",
    "from fugue.constants import FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT\n",
    "from fugue.execution.factory import make_execution_engine\n",
    "from triad import Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _cotransform(\n",
    "    df1: Any,\n",
    "    df2: Any,\n",
    "    using: Any,\n",
    "    schema: Any = None,\n",
    "    params: Any = None,\n",
    "    partition: Any = None,\n",
    "    engine: Any = None,\n",
    "    engine_conf: Any = None,\n",
    "    force_output_fugue_dataframe: bool = False,\n",
    "    as_local: bool = False,\n",
    ") -> Any:\n",
    "    dag = FugueWorkflow(compile_conf={FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT: 0})\n",
    "\n",
    "    src = dag.create_data(df1).zip(dag.create_data(df2), partition=partition)\n",
    "    tdf = src.transform(\n",
    "        using=using,\n",
    "        schema=schema,\n",
    "        params=params,\n",
    "        pre_partition=partition,\n",
    "    )\n",
    "    tdf.yield_dataframe_as(\"result\", as_local=as_local)\n",
    "    dag.run(engine, conf=engine_conf)\n",
    "    result = dag.yields[\"result\"].result  # type:ignore\n",
    "    if force_output_fugue_dataframe or isinstance(df1, (DataFrame, Yielded)):\n",
    "        return result\n",
    "    return result.as_pandas() if result.is_local else result.native  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _DistributedTimeGPT:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            token: Optional[str] = None, \n",
    "            environment: Optional[str] = None,\n",
    "            max_retries: int = 6,\n",
    "            retry_interval: int = 10,\n",
    "            max_wait_time: int = 60,\n",
    "        ):\n",
    "        self.token = token\n",
    "        self.environment = environment\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_interval = retry_interval\n",
    "        self.max_wait_time = max_wait_time\n",
    "\n",
    "    def _distribute_method(\n",
    "            self, \n",
    "            method: Callable,\n",
    "            df: fugue.AnyDataFrame, \n",
    "            kwargs: dict, \n",
    "            schema: str, \n",
    "            num_partitions: int, \n",
    "            id_col: str,\n",
    "            X_df: Optional[fugue.AnyDataFrame] = None, \n",
    "        ):\n",
    "        if id_col not in fa.get_column_names(df):\n",
    "            raise Exception(\n",
    "                'Distributed environment is meant to forecasts '\n",
    "                'multiple time series at once. You did not provide '\n",
    "                'an identifier for each time series.'\n",
    "            )\n",
    "        engine = make_execution_engine(infer_by=[df])\n",
    "        if num_partitions is None:\n",
    "            num_partitions = engine.get_current_parallelism()\n",
    "        partition = dict(by=id_col, num=num_partitions, algo='coarse')\n",
    "        params = dict(kwargs={**kwargs, 'num_partitions': 1},) # local num_partitions\n",
    "        if X_df is not None:\n",
    "            # check same engine\n",
    "            engine_x = make_execution_engine(infer_by=[X_df])\n",
    "            if repr(engine) != repr(engine_x):\n",
    "                raise Exception(\n",
    "                    'Target variable and exogenous variables '\n",
    "                    'have different engines. Please provide the same '\n",
    "                    'distributed engine for both inputs.'\n",
    "                )\n",
    "            result_df = _cotransform(\n",
    "                df,\n",
    "                X_df,\n",
    "                method,\n",
    "                params=params,\n",
    "                schema=schema,\n",
    "                partition=partition,\n",
    "                engine=engine,\n",
    "            )\n",
    "        else:\n",
    "            result_df = fa.transform(\n",
    "                df,\n",
    "                method,\n",
    "                params=params,\n",
    "                schema=schema,\n",
    "                engine=engine,\n",
    "                partition=partition,\n",
    "                as_fugue=True,\n",
    "            )\n",
    "        return fa.get_native_as_df(result_df)\n",
    "\n",
    "    def forecast(\n",
    "            self,\n",
    "            df: fugue.AnyDataFrame,\n",
    "            h: int,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            X_df: Optional[fugue.AnyDataFrame] = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            quantiles: Optional[List[float]] = None,\n",
    "            fewshot_steps: int = 0,\n",
    "            fewshot_loss: str = 'default',\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            add_history: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            model: str = 'timegpt-1',\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ) -> fugue.AnyDataFrame:\n",
    "        kwargs = dict(\n",
    "            h=h,\n",
    "            freq=freq,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            fewshot_steps=fewshot_steps,\n",
    "            fewshot_loss=fewshot_loss,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            validate_token=validate_token,\n",
    "            add_history=add_history,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            model=model,\n",
    "        )\n",
    "        schema = self._get_forecast_schema(\n",
    "            id_col=id_col, \n",
    "            time_col=time_col, \n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "        )\n",
    "        fcst_df = self._distribute_method(\n",
    "            method=self._forecast if X_df is None else self._forecast_x,\n",
    "            df=df,\n",
    "            kwargs=kwargs,\n",
    "            schema=schema,\n",
    "            num_partitions=num_partitions,\n",
    "            id_col=id_col,\n",
    "            X_df=X_df,   \n",
    "        )\n",
    "        return fcst_df\n",
    "\n",
    "    def detect_anomalies(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            level: Union[int, float] = 99,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            model: str = 'timegpt-1',\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ) -> fugue.AnyDataFrame:\n",
    "        kwargs = dict(\n",
    "            freq=freq,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            validate_token=validate_token,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            model=model,\n",
    "        )\n",
    "        schema = self._get_anomalies_schema(id_col=id_col, time_col=time_col)\n",
    "        anomalies_df = self._distribute_method(\n",
    "            method=self._detect_anomalies,\n",
    "            df=df,\n",
    "            kwargs=kwargs,\n",
    "            schema=schema,\n",
    "            num_partitions=num_partitions,\n",
    "            id_col=id_col,\n",
    "            X_df=None,\n",
    "        )\n",
    "        return anomalies_df\n",
    "\n",
    "    def cross_validation(\n",
    "            self,\n",
    "            df: fugue.AnyDataFrame,\n",
    "            h: int,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            quantiles: Optional[List[float]] = None,\n",
    "            fewshot_steps: int = 0,\n",
    "            fewshot_loss: str = 'default',\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            model: str = 'timegpt-1',\n",
    "            n_windows: int = 1,\n",
    "            step_size: Optional[int] = None,\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ) -> fugue.AnyDataFrame:\n",
    "        kwargs = dict(\n",
    "            h=h,\n",
    "            freq=freq,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            fewshot_steps=fewshot_steps,\n",
    "            fewshot_loss=fewshot_loss,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            validate_token=validate_token,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "            model=model,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,\n",
    "        )\n",
    "        schema = self._get_forecast_schema(\n",
    "            id_col=id_col, \n",
    "            time_col=time_col, \n",
    "            level=level,\n",
    "            quantiles=quantiles,\n",
    "            cv=True,\n",
    "        )\n",
    "        fcst_df = self._distribute_method(\n",
    "            method=self._cross_validation,\n",
    "            df=df,\n",
    "            kwargs=kwargs,\n",
    "            schema=schema,\n",
    "            num_partitions=num_partitions,\n",
    "            id_col=id_col,\n",
    "        )\n",
    "        return fcst_df\n",
    "    \n",
    "    def _instantiate_timegpt(self):\n",
    "        from nixtlats.timegpt import _TimeGPT\n",
    "        timegpt = _TimeGPT(\n",
    "            token=self.token, \n",
    "            environment=self.environment,\n",
    "            max_retries=self.max_retries,\n",
    "            retry_interval=self.retry_interval,\n",
    "            max_wait_time=self.max_wait_time,\n",
    "        )\n",
    "        return timegpt\n",
    "\n",
    "    def _forecast(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        timegpt = self._instantiate_timegpt()\n",
    "        return timegpt._forecast(df=df, **kwargs)\n",
    "\n",
    "    def _forecast_x(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            X_df: pd.DataFrame,\n",
    "            kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        timegpt = self._instantiate_timegpt()\n",
    "        return timegpt._forecast(df=df, X_df=X_df, **kwargs)\n",
    "\n",
    "    def _detect_anomalies(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        timegpt = self._instantiate_timegpt()\n",
    "        return timegpt._detect_anomalies(df=df, **kwargs)\n",
    "\n",
    "    def _cross_validation(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        timegpt = self._instantiate_timegpt()\n",
    "        return timegpt._cross_validation(df=df, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_forecast_schema(id_col, time_col, level, quantiles, cv=False):\n",
    "        schema = f'{id_col}:string,{time_col}:datetime'\n",
    "        if cv:\n",
    "            schema = f'{schema},cutoff:datetime'\n",
    "        schema = f'{schema},TimeGPT:double'\n",
    "        if (level is not None) and (quantiles is not None):\n",
    "            raise Exception(\n",
    "                \"you should include `level` or `quantiles` but not both.\"\n",
    "            )\n",
    "        if level is not None:\n",
    "            level = sorted(level)\n",
    "            schema = f'{schema},{\",\".join([f\"TimeGPT-lo-{lv}:double\" for lv in reversed(level)])}'\n",
    "            schema = f'{schema},{\",\".join([f\"TimeGPT-hi-{lv}:double\" for lv in level])}'\n",
    "        if quantiles is not None:\n",
    "            quantiles = sorted(quantiles)\n",
    "            q_cols = [f'TimeGPT-q-{int(q * 100)}:double' for q in quantiles]\n",
    "            schema = f'{schema},{\",\".join(q_cols)}'\n",
    "        return Schema(schema)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_anomalies_schema(id_col, time_col):\n",
    "        schema = f'{id_col}:string,{time_col}:datetime,anomaly:int'\n",
    "        return Schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "\n",
    "from fastcore.test import test_eq\n",
    "from dotenv import load_dotenv\n",
    "from utilsforecast.data import generate_series\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        horizon: int = 12,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    test_eq(n_series * 12, len(fcst_df))\n",
    "    cols = fcst_df.columns.to_list()\n",
    "    exp_cols = [id_col, time_col, 'TimeGPT']\n",
    "    if 'level' in fcst_kwargs:\n",
    "        level = sorted(fcst_kwargs['level'])\n",
    "        exp_cols.extend([f'TimeGPT-lo-{lv}' for lv in reversed(level)])\n",
    "        exp_cols.extend([f'TimeGPT-hi-{lv}' for lv in level])\n",
    "    test_eq(cols, exp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_diff_results_diff_models(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        horizon: int = 12, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        model='timegpt-1',\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        model='timegpt-1-long-horizon',\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    test_fail(\n",
    "        lambda: pd.testing.assert_frame_equal(\n",
    "            fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "            fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        ),\n",
    "        contains='(column name=\"TimeGPT\") are different',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_same_results_num_partitions(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        horizon: int = 12, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_cv_same_results_num_partitions(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        horizon: int = 12, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.cross_validation(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = distributed_timegpt.cross_validation(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_dataframe(df: fugue.AnyDataFrame):\n",
    "    test_cv_same_results_num_partitions(df, n_windows=2, step_size=1)\n",
    "    test_cv_same_results_num_partitions(df, n_windows=3, step_size=None, horizon=1)\n",
    "    test_cv_same_results_num_partitions(df, model='timegpt-1-long-horizon', horizon=1)\n",
    "    test_forecast_diff_results_diff_models(df)\n",
    "    test_forecast(df, num_partitions=1)\n",
    "    test_forecast(df, level=[90, 80], num_partitions=1)\n",
    "    test_forecast_same_results_num_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_dataframe_diff_cols(df: fugue.AnyDataFrame, id_col: str = 'id_col', time_col: str = 'time_col'):\n",
    "    test_forecast(df, id_col=id_col, time_col=time_col, num_partitions=1)\n",
    "    test_forecast(df, id_col=id_col, time_col=time_col, level=[90, 80], num_partitions=1)\n",
    "    test_forecast_same_results_num_partitions(df, id_col=id_col, time_col=time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_x(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        X_df: fugue.AnyDataFrame,\n",
    "        horizon: int = 24,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        X_df=X_df,\n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    n_series = fa.as_pandas(X_df)[id_col].nunique()\n",
    "    test_eq(n_series * horizon, len(fcst_df))\n",
    "    cols = fcst_df.columns.to_list()\n",
    "    exp_cols = [id_col, time_col, 'TimeGPT']\n",
    "    if 'level' in fcst_kwargs:\n",
    "        level = sorted(fcst_kwargs['level'])\n",
    "        exp_cols.extend([f'TimeGPT-lo-{lv}' for lv in reversed(level)])\n",
    "        exp_cols.extend([f'TimeGPT-hi-{lv}' for lv in level])\n",
    "    test_eq(cols, exp_cols)\n",
    "    fcst_df_2 = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    equal_arrays = np.array_equal(\n",
    "        fcst_df.sort_values([id_col, time_col])['TimeGPT'].values,\n",
    "        fcst_df_2.sort_values([id_col, time_col])['TimeGPT'].values\n",
    "    )\n",
    "    assert not equal_arrays, 'Forecasts with and without ex vars are equal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_x_same_results_num_partitions(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        X_df: fugue.AnyDataFrame,\n",
    "        horizon: int = 24, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        X_df=X_df,\n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = distributed_timegpt.forecast(\n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    equal_arrays = np.array_equal(\n",
    "        fcst_df.sort_values([id_col, time_col])['TimeGPT'].values,\n",
    "        fcst_df_2.sort_values([id_col, time_col])['TimeGPT'].values\n",
    "    )\n",
    "    assert not equal_arrays, 'Forecasts with and without ex vars are equal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_x_dataframe(df: fugue.AnyDataFrame, X_df: fugue.AnyDataFrame):\n",
    "    test_forecast_x(df, X_df, num_partitions=1)\n",
    "    test_forecast_x(df, X_df, level=[90, 80], num_partitions=1)\n",
    "    test_forecast_x_same_results_num_partitions(df, X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast_x_dataframe_diff_cols(df: fugue.AnyDataFrame, X_df: fugue.AnyDataFrame, id_col: str = 'id_col', time_col: str = 'time_col'):\n",
    "    test_forecast_x(df, X_df, id_col=id_col, time_col=time_col, num_partitions=1)\n",
    "    test_forecast_x(df, X_df, id_col=id_col, time_col=time_col, level=[90, 80], num_partitions=1)\n",
    "    test_forecast_x_same_results_num_partitions(df, X_df, id_col=id_col, time_col=time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_anomalies(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **anomalies_kwargs,\n",
    "    ):\n",
    "    anomalies_df = distributed_timegpt.detect_anomalies(\n",
    "        df=df, \n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **anomalies_kwargs,\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    test_eq(fa.as_pandas(df)[id_col].unique(), anomalies_df[id_col].unique())\n",
    "    cols = anomalies_df.columns.to_list()\n",
    "    exp_cols = [id_col, time_col, 'anomaly']\n",
    "    test_eq(cols, exp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_anomalies_same_results_num_partitions(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **anomalies_kwargs,\n",
    "    ):\n",
    "    anomalies_df = distributed_timegpt.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    anomalies_df_2 = distributed_timegpt.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df_2 = fa.as_pandas(anomalies_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        anomalies_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        anomalies_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_anomalies_diff_results_diff_models(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **anomalies_kwargs,\n",
    "    ):\n",
    "    anomalies_df = distributed_timegpt.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        model='timegpt-1',\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df = fa.as_pandas(anomalies_df)\n",
    "    anomalies_df_2 = distributed_timegpt.detect_anomalies(\n",
    "        df=df, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        model='timegpt-1-long-horizon',\n",
    "        **anomalies_kwargs\n",
    "    )\n",
    "    anomalies_df_2 = fa.as_pandas(anomalies_df_2)\n",
    "    test_fail(\n",
    "        lambda: pd.testing.assert_frame_equal(\n",
    "            anomalies_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "            anomalies_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        ),\n",
    "        contains='(column name=\"TimeGPT\") are different',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_anomalies_dataframe(df: fugue.AnyDataFrame):\n",
    "    test_anomalies(df, num_partitions=1)\n",
    "    test_anomalies(df, level=90, num_partitions=1)\n",
    "    test_anomalies_same_results_num_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_anomalies_dataframe_diff_cols(df: fugue.AnyDataFrame, id_col: str = 'id_col', time_col: str = 'time_col'):\n",
    "    test_anomalies(df, id_col=id_col, time_col=time_col, num_partitions=1)\n",
    "    test_anomalies(df, id_col=id_col, time_col=time_col, level=90, num_partitions=1)\n",
    "    test_anomalies_same_results_num_partitions(df, id_col=id_col, time_col=time_col)\n",
    "    # @A: document behavior with exogenous variables in distributed environments.  \n",
    "    #test_anomalies_same_results_num_partitions(df, id_col=id_col, time_col=time_col, date_features=True, clean_ex_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_quantiles(df: fugue.AnyDataFrame, id_col: str = 'id_col', time_col: str = 'time_col'):\n",
    "    test_qls = list(np.arange(0.1, 1, 0.1))\n",
    "    exp_q_cols = [f\"TimeGPT-q-{int(q * 100)}\" for q in test_qls]\n",
    "    def test_method_qls(method, **kwargs):\n",
    "        df_qls = method(\n",
    "            df=df, \n",
    "            h=12, \n",
    "            id_col=id_col,\n",
    "            time_col=time_col, \n",
    "            quantiles=test_qls,\n",
    "            **kwargs\n",
    "        )\n",
    "        df_qls = fa.as_pandas(df_qls)\n",
    "        assert all(col in df_qls.columns for col in exp_q_cols)\n",
    "        # test monotonicity of quantiles\n",
    "        df_qls.apply(lambda x: x.is_monotonic_increasing, axis=1).sum() == len(exp_q_cols)\n",
    "    test_method_qls(distributed_timegpt.forecast)\n",
    "    test_method_qls(distributed_timegpt.forecast, add_history=True)\n",
    "    test_method_qls(distributed_timegpt.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "distributed_timegpt = _DistributedTimeGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series, min_length=100)\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "series_diff_cols = series.copy()\n",
    "renamer = {'unique_id': 'id_col', 'ds': 'time_col'}\n",
    "series_diff_cols = series_diff_cols.rename(columns=renamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# data for exogenous tests\n",
    "df_x = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-with-ex-vars.csv')\n",
    "future_ex_vars_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-future-ex-vars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df = spark.createDataFrame(series)\n",
    "spark_diff_cols_df = spark.createDataFrame(series_diff_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_quantiles(spark_df, id_col=\"unique_id\", time_col=\"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_forecast_dataframe(spark_df)\n",
    "test_forecast_dataframe_diff_cols(spark_diff_cols_df)\n",
    "test_anomalies_dataframe(spark_df)\n",
    "test_anomalies_dataframe_diff_cols(spark_diff_cols_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test exogenous variables\n",
    "spark_df_x = spark.createDataFrame(df_x)\n",
    "spark_future_ex_vars_df = spark.createDataFrame(future_ex_vars_df)\n",
    "test_forecast_x_dataframe(spark_df_x, spark_future_ex_vars_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test x different cols\n",
    "spark_df_x_diff_cols = spark.createDataFrame(df_x.rename(columns=renamer))\n",
    "spark_future_ex_vars_df_diff_cols = spark.createDataFrame(future_ex_vars_df.rename(columns=renamer))\n",
    "test_forecast_x_dataframe_diff_cols(spark_df_x_diff_cols, spark_future_ex_vars_df_diff_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "dask_df = dd.from_pandas(series, npartitions=2)\n",
    "dask_diff_cols_df = dd.from_pandas(series_diff_cols, npartitions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_quantiles(dask_df, id_col=\"unique_id\", time_col=\"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_forecast_dataframe(dask_df)\n",
    "test_forecast_dataframe_diff_cols(dask_diff_cols_df)\n",
    "test_anomalies_dataframe(dask_df)\n",
    "test_anomalies_dataframe_diff_cols(dask_diff_cols_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test exogenous variables\n",
    "dask_df_x = dd.from_pandas(df_x, npartitions=2)\n",
    "dask_future_ex_vars_df = dd.from_pandas(future_ex_vars_df, npartitions=2)\n",
    "test_forecast_x_dataframe(dask_df_x, dask_future_ex_vars_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test x different cols\n",
    "dask_df_x_diff_cols = dd.from_pandas(df_x.rename(columns=renamer), npartitions=2)\n",
    "dask_future_ex_vars_df_diff_cols = dd.from_pandas(future_ex_vars_df.rename(columns=renamer), npartitions=2)\n",
    "test_forecast_x_dataframe_diff_cols(dask_df_x_diff_cols, dask_future_ex_vars_df_diff_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import ray\n",
    "from ray.cluster_utils import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ray_cluster = Cluster(\n",
    "    initialize_head=True,\n",
    "    head_node_args={\"num_cpus\": 2}\n",
    ")\n",
    "ray.init(address=ray_cluster.address, ignore_reinit_error=True)\n",
    "# add mock node to simulate a cluster\n",
    "mock_node = ray_cluster.add_node(num_cpus=2)\n",
    "ray_df = ray.data.from_pandas(series)\n",
    "ray_diff_cols_df = ray.data.from_pandas(series_diff_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quantiles(ray_df, id_col=\"unique_id\", time_col=\"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_forecast_dataframe(ray_df)\n",
    "test_forecast_dataframe_diff_cols(ray_diff_cols_df)\n",
    "test_anomalies_dataframe(ray_df)\n",
    "test_anomalies_dataframe_diff_cols(ray_diff_cols_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test exogenous variables\n",
    "ray_df_x = ray.data.from_pandas(df_x)\n",
    "ray_future_ex_vars_df = ray.data.from_pandas(future_ex_vars_df)\n",
    "test_forecast_x_dataframe(ray_df_x, ray_future_ex_vars_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test x different cols\n",
    "ray_df_x_diff_cols = ray.data.from_pandas(df_x.rename(columns=renamer))\n",
    "ray_future_ex_vars_df_diff_cols = ray.data.from_pandas(future_ex_vars_df.rename(columns=renamer))\n",
    "test_forecast_x_dataframe_diff_cols(ray_df_x_diff_cols, ray_future_ex_vars_df_diff_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
