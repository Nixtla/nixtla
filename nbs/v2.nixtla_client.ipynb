{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7c905-5495-4fdb-9651-eadbe99a5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeef897-a7b9-4cdc-be12-de1bf56ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp v2.nixtla_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de9d09-99fd-4c2e-8737-06b1f3674800",
   "metadata": {},
   "source": [
    "# V2 Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee3a4c-f29b-400a-846c-91f1ee50da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "import math\n",
    "import os\n",
    "from itertools import chain\n",
    "from typing import List, Literal, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "from utilsforecast.compat import DataFrame, pl_DataFrame, pl_Series\n",
    "from utilsforecast.validation import validate_format, validate_freq\n",
    "\n",
    "import nixtla.types as types\n",
    "from nixtla.client import AsyncNixtla, Nixtla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268dbd1-e52a-455d-8acb-5430b09d554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_LOSS = Literal[\"default\", \"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]\n",
    "_MODEL = Literal[\"timepgt-1\", \"timegpt-1-long-horizon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a14e6b-d604-4e40-8df1-ebb485480261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NixtlaClient:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        timeout: int = 60,\n",
    "        max_retries: int = 6,\n",
    "    ):\n",
    "        if api_key is None:\n",
    "            api_key = os.environ['NIXTLA_API_KEY']\n",
    "        if base_url is None:\n",
    "            base_url = os.getenv('NIXTLA_BASE_URL', 'https://dashboard.nixtla.io/api')\n",
    "        self._client_kwargs = {'base_url': base_url, 'token': api_key, 'timeout': timeout}\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def _partition_series(self, payload, n_part, h):\n",
    "        parts = []\n",
    "        series = payload.pop('series')\n",
    "        n_series = len(series['sizes'])\n",
    "        n_part = min(n_part, n_series)\n",
    "        series_per_part = math.ceil(n_series / n_part)\n",
    "        indptr = np.array([0] + series['sizes']).cumsum()\n",
    "        for i in range(0, n_series, series_per_part):\n",
    "            sizes = series['sizes'][i : i + series_per_part]\n",
    "            part_idxs = slice(indptr[i], indptr[i + series_per_part])\n",
    "            part_series = {\n",
    "                'y': series['y'][part_idxs],\n",
    "                'sizes': sizes,\n",
    "            }\n",
    "            if series['X'] is None:\n",
    "                part_series['X'] = None\n",
    "                part_series['X_future'] = None\n",
    "            else:\n",
    "                part_series['X'] = [x[part_idxs] for x in series['X']]\n",
    "                part_series['X_future'] = [x[i * h : (i + series_per_part) * h] for x in series['X_future']]\n",
    "            parts.append({'series': part_series, **payload})\n",
    "        return parts\n",
    "\n",
    "    async def async_forecast(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        h: int,\n",
    "        freq: str,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        quantiles: Optional[List[float]] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        finetune_loss: _LOSS = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        validate_api_key: bool = False,\n",
    "        add_history: bool = False,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        model: _MODEL = 'timegpt-1',\n",
    "        num_partitions: Optional[int] = None,\n",
    "    ):\n",
    "        if add_history:\n",
    "            raise NotImplementedError('add_history=True not yet supported')\n",
    "        if quantiles is not None:\n",
    "            raise NotImplementedError('quantiles!=None not yet supported')\n",
    "        validate_format(df=df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "        validate_freq(times=df[time_col], freq=freq)\n",
    "        processed = ufp.process_df(\n",
    "            df=df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].tolist()\n",
    "            if X_df is None:\n",
    "                raise ValueError(\n",
    "                    'Training exogenous features were provided but no future values.'\n",
    "                )\n",
    "            processed_X = ufp.process_df(\n",
    "                df=X_df, id_col=id_col, time_col=time_col, target_col=None,\n",
    "            )\n",
    "            X_future = processed_X.data.tolist()\n",
    "            x_cols = [c for c in df.columns if c not in (id_col, time_col, target_col)]\n",
    "        else:\n",
    "            X = None\n",
    "            X_future = None\n",
    "            x_cols = None\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0].tolist(),\n",
    "                'sizes': np.diff(processed.indptr).tolist(),\n",
    "                'X': X,\n",
    "                'X_future': X_future,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'freq': freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_loss': finetune_loss,\n",
    "        }\n",
    "        if num_partitions is None:\n",
    "            client = Nixtla(**self._client_kwargs)\n",
    "            resp = client.v_2_forecast(\n",
    "                **payload,\n",
    "                request_options={'max_retries': self.max_retries}\n",
    "            )\n",
    "        else:\n",
    "            client = AsyncNixtla(**self._client_kwargs)\n",
    "            payloads = self._partition_series(payload, num_partitions, h)\n",
    "            coros = [\n",
    "                client.v_2_forecast(**payload, request_options={'max_retries': self.max_retries})\n",
    "                for payload in payloads\n",
    "            ]\n",
    "            results = await asyncio.gather(*coros)\n",
    "            resp = types.ForecastOutput(\n",
    "                mean=list(chain.from_iterable(res.mean for res in results)),\n",
    "                intervals=None if level is None else {}\n",
    "            )\n",
    "            if payload['level'] is not None:\n",
    "                for lvl in payload['level']:\n",
    "                    for side in ['lo', 'hi']:\n",
    "                        resp.intervals[f'{side}-{lvl}'] = list(chain.from_iterable(res.intervals[f'{side}-{lvl}'] for res in results))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            times = pd.Index(processed.times)\n",
    "        elif isinstance(df, pl_DataFrame):\n",
    "            times = pl_Series(processed.times)\n",
    "        out = ufp.make_future_dataframe(\n",
    "            uids=processed.uids,\n",
    "            last_times=times,\n",
    "            freq=freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp.mean)\n",
    "        if resp.intervals is not None:\n",
    "            for lvl, vals in resp.intervals.items():\n",
    "                out = ufp.assign_columns(out, f'TimeGPT-{lvl}', vals)\n",
    "        if resp.weights_x is not None:\n",
    "            self.weights_x = type(df)({\n",
    "                'features': x_cols,\n",
    "                'weights': resp.weights_x,\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        h: int,\n",
    "        freq: str,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        quantiles: Optional[List[float]] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        finetune_loss: _LOSS = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        validate_api_key: bool = False,\n",
    "        add_history: bool = False,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        model: _MODEL = 'timegpt-1',\n",
    "        num_partitions: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        quantiles : List[float], optional (default=None)\n",
    "            Quantiles to forecast, list between (0, 1).\n",
    "            `level` and `quantiles` should not be used simultaneously.\n",
    "            The output dataframe will have the quantile columns\n",
    "            formatted as TimeGPT-q-(100 * q) for each q.\n",
    "            100 * q represents percentiles but we choose this notation\n",
    "            to avoid having dots in column names.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the\n",
    "            new data.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_api_key : bool (default=False)\n",
    "            If True, validates api_key before \n",
    "            sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources in distributed environments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            raise Exception(\n",
    "                \"Can't use this function when there's already a running loop. \"\n",
    "                \"Use `await async_forecast(...) instead.`\"\n",
    "            )\n",
    "        return asyncio.run(\n",
    "            self.async_forecast(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                quantiles=quantiles,\n",
    "                finetune_steps=finetune_steps,\n",
    "                finetune_loss=finetune_loss,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_api_key=validate_api_key,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                model=model,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bc225-6ae3-4305-b002-ade5abbc145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.data import generate_series\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e4819-4a57-49b9-8ef0-38497d9f8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_series(4)\n",
    "series['unique_id'] = series['unique_id'].astype('int64')\n",
    "series_pl = pl.from_pandas(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ff47f-5bbe-4219-b90f-794acc9f9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = NixtlaClient(max_retries=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc288a9-66b5-4746-883f-158bfb71beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.async_forecast(df=series, h=2, freq=\"D\", level=[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed410b8-8e47-4dea-abb2-c9fe61b999f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.async_forecast(df=series, h=2, freq=\"D\", level=[80], num_partitions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd08c9-2b2c-46ec-b77a-ed5b8dbac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.async_forecast(df=series_pl, h=2, freq=\"1d\", level=[80])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
