{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "!pip install -Uqq nixtla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "from nixtla.utils import in_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "IN_COLAB = in_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if not IN_COLAB:\n",
    "    from nixtla.utils import colab_badge\n",
    "    from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Forecast Accuracy with TimeGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to use TimeGPT for forecasting and explore three common strategies to enhance forecast accuracy. We use the hourly electricity price data from Germany as our example dataset. Before running the notebook, please initiate a NixtlaClient object with your api_key in the code snippet below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Summary\n",
    "\n",
    "| Steps | Description                  | MAE  | MAE Improvement (%) | RMSE  | RMSE Improvement (%) |\n",
    "|-------|------------------------------|------|---------------------|-------|----------------------|\n",
    "| 0     | Benchmark Model              | 18.5 | N/A                 | 20.0  | N/A                  |\n",
    "| 1     | Add Fine-Tuning Steps        | 12.0 | 35.14%              | 13.3  | 33.5%                |\n",
    "| 2     | Adjust Fine-Tuning Loss      | 9.2  | 50.27%              | 12.0  | 40.0%                |\n",
    "| 3     | Add Exogenous Variables      | 10.1 | 45.41%              | 11.4  | 43.0%                |\n",
    "| 4     | Switch to Long-Horizon Model  | 6.4  | 65.38%              | 7.7   | 61.50%               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "if not IN_COLAB:\n",
    "    load_dotenv()\n",
    "    colab_badge('docs/frequently-asked-questions/01_how_to_improve_forecast_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install and import the required packages, initialize the Nixtla client and create a function for calculating evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from utilsforecast.plotting import plot_series\n",
    "from nixtla import NixtlaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    api_key = 'LYxnW0M1FvKMaIS6yFwP2wnG3IqWFP86NfOsreLs4Us0LBYgG3xTWOzzNgm6lYiOHm85ff6WIyplg6yHqvr2FrFGvxtazVJIpFN3f9ICpV2VIpGMFA7GHnYMBVyHlFmVJc5UbjSvNgYduDUiJi0BkzUxxle3t4Q933NYN8K4d3Xt36qRTKN3OjRgbNs4ycT21IJLqDV8F4OxvthzIy82Q0TOS7An4QShmgm8sl5uarui782lQ6SwLTHq4r1MZYts'\n",
    "    # api_key = 'my_api_key_provided_by_nixtla'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load in dataset\n",
    "In this notebook, we use hourly electricity prices as our example dataset, which consists of 5 time series, each with approximately 1700 data points. For demonstration purposes, we focus on the German electricity price series. The time series is split, with the last 48 steps (2 days) set aside as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-with-ex-vars.csv')\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df_sub = df.query('unique_id == \"DE\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_sub.query('ds < \"2017-12-29\"')\n",
    "df_test = df_sub.query('ds >= \"2017-12-29\"')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df_train[['unique_id','ds','y']][-200:], forecasts_df= df_test[['unique_id','ds','y']].rename(columns={'y': 'test'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Forecasting using TimeGPT\n",
    "We used TimeGPT to generate a one-shot forecast for the time series. As illustrated in the plot, TimeGPT captures the overall trend reasonably well, but it falls short in modeling the short-term fluctuations and cyclical patterns present in the actual data. During the test period, the model achieved a Mean Absolute Error (MAE) of 18.5 and a Root Mean Square Error (RMSE) of 20. This forecast serves as a baseline for further comparison and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_timegpt = nixtla_client.forecast(df = df_train[['unique_id','ds','y']],\n",
    "                                      h=2*24,\n",
    "                                      target_col = 'y',\n",
    "                                      level = [90, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse = evaluate_performance(df_test['y'], fcst_timegpt['TimeGPT'])\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df_sub.iloc[-150:], forecasts_df= fcst_timegpt, level = [90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods to Improve Forecast Accuracy\n",
    "### 3a. Add Finetune Steps\n",
    "The first approach to enhance forecast accuracy is to increase the number of fine-tuning steps. The fine-tuning process adjusts the weights within the TimeGPT model, allowing it to better fit your customized data. This adjustment enables TimeGPT to learn the nuances of your time series more effectively, leading to more accurate forecasts. With 30 fine-tuning steps, we observe that the MAE decreases to 12 and the RMSE drops to 13.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_finetune_df = nixtla_client.forecast(df=df_train[['unique_id', 'ds', 'y']],\n",
    "                                          h=24*2,\n",
    "                                          finetune_steps = 30,\n",
    "                                          level=[90, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse = evaluate_performance(df_test['y'], fcst_finetune_df['TimeGPT'])\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df_sub[-200:], forecasts_df= fcst_finetune_df, level = [90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Finetune with Different Loss Function\n",
    "The second way to further reduce forecast error is to adjust the loss function used during fine-tuning. You can specify your customized loss function using the `finetune_loss` parameter. By modifying the loss function, we observe that the MAE decreases to 10 and the RMSE reduces to 11.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_finetune_mae_df = nixtla_client.forecast(df=df_train[['unique_id', 'ds', 'y']],\n",
    "                                          h=24*2,\n",
    "                                          finetune_steps = 30,\n",
    "                                          finetune_loss = 'mae',\n",
    "                                          level=[90, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse = evaluate_performance(df_test['y'], fcst_finetune_mae_df['TimeGPT'])\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df_sub[-200:], forecasts_df= fcst_finetune_mae_df, level = [90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Forecast with Exogenous Variables\n",
    "Exogenous variables are external factors or predictors that are not part of the target time series but can influence its behavior. Incorporating these variables can provide the model with additional context, improving its ability to understand complex relationships and patterns in the data.\n",
    "\n",
    "To use exogenous variables in TimeGPT, pair each point in your input time series with the corresponding external data. If you have future values available for these variables during the forecast period, include them using the X_df parameter. Otherwise, you can omit this parameter and still see improvements using only historical values. In the example below, we incorporate 8 historical exogenous variables along with their values during the test period, which reduces the MAE and RMSE to 9.2 and 11.9, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_ex_vars_df = df_test.drop(columns = ['y'])\n",
    "future_ex_vars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_ex_vars_df = nixtla_client.forecast(df=df_train,\n",
    "                                         X_df=future_ex_vars_df,\n",
    "                                         h=24*2,\n",
    "                                         level=[90, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse = evaluate_performance(df_test['y'], fcst_ex_vars_df['TimeGPT'])\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df_sub[-200:], forecasts_df= fcst_ex_vars_df, level = [90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. TimeGPT for Long Horizon Forecasting\n",
    "When the forecasting period is too long, the predicted results may not be as accurate. TimeGPT performs best with forecast periods that are shorter than one complete cycle of the time series. For longer forecast periods, switching to the timegpt-1-long-horizon model can yield better results. You can specify this model by using the model parameter.\n",
    "\n",
    "In the electricity price time series used here, one cycle is 24 steps (representing one day). Since we’re forecasting two days (48 steps) into the future, using timegpt-1-long-horizon significantly improves the forecasting accuracy, reducing the MAE to 5.7 and RMSE to 7.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_long_df = nixtla_client.forecast(df=df_train[['unique_id', 'ds', 'y']],\n",
    "                                          h=24*2,\n",
    "                                          model = 'timegpt-1-long-horizon',\n",
    "                                          level=[90, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse = evaluate_performance(df_test['y'], fcst_long_df['TimeGPT'])\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df_sub[-200:], forecasts_df= fcst_long_df, level = [90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we demonstrated four effective strategies for enhancing forecast accuracy with TimeGPT:\n",
    "\n",
    "1. **Increasing the number of fine-tuning steps.**\n",
    "2. **Adjusting the fine-tuning loss function.**\n",
    "3. **Incorporating exogenous variables.**\n",
    "4. **Switching to the long-horizon model for extended forecasting periods.**\n",
    "\n",
    "We encourage you to experiment with these hyperparameters to identify the optimal settings that best suit your specific needs. Additionally, please refer to our documentation for further features, such as **model explainability** and more.\n",
    "\n",
    "In the examples provided, after applying these methods, we observed significant improvements in forecast accuracy metrics, as summarized below.\n",
    "\n",
    "### Result Summary\n",
    "\n",
    "| Steps | Description                  | MAE  | MAE Improvement (%) | RMSE  | RMSE Improvement (%) |\n",
    "|-------|------------------------------|------|---------------------|-------|----------------------|\n",
    "| 0     | Benchmark Model              | 18.5 | N/A                 | 20.0  | N/A                  |\n",
    "| 1     | Add Fine-Tuning Steps        | 12.0 | 35.14%              | 13.3  | 33.5%                |\n",
    "| 2     | Adjust Fine-Tuning Loss      | 9.2  | 50.27%              | 12.0  | 40.0%                |\n",
    "| 3     | Add Exogenous Variables      | 10.1 | 45.41%              | 11.4  | 43.0%                |\n",
    "| 4     | Switch to Long-Horizon Model  | 6.4  | 65.38%              | 7.7   | 61.50%               |\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
