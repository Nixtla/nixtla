{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7c905-5495-4fdb-9651-eadbe99a5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeef897-a7b9-4cdc-be12-de1bf56ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp v2.nixtla_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de9d09-99fd-4c2e-8737-06b1f3674800",
   "metadata": {},
   "source": [
    "# V2 Nixtla Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee3a4c-f29b-400a-846c-91f1ee50da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from itertools import chain\n",
    "from typing import Any, List, Literal, Mapping, Optional, Union\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "from pydantic import NonNegativeInt, PositiveInt\n",
    "from utilsforecast.compat import DataFrame, pl_DataFrame, pl_Series\n",
    "from utilsforecast.validation import validate_format, validate_freq\n",
    "\n",
    "from nixtla.core.api_error import ApiError\n",
    "from nixtla.core.http_client import HttpClient\n",
    "from nixtla.utils import _restrict_input_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6dcc9-14a2-45f5-944d-62f31bb931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268dbd1-e52a-455d-8acb-5430b09d554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_LOSS = Literal[\"default\", \"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]\n",
    "_MODEL = Literal[\"timepgt-1\", \"timegpt-1-long-horizon\"]\n",
    "\n",
    "def _tail(proc: ufp.ProcessedDF, n: int) -> ufp.ProcessedDF:\n",
    "    n_series = proc.indptr.size - 1\n",
    "    new_sizes = np.minimum(np.diff(proc.indptr), n)\n",
    "    new_indptr = np.append(0, new_sizes.cumsum())\n",
    "    new_data = np.empty_like(proc.data, shape=(new_indptr[-1], proc.data.shape[1]))\n",
    "    for i in range(n_series):\n",
    "        new_data[new_indptr[i] : new_indptr[i + 1]] = proc.data[\n",
    "            proc.indptr[i + 1] - new_sizes[i] : proc.indptr[i + 1]\n",
    "        ]\n",
    "    return ufp.ProcessedDF(\n",
    "        uids=proc.uids,\n",
    "        last_times=proc.last_times,\n",
    "        data=new_data,\n",
    "        indptr=new_indptr,\n",
    "        sort_idxs=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a14e6b-d604-4e40-8df1-ebb485480261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NixtlaClient:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "        timeout: int = 60,\n",
    "        max_retries: int = 6,\n",
    "    ):\n",
    "        if api_key is None:\n",
    "            api_key = os.environ['NIXTLA_API_KEY']\n",
    "        if base_url is None:\n",
    "            base_url = os.getenv('NIXTLA_BASE_URL', 'https://dashboard.nixtla.io/api')\n",
    "        self._client_kwargs = {\n",
    "            'base_url': base_url,\n",
    "            'headers': {'Authorization': f'Bearer {api_key}'},\n",
    "            'timeout': timeout,\n",
    "        }\n",
    "        self.max_retries = max_retries\n",
    "        self._model_params: Mapping[str, Tuple[int]] = {}\n",
    "\n",
    "    def _make_request(self, endpoint: str, payload: Mapping[str, any]):\n",
    "        with httpx.Client(**self._client_kwargs) as httpx_client:\n",
    "            client = HttpClient(httpx_client=httpx_client)\n",
    "            resp = client.request(\n",
    "                method='post',\n",
    "                url=endpoint,\n",
    "                json=payload,\n",
    "                max_retries=self.max_retries,\n",
    "            )\n",
    "            resp_body = resp.json()\n",
    "            if resp.status_code != 200:\n",
    "                raise ApiError(status_code=resp.status_code, body=resp_body)\n",
    "        if 'data' in resp_body:\n",
    "            resp_body = resp_body['data']\n",
    "        return resp_body\n",
    "\n",
    "    def _get_model_params(self, freq: str, model: str):\n",
    "        key = (freq, model)\n",
    "        if key not in self._model_params:\n",
    "            payload = {'freq': freq, 'model': model}\n",
    "            params = self._make_request('/model_params', payload)['detail']\n",
    "            self._model_params[key] = (params['input_size'], params['horizon'])\n",
    "        return self._model_params[key]\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        h: PositiveInt,\n",
    "        freq: str,    \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        finetune_steps: NonNegativeInt = 0,\n",
    "        finetune_loss: _LOSS = 'default',\n",
    "        clean_ex_first: bool = True,\n",
    "        model: _MODEL = 'timegpt-1',\n",
    "    ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas or polars DataFrame\n",
    "            dataframe to forecast. Must have at least [id_col, time_col, target_col] columns.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            pandas or polars frequency string.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values must be timestamps.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [id_col, time_col] columns and `df`'s future exogenous.\n",
    "        level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune learning TimeGPT in the\n",
    "            new data.\n",
    "        finetune_loss : str (default='default')\n",
    "            Loss function to use for finetuning. Options are: `default`, `mae`, `mse`, `rmse`, `mape`, and `smape`.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        model : str (default='timegpt-1')\n",
    "            Model to use as a string. Options are: `timegpt-1`, and `timegpt-1-long-horizon`. \n",
    "            We recommend using `timegpt-1-long-horizon` for forecasting \n",
    "            if you want to predict more than one seasonal \n",
    "            period given the frequency of your data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        logger.info('Validating inputs...')\n",
    "        validate_format(df=df, id_col=id_col, time_col=time_col, target_col=target_col)\n",
    "        validate_freq(times=df[time_col], freq=freq)\n",
    "        exog_df = [c for c in df.columns if c not in (id_col, time_col, target_col)]\n",
    "        if X_df is not None:\n",
    "            exog_X_df = [c for c in X_df.columns if c not in (id_col, time_col)]\n",
    "            missing_df = set(exog_X_df) - set(exog_df)\n",
    "            if missing_df:\n",
    "                raise ValueError(\n",
    "                    'The following exogenous features are present in `X_df` '\n",
    "                    f'but not in `df`: {missing_df}.'\n",
    "                )\n",
    "            missing_X_df = set(exog_df) - set(exog_X_df)\n",
    "            if missing_X_df:\n",
    "                raise ValueError(\n",
    "                    'The following exogenous features are present in `df` '\n",
    "                    f'but not in `X_df`: {missing_X_df}.'\n",
    "                )\n",
    "            if exog_df != exog_X_df:\n",
    "                # rearrange columns if necessary\n",
    "                X_df = X_df[[id_col, time_col, *x_cols]]\n",
    "            x_cols = exog_df\n",
    "        else:\n",
    "            if exog_df:\n",
    "                warnings.warn(\n",
    "                    f'`df` contains the following exogenous features: {exog_df}, '\n",
    "                    'but `X_df` was not provided. They will be ignored.'\n",
    "                )\n",
    "                df = df[[id_col, time_col, target_col]]\n",
    "            x_cols = None\n",
    "\n",
    "        logger.info('Preprocessing dataframes...')\n",
    "        processed = ufp.process_df(\n",
    "            df=df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        if X_df is not None:\n",
    "            processed_X = ufp.process_df(\n",
    "                df=X_df, id_col=id_col, time_col=time_col, target_col=None,\n",
    "            )\n",
    "            X_future = processed_X.data.T.tolist()\n",
    "        else:\n",
    "            X_future = None\n",
    "\n",
    "        model_input_size, model_horizon = self._get_model_params(freq, model)\n",
    "        restrict_input = finetune_steps == 0 and X_df is None\n",
    "        if restrict_input:\n",
    "            logger.info('Restricting input...')\n",
    "            new_input_size = _restrict_input_samples(\n",
    "                level=level,\n",
    "                input_size=model_input_size,\n",
    "                model_horizon=model_horizon,\n",
    "                h=h,\n",
    "            )\n",
    "            processed = _tail(processed, new_input_size)\n",
    "        if processed.data.shape[1] > 1:\n",
    "            X = processed.data[:, 1:].T.tolist()\n",
    "        else:\n",
    "            X = None\n",
    "        payload = {\n",
    "            'series': {\n",
    "                'y': processed.data[:, 0].tolist(),\n",
    "                'sizes': np.diff(processed.indptr).tolist(),\n",
    "                'X': X,\n",
    "                'X_future': X_future,\n",
    "            },\n",
    "            'model': model,\n",
    "            'h': h,\n",
    "            'freq': freq,\n",
    "            'clean_ex_first': clean_ex_first,\n",
    "            'level': level,\n",
    "            'finetune_steps': finetune_steps,\n",
    "            'finetune_loss': finetune_loss,\n",
    "        }\n",
    "\n",
    "        logger.info('Calling Forecast Endpoint...')\n",
    "        resp = self._make_request('/v2/forecast', payload)\n",
    "\n",
    "        # assemble result\n",
    "        out = ufp.make_future_dataframe(\n",
    "            uids=processed.uids,\n",
    "            last_times=type(processed.uids)(processed.last_times),\n",
    "            freq=freq,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        out = ufp.assign_columns(out, 'TimeGPT', resp['mean'])\n",
    "        if resp['intervals'] is not None:\n",
    "            intervals_df = type(df)(\n",
    "                {\n",
    "                    f'TimeGPT-{k}': v for k, v in resp['intervals'].items()\n",
    "                }\n",
    "            )\n",
    "            out = ufp.horizontal_concat([out, intervals_df])\n",
    "        if resp['weights_x'] is not None:\n",
    "            self.weights_x = type(df)({\n",
    "                'features': x_cols,\n",
    "                'weights': resp['weights_x'],\n",
    "            })\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e5f84-3826-49fb-9d34-a66848aba645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NIXTLA_BASE_URL'] = 'http://localhost:8000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bc225-6ae3-4305-b002-ade5abbc145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.data import generate_series\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e4819-4a57-49b9-8ef0-38497d9f8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_series(4, min_length=40, max_length=200, n_static_features=2, static_as_categorical=False)\n",
    "series['unique_id'] = series['unique_id'].astype('int64')\n",
    "valid = series.groupby('unique_id', observed=True).tail(20)\n",
    "train = series.drop(valid.index)\n",
    "train_pl = pl.from_pandas(train)\n",
    "valid_pl = pl.from_pandas(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ff47f-5bbe-4219-b90f-794acc9f9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = NixtlaClient(max_retries=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1260ad-a519-4256-ba59-0730409a5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.forecast(df=train, h=20, freq=\"D\", level=[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef60dc2-34d0-4acf-b64f-d828daf027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.forecast(df=train, X_df=valid.drop(columns='y'), h=20, freq=\"D\", level=[80, 90.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc288a9-66b5-4746-883f-158bfb71beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.forecast(df=series, h=2, freq=\"D\", level=[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd08c9-2b2c-46ec-b77a-ed5b8dbac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.forecast(df=train_pl, X_df=valid_pl.drop('y'), h=20, freq=\"1d\", level=[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad019e30-3211-47fa-a570-2ad550719f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla import NixtlaClient as NixtlaV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d90f5a-e3d5-429c-ad88-0e51c4d5b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_client = NixtlaV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98d7fb-b87f-4b64-b0b6-91d3eb2d867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_client.forecast(df=series, h=2, freq=\"D\", level=[80])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
