---
title: "Fine-tuning Tutorial TimeGPT"
description: "Adapt TimeGPT to your specific datasets for more accurate forecasts"
icon: "sliders"
---

Fine-tuning is a powerful process for utilizing TimeGPT more effectively.
Foundation models such as TimeGPT are pre-trained on vast amounts of data,
capturing wide-ranging features and patterns. These models can then be
specialized for specific contexts or domains. With fine-tuning, the model's
parameters are refined to forecast a new task, allowing it to tailor its vast
pre-existing knowledge towards the requirements of the new data. Fine-tuning
thus serves as a crucial bridge, linking TimeGPT's broad capabilities to your
tasks specificities.

Concretely, the process of fine-tuning consists of performing a certain number
of training iterations on your input data minimizing the forecasting error.
The forecasts will then be produced with the updated model. To control the
number of iterations, use the `finetune_steps` argument of the `forecast` method.

## Tutorial

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nixtla/nixtla/blob/main/nbs/docs/tutorials/06_finetuning.ipynb)

### Step 1: Import Packages and Initialize Client

First, we import the required packages and initialize the Nixtla client.

```python
import pandas as pd
from nixtla import NixtlaClient
from utilsforecast.losses import mae, mse
from utilsforecast.evaluation import evaluate
```

Next, initialize the NixtlaClient instance, providing your API key (or rely on
environment variables):

```python initialize-client
nixtla_client = NixtlaClient(
    api_key='my_api_key_provided_by_nixtla'  # Defaults to os.environ.get("NIXTLA_API_KEY")
)
```

### Step 2: Load Data

Load the dataset from the provided CSV URL:

```python load-data
df = pd.read_csv(
    "https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv"
)
df.head()
```

|       | timestamp    | value   |
| ----- | ------------ | ------- |
| 0     | 1949-01-01   | 112     |
| 1     | 1949-02-01   | 118     |
| 2     | 1949-03-01   | 132     |
| 3     | 1949-04-01   | 129     |
| 4     | 1949-05-01   | 121     |


### Step 3: Fine-tune the Model

Set the number of fine-tuning iterations with the **finetune_steps** parameter.
Here, `finetune_steps=10` means the model will go through 10 iterations of
training on your time series data.

```python
timegpt_fcst_finetune_df = nixtla_client.forecast(
    df=df,
    h=12,
    finetune_steps=10,
    time_col='timestamp',
    target_col='value',
)
```

Visualize forecasts to confirm performance:

```python
nixtla_client.plot(
    df,
    timegpt_fcst_finetune_df,
    time_col='timestamp',
    target_col='value',
)
```

<Frame>
  ![Forecast Plot](https://raw.githubusercontent.com/Nixtla/nixtla/readme_docs/nbs/_docs/docs/tutorials/06_finetuning_files/figure-markdown_strict/cell-12-output-1.png)
</Frame>

## Conclusion

Keep in mind that fine-tuning can be a bit of trial and error. You might need to
adjust the number of `finetune_steps` based on your specific needs and the
complexity of your data. Usually, a larger value of `finetune_steps` works
better for large datasets.

It's recommended to monitor the model's performance during fine-tuning and
adjust as needed. Be aware that more `finetune_steps` may lead to longer
training times and could potentially lead to overfitting if not managed properly.

Remember, fine-tuning is a powerful feature, but it should be used thoughtfully
and carefully.

## Additional Resources

- For a detailed guide on using a specific loss function for fine-tuning, check out the
[Fine-tuning with a specific loss function](/forecasting/fine-tuning/custom_loss)
tutorial.
- Also, read our detailed tutorial on [controlling the level of fine-tuning](/forecasting/fine-tuning/depth)
using `finetune_depth`.