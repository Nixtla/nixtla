# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/utils.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/utils.ipynb 3
def colab_badge(path: str):
    from IPython.display import Markdown

    base_url = "https://colab.research.google.com/github"
    badge_svg = "https://colab.research.google.com/assets/colab-badge.svg"
    nb_url = f"{base_url}/Nixtla/nixtla/blob/main/nbs/{path}.ipynb"
    badge_md = f"[![]({badge_svg})]({nb_url})"
    display(Markdown(badge_md))

# %% ../nbs/utils.ipynb 4
import sys

# %% ../nbs/utils.ipynb 5
def in_colab():
    return "google.colab" in sys.modules

# %% ../nbs/utils.ipynb 6
def _restrict_input_samples(level, input_size, model_horizon, h) -> int:
    if level is not None:
        # add sufficient info to compute
        # conformal interval
        # @AzulGarza
        #  this is an old opinionated decision
        #  about reducing the data sent to the api
        #  to reduce latency when
        #  a user passes level. since currently the model
        #  uses conformal prediction, we can change a minimum
        #  amount of data if the series are too large
        new_input_size = 3 * input_size + max(model_horizon, h)
    else:
        # we only want to forecast
        new_input_size = input_size
    return new_input_size
