{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp timegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import inspect\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nixtlats.client import Nixtla, SingleSeriesForecast\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "main_logger = logging.getLogger(__name__)\n",
    "httpx_logger = logging.getLogger('httpx')\n",
    "httpx_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "import warnings\n",
    "from itertools import product\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastcore.test import test_eq, test_fail, test_warns\n",
    "from nbdev.showdoc import show_doc\n",
    "from tqdm import TqdmExperimentalWarning\n",
    "\n",
    "load_dotenv()\n",
    "logging.getLogger('statsforecast').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "date_features_by_freq = {\n",
    "    # Daily frequencies\n",
    "    'B': ['year', 'month', 'day', 'weekday'],\n",
    "    'C': ['year', 'month', 'day', 'weekday'],\n",
    "    'D': ['year', 'month', 'day', 'weekday'],\n",
    "    # Weekly\n",
    "    'W': ['year', 'week', 'weekday'],\n",
    "    # Monthly\n",
    "    'M': ['year', 'month'],\n",
    "    'SM': ['year', 'month', 'day'],\n",
    "    'BM': ['year', 'month'],\n",
    "    'CBM': ['year', 'month'],\n",
    "    'MS': ['year', 'month'],\n",
    "    'SMS': ['year', 'month', 'day'],\n",
    "    'BMS': ['year', 'month'],\n",
    "    'CBMS': ['year', 'month'],\n",
    "    # Quarterly\n",
    "    'Q': ['year', 'quarter'],\n",
    "    'BQ': ['year', 'quarter'],\n",
    "    'QS': ['year', 'quarter'],\n",
    "    'BQS': ['year', 'quarter'],\n",
    "    # Yearly\n",
    "    'A': ['year'],\n",
    "    'Y': ['year'],\n",
    "    'BA': ['year'],\n",
    "    'BY': ['year'],\n",
    "    'AS': ['year'],\n",
    "    'YS': ['year'],\n",
    "    'BAS': ['year'],\n",
    "    'BYS': ['year'],\n",
    "    # Hourly\n",
    "    'BH': ['year', 'month', 'day', 'hour', 'weekday'],\n",
    "    'H': ['year', 'month', 'day', 'hour'],\n",
    "    # Minutely\n",
    "    'T': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    'min': ['year', 'month', 'day', 'hour', 'minute'],\n",
    "    # Secondly\n",
    "    'S': ['year', 'month', 'day', 'hour', 'minute', 'second'],\n",
    "    # Milliseconds\n",
    "    'L': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    'ms': ['year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond'],\n",
    "    # Microseconds\n",
    "    'U': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    'us': ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond'],\n",
    "    # Nanoseconds\n",
    "    'N': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _TimeGPTModel:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            client: Nixtla,\n",
    "            h: int,\n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            freq: str = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            finetune_steps: int = 0,\n",
    "            clean_ex_first: bool = True,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        ):\n",
    "        self.client = client\n",
    "        self.h = h\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self.base_freq = freq\n",
    "        self.level = level\n",
    "        self.finetune_steps = finetune_steps\n",
    "        self.clean_ex_first = clean_ex_first\n",
    "        self.date_features = date_features\n",
    "        self.date_features_to_one_hot = date_features_to_one_hot\n",
    "        # variables defined by each flow\n",
    "        self.weights_x: pd.DataFrame = None\n",
    "        self.freq: str = self.base_freq\n",
    "        self.drop_uid: bool = False\n",
    "        self.x_cols: List[str]\n",
    "        self.input_size: int\n",
    "        self.model_horizon: int\n",
    "\n",
    "\n",
    "    def transform_inputs(self, df: pd.DataFrame, X_df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        main_logger.info('Validating inputs...')\n",
    "        if self.base_freq is None and hasattr(df.index, 'freq'):\n",
    "            inferred_freq = df.index.freq\n",
    "            if inferred_freq is not None:\n",
    "                inferred_freq = inferred_freq.rule_code\n",
    "                main_logger.info(f'Inferred freq: {inferred_freq}')\n",
    "            self.freq = inferred_freq\n",
    "            time_col = df.index.name\n",
    "            if time_col is None:\n",
    "                time_col = 'ds'\n",
    "                df.index.name = time_col\n",
    "            self.time_col = time_col\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            self.freq = self.base_freq\n",
    "        renamer = {\n",
    "            self.id_col: 'unique_id',\n",
    "            self.time_col: 'ds',\n",
    "            self.target_col: 'y',\n",
    "        }\n",
    "        df = df.rename(columns=renamer)\n",
    "        if df.dtypes.ds != 'object':\n",
    "            df['ds'] = df['ds'].astype(str)\n",
    "        if 'unique_id' not in df.columns:\n",
    "            # Insert unique_id column\n",
    "            df = df.assign(unique_id='ts_0')\n",
    "            self.drop_uid = True\n",
    "        if X_df is not None:\n",
    "            X_df = X_df.rename(columns=renamer)\n",
    "            if 'unique_id' not in X_df.columns:\n",
    "                X_df = X_df.assign(unique_id='ts_0')\n",
    "            if X_df.dtypes.ds != 'object':\n",
    "                X_df['ds'] = X_df['ds'].astype(str)\n",
    "        return df, X_df\n",
    "\n",
    "    def transform_outputs(self, fcst_df: pd.DataFrame):\n",
    "        renamer = {\n",
    "            'unique_id': self.id_col,\n",
    "            'ds': self.time_col,\n",
    "            'target_col': self.target_col,\n",
    "        }\n",
    "        if self.drop_uid:\n",
    "            fcst_df = fcst_df.drop(columns='unique_id')\n",
    "        fcst_df = fcst_df.rename(columns=renamer)\n",
    "        return fcst_df\n",
    "\n",
    "    def infer_freq(self, df: pd.DataFrame):\n",
    "        # special freqs that need to be checked\n",
    "        # for example to ensure 'W'-> 'W-MON'\n",
    "        special_freqs = ['W', 'M', 'Q', 'Y', 'A']\n",
    "        if self.freq is None or self.freq in special_freqs:\n",
    "            unique_id = df.iloc[0]['unique_id']\n",
    "            df_id = df.query('unique_id == @unique_id')\n",
    "            inferred_freq = pd.infer_freq(df_id['ds'])\n",
    "            if inferred_freq is None:\n",
    "                raise Exception(\n",
    "                    'Could not infer frequency of ds column. This could be due to '\n",
    "                    'inconsistent intervals. Please check your data for missing, '\n",
    "                    'duplicated or irregular timestamps'\n",
    "                )\n",
    "            if self.freq is not None:\n",
    "                # check we have the same base frequency\n",
    "                # except when we have yearly frequency (A, and Y means the same)\n",
    "                if (self.freq != inferred_freq[0] and self.freq != 'Y') or (self.freq == 'Y' and inferred_freq[0] != 'A'):\n",
    "                    raise Exception(f'Failed to infer special date, inferred freq {inferred_freq}')\n",
    "            main_logger.info(f'Inferred freq: {inferred_freq}')\n",
    "            self.freq = inferred_freq\n",
    "\n",
    "    def resample_dataframe(self, df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        df['ds'] = pd.to_datetime(df['ds'])\n",
    "        resampled_df = df.set_index('ds').groupby('unique_id').resample(self.freq).bfill()\n",
    "        resampled_df = resampled_df.drop(columns='unique_id').reset_index()\n",
    "        resampled_df['ds'] = resampled_df['ds'].astype(str)\n",
    "        return resampled_df\n",
    "\n",
    "    def make_future_dataframe(self, df: pd.DataFrame, reconvert: bool = True):\n",
    "        last_dates = df.groupby('unique_id')['ds'].max()\n",
    "        def _future_date_range(last_date):\n",
    "            future_dates = pd.date_range(last_date, freq=self.freq, periods=self.h + 1)\n",
    "            future_dates = future_dates[-self.h:]\n",
    "            return future_dates\n",
    "        future_df = last_dates.apply(_future_date_range).reset_index()\n",
    "        future_df = future_df.explode('ds').reset_index(drop=True)\n",
    "        if reconvert and df.dtypes['ds'] == 'object':\n",
    "            # avoid date 000\n",
    "            future_df['ds'] = future_df['ds'].astype(str)\n",
    "        return future_df\n",
    "\n",
    "    def compute_date_feature(self, dates, feature):\n",
    "        if callable(feature):\n",
    "            feat_name = feature.__name__\n",
    "            feat_vals = feature(dates)\n",
    "        else:\n",
    "            feat_name = feature\n",
    "            if feature in (\"week\", \"weekofyear\"):\n",
    "                dates = dates.isocalendar()\n",
    "            feat_vals = getattr(dates, feature)\n",
    "        if not isinstance(feat_vals, pd.DataFrame):\n",
    "            vals = np.asarray(feat_vals)\n",
    "            feat_vals = pd.DataFrame({feat_name: vals})\n",
    "        feat_vals['ds'] = dates\n",
    "        return feat_vals\n",
    "\n",
    "    def add_date_features( \n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            X_df: Optional[pd.DataFrame],\n",
    "        ):\n",
    "        # df contains exogenous variables\n",
    "        # X_df are the future values of the exogenous variables\n",
    "        # construct dates\n",
    "        train_dates = df['ds'].unique().tolist()\n",
    "        # if we dont have future exogenos variables\n",
    "        # we need to compute the future dates\n",
    "        if (self.h is not None) and X_df is None:\n",
    "            X_df = self.make_future_dataframe(df=df)\n",
    "            future_dates = X_df['ds'].unique().tolist()\n",
    "        elif X_df is not None:\n",
    "            future_dates = X_df['ds'].unique().tolist()\n",
    "        else:\n",
    "            future_dates = []\n",
    "        dates = pd.DatetimeIndex(train_dates + future_dates)\n",
    "        date_features_df = pd.DataFrame({'ds': dates})\n",
    "        for feature in self.date_features:\n",
    "            feat_df = self.compute_date_feature(dates, feature)\n",
    "            date_features_df = date_features_df.merge(feat_df, on=['ds'], how='left')\n",
    "        if df.dtypes['ds'] == 'object':\n",
    "            date_features_df['ds'] = date_features_df['ds'].astype(str)\n",
    "        if self.date_features_to_one_hot is not None:\n",
    "            date_features_df = pd.get_dummies(\n",
    "                date_features_df, \n",
    "                columns=self.date_features_to_one_hot, \n",
    "                dtype=int,\n",
    "            )\n",
    "        # remove duplicated columns if any\n",
    "        date_features_df = date_features_df.drop(\n",
    "            columns=[col for col in date_features_df.columns if col in df.columns and col not in ['unique_id', 'ds']]\n",
    "        )\n",
    "        # add date features to df\n",
    "        df = df.merge(date_features_df, on='ds', how='left')\n",
    "        # add date features to X_df\n",
    "        if X_df is not None:\n",
    "            X_df = X_df.merge(date_features_df, on='ds', how='left')\n",
    "        return df, X_df\n",
    "\n",
    "    def preprocess_X_df(self, X_df: pd.DataFrame):\n",
    "        if X_df.isna().any().any():\n",
    "            raise Exception('Some of your exogenous variables contain NA, please check')\n",
    "        X_df = X_df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "        X_df = self.resample_dataframe(X_df)\n",
    "        return X_df\n",
    "\n",
    "    def preprocess_dataframes(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            X_df: Optional[pd.DataFrame],\n",
    "        ):\n",
    "        self.infer_freq(df=df)\n",
    "        \"\"\"Returns Y_df and X_df dataframes in the structure expected by the endpoints.\"\"\"\n",
    "\n",
    "        y_cols = ['unique_id', 'ds', 'y']\n",
    "        Y_df = df[y_cols]\n",
    "        if Y_df['y'].isna().any():\n",
    "            raise Exception('Your target variable contains NA, please check')\n",
    "        # Azul: efficient this code\n",
    "        # and think about returning dates that are not in the training set\n",
    "        Y_df = self.resample_dataframe(Y_df)\n",
    "        x_cols = []\n",
    "        if X_df is not None:\n",
    "            x_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()\n",
    "            if not all(col in df.columns for col in x_cols):\n",
    "                raise Exception(\n",
    "                    'You must include the exogenous variables in the `df` object, '\n",
    "                    f'exogenous variables {\",\".join(x_cols)}'\n",
    "                )\n",
    "            if (self.h is not None) and (len(X_df) != df['unique_id'].nunique() * self.h):\n",
    "                raise Exception(\n",
    "                    f'You have to pass the {self.h} future values of your '\n",
    "                    'exogenous variables for each time series'\n",
    "                )\n",
    "            X_df_history = df[['unique_id', 'ds'] + x_cols]\n",
    "            X_df = pd.concat([X_df_history, X_df])\n",
    "            X_df = self.preprocess_X_df(X_df)\n",
    "        elif (X_df is None) and (self.h is None) and (len(y_cols) < df.shape[1]):\n",
    "            # case for just insample, \n",
    "            # we dont need h\n",
    "            X_df = df.drop(columns='y')\n",
    "            x_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()\n",
    "            X_df = self.preprocess_X_df(X_df)\n",
    "        self.x_cols = x_cols\n",
    "        return Y_df, X_df\n",
    "\n",
    "    def dataframes_to_dict(self, Y_df: pd.DataFrame, X_df: pd.DataFrame):\n",
    "        to_dict_args = {'orient': 'split'}\n",
    "        if 'index' in inspect.signature(pd.DataFrame.to_dict).parameters:\n",
    "            to_dict_args['index'] = False\n",
    "        y = Y_df.to_dict(**to_dict_args)\n",
    "        x = X_df.to_dict(**to_dict_args) if X_df is not None else None\n",
    "        return y, x\n",
    "    \n",
    "    def set_model_params(self):\n",
    "        model_params = self.client.timegpt_model_params(request=SingleSeriesForecast(freq=self.freq))\n",
    "        if 'data' in model_params:\n",
    "            model_params = model_params['data']\n",
    "        model_params = model_params['detail']\n",
    "        self.input_size, self.model_horizon = model_params['input_size'], model_params['horizon']\n",
    "        if self.h > self.model_horizon:\n",
    "            main_logger.warning(\n",
    "                'The specified horizon \"h\" exceeds the model horizon. '\n",
    "                'This may lead to less accurate forecasts. '\n",
    "                'Please consider using a smaller horizon.'\n",
    "            )\n",
    "    \n",
    "    def validate_input_size(self, Y_df: pd.DataFrame):\n",
    "        min_history = Y_df.groupby('unique_id').size().min()\n",
    "        if min_history < self.input_size + self.model_horizon:\n",
    "            raise Exception(\n",
    "                'Your time series data is too short '\n",
    "                'Please be sure that your unique time series contain '\n",
    "                f'at least {self.input_size + self.model_horizon} observations'\n",
    "            )\n",
    "\n",
    "    def forecast(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        X_df: Optional[pd.DataFrame] = None,\n",
    "        add_history: bool = False,\n",
    "    ):\n",
    "        df, X_df = self.transform_inputs(df=df, X_df=X_df)\n",
    "        main_logger.info('Preprocessing dataframes...')\n",
    "        Y_df, X_df = self.preprocess_dataframes(df=df, X_df=X_df)\n",
    "        self.set_model_params()\n",
    "        # restrict input if\n",
    "        # - we dont want to finetune\n",
    "        # - we dont have exogenous regegressors\n",
    "        # - and we dont want to produce pred intervals\n",
    "        # - no add history\n",
    "        restrict_input = self.finetune_steps == 0 and X_df is None and self.level is not None and not add_history\n",
    "        if restrict_input:\n",
    "            # add sufficient info to compute\n",
    "            # conformal interval\n",
    "            new_input_size = 3 * self.input_size + max(self.model_horizon, self.h)\n",
    "            Y_df = Y_df.groupby('unique_id').tail(new_input_size)\n",
    "            if X_df is not None:\n",
    "                X_df = X_df.groupby('unique_id').tail(new_input_size + self.h) # history plus exogenous\n",
    "        if self.finetune_steps > 0 or self.level is not None:\n",
    "            self.validate_input_size(Y_df=Y_df)\n",
    "        y, x = self.dataframes_to_dict(Y_df, X_df)\n",
    "        print(y, x)\n",
    "        print(self.x_cols)\n",
    "        main_logger.info('Calling Forecast Endpoint...')\n",
    "        response_timegpt = self.client.timegpt_multi_series(\n",
    "            y=y,\n",
    "            x=x,\n",
    "            fh=self.h,\n",
    "            freq=self.freq,\n",
    "            level=self.level,\n",
    "            finetune_steps=self.finetune_steps,\n",
    "            clean_ex_first=self.clean_ex_first,\n",
    "        )\n",
    "        if 'data' in response_timegpt:\n",
    "            response_timegpt = response_timegpt['data']\n",
    "        if 'weights_x' in response_timegpt:\n",
    "            print(response_timegpt['weights_x'])\n",
    "            self.weights_x = pd.DataFrame({\n",
    "                'features': self.x_cols,\n",
    "                'weights': response_timegpt['weights_x'],\n",
    "            })\n",
    "        fcst_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "        if add_history:\n",
    "            main_logger.info('Calling Historical Forecast Endpoint...')\n",
    "            self.validate_input_size(Y_df=Y_df)\n",
    "            if 'data' in response_timegpt:\n",
    "                response_timegpt = response_timegpt['data']\n",
    "            response_timegpt = self.client.timegpt_multi_series_historic(\n",
    "                y=y,\n",
    "                x=x,\n",
    "                freq=self.freq,\n",
    "                level=self.level,\n",
    "                clean_ex_first=self.clean_ex_first,\n",
    "            )\n",
    "            fitted_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "            fitted_df = fitted_df.drop(columns='y')\n",
    "            fcst_df = pd.concat([fitted_df, fcst_df]).sort_values(['unique_id', 'ds'])\n",
    "        fcst_df = self.transform_outputs(fcst_df)\n",
    "        return fcst_df\n",
    "\n",
    "    def detect_anomalies(self, df: pd.DataFrame):\n",
    "        # Azul\n",
    "        # Remember the input X_df is the FUTURE ex vars\n",
    "        # there is a misleading notation here\n",
    "        # because X_df inputs in the following methods\n",
    "        # returns X_df outputs that means something different\n",
    "        # ie X_df = [X_df_history, X_df]\n",
    "        # exogenous variables are passed after df \n",
    "        df, _ = self.transform_inputs(df=df, X_df=None)\n",
    "        main_logger.info('Preprocessing dataframes...')\n",
    "        Y_df, X_df = self.preprocess_dataframes(df=df, X_df=None)\n",
    "        main_logger.info('Calling Anomaly Detector Endpoint...')\n",
    "        y, x = self.dataframes_to_dict(Y_df, X_df)\n",
    "        response_timegpt = self.client.timegpt_multi_series_anomalies(\n",
    "            y=y,\n",
    "            x=x,\n",
    "            freq=self.freq,\n",
    "            level=[self.level] if (isinstance(self.level, int) or isinstance(self.level, float)) else [self.level[0]],\n",
    "            clean_ex_first=self.clean_ex_first,\n",
    "        )\n",
    "        if 'data' in response_timegpt:\n",
    "            response_timegpt = response_timegpt['data']\n",
    "        if 'weights_x' in response_timegpt:\n",
    "            self.weights_x = pd.DataFrame({\n",
    "                'features': self.x_cols,\n",
    "                'weights': response_timegpt['weights_x'],\n",
    "            })\n",
    "        anomalies_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "        anomalies_df = anomalies_df.drop(columns='y')\n",
    "        anomalies_df = self.transform_outputs(anomalies_df)\n",
    "        return anomalies_df\n",
    "\n",
    "    def cross_validation(self, df: pd.DataFrame, n_windows: int, step_size: int):\n",
    "        # Azul\n",
    "        # Remember the input X_df is the FUTURE ex vars\n",
    "        # there is a misleading notation here\n",
    "        # because X_df inputs in the following methods\n",
    "        # returns X_df outputs that means something different\n",
    "        # ie X_df = [X_df_history, X_df]\n",
    "        # exogenous variables are passed after df \n",
    "        df, _ = self.transform_inputs(df=df, X_df=None)\n",
    "        main_logger.info('Preprocessing dataframes...')\n",
    "        Y_df, X_df = self.preprocess_dataframes(df=df, X_df=None)\n",
    "        main_logger.info('Calling Cross Validation Endpoint...')\n",
    "        y, x = self.dataframes_to_dict(Y_df, X_df)\n",
    "        print(y, x)\n",
    "        print(self.x_cols)\n",
    "        response_timegpt = self.client.timegpt_multi_series_cross_validation(\n",
    "            y=y,\n",
    "            x=x,\n",
    "            fh=self.h,\n",
    "            freq=self.freq,\n",
    "            level=self.level,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,\n",
    "            finetune_steps=self.finetune_steps,\n",
    "            clean_ex_first=self.clean_ex_first,\n",
    "        )\n",
    "        if 'data' in response_timegpt:\n",
    "            response_timegpt = response_timegpt['data']\n",
    "        if 'weights_x' in response_timegpt:\n",
    "            self.weights_x = pd.DataFrame({\n",
    "                'features': self.x_cols,\n",
    "                'weights': response_timegpt['weights_x'],\n",
    "            })\n",
    "        cv_df = pd.DataFrame(**response_timegpt['forecast'])\n",
    "        cv_df = self.transform_outputs(cv_df)\n",
    "        return cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _TimeGPT:\n",
    "    \"\"\"\n",
    "    A class used to interact with the TimeGPT API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token: str, environment: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the TimeGPT object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token : str\n",
    "            The authorization token to interact with the TimeGPT API.\n",
    "        environment : str\n",
    "            Custom environment. Pass only if provided.\n",
    "        \"\"\"\n",
    "        if environment is None:\n",
    "            environment = \"https://dashboard.nixtla.io/api\"\n",
    "        self.client = Nixtla(base_url=environment, token=token)\n",
    "        self.weights_x: pd.DataFrame = None\n",
    "\n",
    "    def validate_token(self, log: bool = True) -> bool:\n",
    "        \"\"\"Returns True if your token is valid.\"\"\"\n",
    "        validation = self.client.validate_token()\n",
    "        valid = False\n",
    "        if 'message' in validation:\n",
    "            if validation['message'] == 'success':\n",
    "                valid = True\n",
    "        elif 'detail' in validation:\n",
    "            if 'Forecasting! :)' in validation['detail']:\n",
    "                valid = True\n",
    "        if 'support' in validation and log:\n",
    "            main_logger.info(f'Happy Forecasting! :), {validation[\"support\"]}')\n",
    "        return valid\n",
    "        \n",
    "    def _forecast(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            h: int,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            X_df: Optional[pd.DataFrame] = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            finetune_steps: int = 0,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            add_history: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_token : bool (default=False)\n",
    "            If True, validates token before \n",
    "            sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        if validate_token and not self.validate_token(log=False):\n",
    "            raise Exception(\n",
    "                'Token not valid, please email ops@nixtla.io'\n",
    "            )\n",
    "        self.weights_x = None\n",
    "        model = _TimeGPTModel(\n",
    "            client=self.client,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            freq=freq,\n",
    "            level=level,\n",
    "            finetune_steps=finetune_steps,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "        )\n",
    "        fcst_df = model.forecast(df=df, X_df=X_df, add_history=add_history)\n",
    "        self.weights_x = model.weights_x\n",
    "        return fcst_df\n",
    "\n",
    "    def _detect_anomalies(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            level: Union[int, float] = 99,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        ):\n",
    "        \"\"\"Detect anomalies in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_token : bool (default=False)\n",
    "            If True, validates token before \n",
    "            sending requests.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        anomalies_df : pandas.DataFrame\n",
    "            DataFrame with anomalies flagged with 1 detected by TimeGPT.\n",
    "        \"\"\"\n",
    "        if validate_token and not self.validate_token(log=False):\n",
    "            raise Exception(\n",
    "                'Token not valid, please email ops@nixtla.io'\n",
    "            )\n",
    "        self.weights_x = None\n",
    "        model = _TimeGPTModel(\n",
    "            client=self.client,\n",
    "            h=None,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            freq=freq,\n",
    "            level=level,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "        )\n",
    "        anomalies_df = model.detect_anomalies(df=df)\n",
    "        self.weights_x = model.weights_x\n",
    "        return anomalies_df\n",
    "\n",
    "    def _cross_validation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        h: int,\n",
    "        freq: Optional[str] = None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        validate_token: bool = False,\n",
    "        n_windows: int = 1,\n",
    "        step_size: Optional[int] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        clean_ex_first: bool = True,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "    ):\n",
    "        \"\"\"Perform cross validation in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        validate_token : bool (default=False)\n",
    "            If True, validates token before\n",
    "            sending requests.\n",
    "        n_windows : int (defaul=1)\n",
    "            Number of windows to evaluate.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cv_df : pandas.DataFrame\n",
    "            DataFrame with anomalies flagged with 1 detected by TimeGPT.\n",
    "        \"\"\"\n",
    "        if validate_token and not self.validate_token(log=False):\n",
    "            raise Exception(\n",
    "                'Token not valid, please email ops@nixtla.io'\n",
    "            )\n",
    "        self.weights_x = None\n",
    "        model = _TimeGPTModel(\n",
    "            client=self.client,\n",
    "            h=h,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            freq=freq,\n",
    "            level=level,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "        )\n",
    "        cv_df = model.cross_validation(df=df, n_windows=n_windows, step_size=step_size)\n",
    "        self.weights_x = model.weights_x\n",
    "        return cv_df\n",
    "\n",
    "    def plot(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            forecasts_df: Optional[pd.DataFrame] = None,\n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            unique_ids: Union[Optional[List[str]], np.ndarray] = None,\n",
    "            plot_random: bool = True,\n",
    "            models: Optional[List[str]] = None,\n",
    "            level: Optional[List[float]] = None,\n",
    "            max_insample_length: Optional[int] = None,\n",
    "            plot_anomalies: bool = False,\n",
    "            engine: str = 'matplotlib',\n",
    "            resampler_kwargs: Optional[Dict] = None,\n",
    "        ):\n",
    "        \"\"\"Plot forecasts and insample values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        forecasts_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with columns [`unique_id`, `ds`] and models.\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        unique_ids : List[str], optional (default=None)\n",
    "            Time Series to plot.\n",
    "            If None, time series are selected randomly.\n",
    "        plot_random : bool (default=True)\n",
    "            Select time series to plot randomly.\n",
    "        models : List[str], optional (default=None)\n",
    "            List of models to plot.\n",
    "        level : List[float], optional (default=None)\n",
    "            List of prediction intervals to plot if paseed.\n",
    "        max_insample_length : int, optional (default=None)\n",
    "            Max number of train/insample observations to be plotted.\n",
    "        plot_anomalies : bool (default=False)\n",
    "            Plot anomalies for each prediction interval.\n",
    "        engine : str (default='plotly')\n",
    "            Library used to plot. 'plotly', 'plotly-resampler' or 'matplotlib'.\n",
    "        resampler_kwargs : dict\n",
    "            Kwargs to be passed to plotly-resampler constructor.\n",
    "            For further custumization (\"show_dash\") call the method,\n",
    "            store the plotting object and add the extra arguments to\n",
    "            its `show_dash` method.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from utilsforecast.plotting import plot_series\n",
    "        except ModuleNotFoundError:\n",
    "            raise Exception(\n",
    "                'You have to install additional dependencies to use this method, '\n",
    "                'please install them using `pip install \"nixtlats[plotting]\"`'\n",
    "            )\n",
    "        df = df.copy()\n",
    "        if id_col not in df:\n",
    "            df[id_col] = 'ts_0'\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "        if forecasts_df is not None:\n",
    "            forecasts_df = forecasts_df.copy()\n",
    "            if id_col not in forecasts_df:\n",
    "                forecasts_df[id_col] = 'ts_0'\n",
    "            forecasts_df[time_col] = pd.to_datetime(forecasts_df[time_col])\n",
    "            if 'anomaly' in forecasts_df:\n",
    "                # special case to plot outputs\n",
    "                # from detect_anomalies\n",
    "                forecasts_df = forecasts_df.drop(columns='anomaly')\n",
    "                cols = forecasts_df.columns\n",
    "                cols = cols[cols.str.contains('TimeGPT-lo-')]\n",
    "                level = cols.str.replace('TimeGPT-lo-', '')[0]\n",
    "                level = float(level) if '.' in level else int(level)\n",
    "                level = [level]\n",
    "                plot_anomalies = True\n",
    "                models = ['TimeGPT']\n",
    "                forecasts_df = df.merge(forecasts_df, how='left')\n",
    "                df = df.groupby('unique_id').head(1)\n",
    "                # prevent double plotting\n",
    "                df.loc[:, target_col] = np.nan\n",
    "        return plot_series(\n",
    "            df=df,\n",
    "            forecasts_df=forecasts_df,\n",
    "            ids=unique_ids,\n",
    "            plot_random=plot_random,\n",
    "            models=models,\n",
    "            level=level,\n",
    "            max_insample_length=max_insample_length,\n",
    "            plot_anomalies=plot_anomalies,\n",
    "            engine=engine,\n",
    "            resampler_kwargs=resampler_kwargs,\n",
    "            palette=\"tab20b\",\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TimeGPT(_TimeGPT):\n",
    "    \n",
    "    def forecast(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            h: int,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            X_df: Optional[pd.DataFrame] = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            finetune_steps: int = 0,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            add_history: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ):\n",
    "        \"\"\"Forecast your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        X_df : pandas.DataFrame, optional (default=None)\n",
    "            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n",
    "        level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_token : bool (default=False)\n",
    "            If True, validates token before \n",
    "            sending requests.\n",
    "        add_history : bool (default=False)\n",
    "            Return fitted values of the model.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        num_partitions : int (default=None)\n",
    "            Number of partitions to use.\n",
    "            Only used in distributed environments (spark, ray, dask).\n",
    "            If None, the number of partitions will be equal\n",
    "            to the available parallel resources.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame\n",
    "            DataFrame with TimeGPT forecasts for point predictions and probabilistic\n",
    "            predictions (if level is not None).\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._forecast(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                finetune_steps=finetune_steps,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_token=validate_token,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "            )\n",
    "        else:\n",
    "            from nixtlats.distributed.timegpt import _DistributedTimeGPT\n",
    "            return _DistributedTimeGPT().forecast(\n",
    "                token=self.client._client_wrapper._token,\n",
    "                environment=self.client._client_wrapper._base_url,\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                finetune_steps=finetune_steps,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_token=validate_token,\n",
    "                add_history=add_history,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "            \n",
    "    def detect_anomalies(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            level: Union[int, float] = 99,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "        ):\n",
    "        \"\"\"Detect anomalies in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we \n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        validate_token : bool (default=False)\n",
    "            If True, validates token before \n",
    "            sending requests.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        anomalies_df : pandas.DataFrame\n",
    "            DataFrame with anomalies flagged with 1 detected by TimeGPT.\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._detect_anomalies(\n",
    "                df=df,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_token=validate_token,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "            )\n",
    "        else:\n",
    "            from nixtlats.distributed.timegpt import _DistributedTimeGPT\n",
    "            return _DistributedTimeGPT().detect_anomalies(\n",
    "                token=self.client._client_wrapper._token,\n",
    "                environment=self.client._environment,\n",
    "                df=df,\n",
    "                freq=freq,    \n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                validate_token=validate_token,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "                num_partitions=num_partitions,\n",
    "            )\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        h: int,\n",
    "        freq: Optional[str] = None,\n",
    "        id_col: str = \"unique_id\",\n",
    "        time_col: str = \"ds\",\n",
    "        target_col: str = \"y\",\n",
    "        level: Optional[List[Union[int, float]]] = None,\n",
    "        validate_token: bool = False,\n",
    "        n_windows: int = 1,\n",
    "        step_size: Optional[int] = None,\n",
    "        finetune_steps: int = 0,\n",
    "        clean_ex_first: bool = True,\n",
    "        date_features: Union[bool, List[str]] = False,\n",
    "        date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "    ):\n",
    "        \"\"\"Perform cross validation in your time series using TimeGPT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The DataFrame on which the function will operate. Expected to contain at least the following columns:\n",
    "            - time_col:\n",
    "                Column name in `df` that contains the time indices of the time series. This is typically a datetime\n",
    "                column with regular intervals, e.g., hourly, daily, monthly data points.\n",
    "            - target_col:\n",
    "                Column name in `df` that contains the target variable of the time series, i.e., the variable we\n",
    "                wish to predict or analyze.\n",
    "            Additionally, you can pass multiple time series (stacked in the dataframe) considering an additional column:\n",
    "            - id_col:\n",
    "                Column name in `df` that identifies unique time series. Each unique value in this column\n",
    "                corresponds to a unique time series.\n",
    "        h : int\n",
    "            Forecast horizon.\n",
    "        freq : str\n",
    "            Frequency of the data. By default, the freq will be inferred automatically.\n",
    "            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).\n",
    "        id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\n",
    "        time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\n",
    "        target_col : str (default='y')\n",
    "            Column that contains the target.\n",
    "        level : float (default=99)\n",
    "            Confidence level between 0 and 100 for detecting the anomalies.\n",
    "        validate_token : bool (default=False)\n",
    "            If True, validates token before\n",
    "            sending requests.\n",
    "        n_windows : int (defaul=1)\n",
    "            Number of windows to evaluate.\n",
    "        step_size : int, optional (default=None)\n",
    "            Step size between each cross validation window. If None it will be equal to `h`.\n",
    "        finetune_steps : int (default=0)\n",
    "            Number of steps used to finetune TimeGPT in the\n",
    "            new data.\n",
    "        clean_ex_first : bool (default=True)\n",
    "            Clean exogenous signal before making forecasts\n",
    "            using TimeGPT.\n",
    "        date_features : bool or list of str or callable, optional (default=False)\n",
    "            Features computed from the dates. \n",
    "            Can be pandas date attributes or functions that will take the dates as input.\n",
    "            If True automatically adds most used date features for the \n",
    "            frequency of `df`.\n",
    "        date_features_to_one_hot : bool or list of str (default=True)\n",
    "            Apply one-hot encoding to these date features.\n",
    "            If `date_features=True`, then all date features are\n",
    "            one-hot encoded by default.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cv_df : pandas.DataFrame\n",
    "            DataFrame with cross validation fitted values.\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._cross_validation(\n",
    "                df=df,\n",
    "                h=h,\n",
    "                freq=freq,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,\n",
    "                level=level,\n",
    "                validate_token=validate_token,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                finetune_steps=finetune_steps,\n",
    "                clean_ex_first=clean_ex_first,\n",
    "                date_features=date_features,\n",
    "                date_features_to_one_hot=date_features_to_one_hot,\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Only pandas supported. \"\n",
    "                \"Please raise an issue on Github or mail to ops@nixtla.io. \"\n",
    "                \"Or ask Az.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeGPT\n",
       "\n",
       ">      TimeGPT (token:str, environment:Optional[str]=None)\n",
       "\n",
       "Constructs all the necessary attributes for the TimeGPT object.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| token | str |  | The authorization token to interact with the TimeGPT API. |\n",
       "| environment | Optional | None | Custom environment. Pass only if provided. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeGPT\n",
       "\n",
       ">      TimeGPT (token:str, environment:Optional[str]=None)\n",
       "\n",
       "Constructs all the necessary attributes for the TimeGPT object.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| token | str |  | The authorization token to interact with the TimeGPT API. |\n",
       "| environment | Optional | None | Custom environment. Pass only if provided. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeGPT.__init__, title_level=3, name='TimeGPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "timegpt = TimeGPT(token=os.environ['TIMEGPT_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "## TimeGPT.validate_token\n",
       "\n",
       ">      TimeGPT.validate_token (log:bool=True)\n",
       "\n",
       "Returns True if your token is valid."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "## TimeGPT.validate_token\n",
       "\n",
       ">      TimeGPT.validate_token (log:bool=True)\n",
       "\n",
       "Returns True if your token is valid."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeGPT.validate_token, title_level=2, name='TimeGPT.validate_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Happy Forecasting! :), If you have questions or need support, please email ops@nixtla.io\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "timegpt.validate_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "_timegpt = TimeGPT(os.environ['TIMEGPT_CUSTOM_URL_TOKEN'], os.environ['TIMEGPT_CUSTOM_URL'])\n",
    "_timegpt.validate_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    lambda: TimeGPT(token='transphobic').forecast(df=pd.DataFrame(), h=None, validate_token=True),\n",
    "    contains='nixtla'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test input_size\n",
    "test_eq(\n",
    "    timegpt.client.timegpt_model_params(request=SingleSeriesForecast(freq='D'))['data']['detail'],\n",
    "    {'input_size': 28, 'horizon': 7},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to make forecasts! Let's import an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02-01</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03-01</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04-01</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05-01</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  value\n",
       "0  1949-01-01    112\n",
       "1  1949-02-01    118\n",
       "2  1949-03-01    132\n",
       "3  1949-04-01    129\n",
       "4  1949-05-01    121"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Inferred freq: MS\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Inferred freq: MS\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Calling Anomaly Detector Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: MS\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: MS\n",
      "INFO:__main__:Calling Anomaly Detector Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Inferred freq: A-DEC\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: A-DEC\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Inferred freq: W-MON\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: W-MON\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Inferred freq: Q-DEC\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: Q-DEC\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Inferred freq: H\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Calling Forecast Endpoint...\n",
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: H\n",
      "INFO:__main__:Calling Forecast Endpoint...\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# test pass dataframe with index\n",
    "df_ds_index = df.set_index('timestamp')\n",
    "df_ds_index.index = pd.DatetimeIndex(df_ds_index.index, freq='MS')\n",
    "fcst_inferred_df_index = timegpt.forecast(df_ds_index, h=10, time_col='timestamp', target_col='value')\n",
    "anom_inferred_df_index = timegpt.detect_anomalies(df_ds_index, time_col='timestamp', target_col='value')\n",
    "fcst_inferred_df = timegpt.forecast(df, h=10, time_col='timestamp', target_col='value')\n",
    "anom_inferred_df = timegpt.detect_anomalies(df, time_col='timestamp', target_col='value')\n",
    "pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df)\n",
    "pd.testing.assert_frame_equal(anom_inferred_df_index, anom_inferred_df)\n",
    "for freq in ['Y', 'W-MON', 'Q-DEC', 'H']:\n",
    "    df_ds_index.index = pd.date_range(end='2023-01-01', periods=len(df), freq=freq)\n",
    "    df_ds_index.index.name = 'timestamp'\n",
    "    fcst_inferred_df_index = timegpt.forecast(df_ds_index, h=10, time_col='timestamp', target_col='value')\n",
    "    df_test = df_ds_index.reset_index()\n",
    "    fcst_inferred_df = timegpt.forecast(df_test, h=10, time_col='timestamp', target_col='value')\n",
    "    pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeGPT.plot, name='TimeGPT.plot', title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "timegpt.plot(df, time_col='timestamp', target_col='value', engine='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeGPT.forecast, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test infer freq\n",
    "input_freqs = ['W', 'W', 'W', 'Q', 'Q', 'Q', 'M', 'M', 'Y', 'Y', 'Y']\n",
    "expected_freqs = ['W-MON', 'W-TUE', 'W-WED', 'Q-DEC', 'QS-OCT', 'QS-DEC', 'MS', 'M', 'AS-JAN', 'A-DEC', 'AS-DEC']\n",
    "for input_freq, output_freq in zip(input_freqs, expected_freqs):\n",
    "    df_freq = pd.DataFrame({\n",
    "        'unique_id': 'test_ts',\n",
    "        'ds': pd.date_range('2021-01-01', periods=10, freq=output_freq),\n",
    "    })\n",
    "    model = _TimeGPTModel(client=timegpt.client, h=None, freq=input_freq)\n",
    "    model.infer_freq(df_freq)\n",
    "    test_eq(output_freq, model.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test make future dataframe for one series\n",
    "df_ = df.rename(columns={'timestamp': 'ds', 'value': 'y'})\n",
    "df_.insert(0, 'unique_id', 'AirPassengers')\n",
    "df_actual_future = df_.tail(12)[['unique_id', 'ds']]\n",
    "df_history = df_.drop(df_actual_future.index)\n",
    "df_future = _TimeGPTModel(client=timegpt.client, h=12, freq='MS').make_future_dataframe(df_history)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_actual_future.reset_index(drop=True),\n",
    "    df_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features\n",
    "date_features = ['year', 'month']\n",
    "df_date_features, future_df = _TimeGPTModel(\n",
    "    client=timegpt.client,\n",
    "    h=12, \n",
    "    freq='MS', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=None,\n",
    ").add_date_features(df_,  X_df=None)\n",
    "assert all(col in df_date_features for col in date_features)\n",
    "assert all(col in future_df for col in date_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nixtlats.date_features import SpecialDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add callables\n",
    "date_features = [SpecialDates({'first_dates': ['2021-01-1'], 'second_dates': ['2021-01-01']})]\n",
    "df_daily = df_.copy()\n",
    "df_daily['ds'] = pd.date_range(end='2021-01-01', periods=len(df_daily))\n",
    "df_date_features, future_df = _TimeGPTModel(\n",
    "    client=timegpt.client,\n",
    "    h=12, \n",
    "    freq='D', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=None,\n",
    ").add_date_features(df_,  X_df=None)\n",
    "assert all(col in df_date_features for col in ['first_dates', 'second_dates'])\n",
    "assert all(col in future_df for col in ['first_dates', 'second_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features one hot encoded\n",
    "date_features = ['year', 'month']\n",
    "date_features_to_one_hot = ['month']\n",
    "df_date_features, future_df = _TimeGPTModel(\n",
    "    client=timegpt.client,\n",
    "    h=12, \n",
    "    freq='D', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=date_features_to_one_hot,\n",
    ").add_date_features(df_,  X_df=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test future dataframe for multiple series\n",
    "df_ = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-with-ex-vars.csv')\n",
    "df_actual_future = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short-future-ex-vars.csv')\n",
    "df_future = _TimeGPTModel(\n",
    "        client=timegpt.client, \n",
    "        h=24, \n",
    "        freq='H',\n",
    "    ).make_future_dataframe(df_[['unique_id', 'ds', 'y']])\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_actual_future[['unique_id', 'ds']],\n",
    "    df_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeGPT.cross_validation, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: MS\n",
      "INFO:__main__:Calling Cross Validation Endpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], 'columns': ['unique_id', 'ds', 'y'], 'data': [['AirPassengers', '1949-01-01', 112], ['AirPassengers', '1949-02-01', 118], ['AirPassengers', '1949-03-01', 132], ['AirPassengers', '1949-04-01', 129], ['AirPassengers', '1949-05-01', 121], ['AirPassengers', '1949-06-01', 135], ['AirPassengers', '1949-07-01', 148], ['AirPassengers', '1949-08-01', 148], ['AirPassengers', '1949-09-01', 136], ['AirPassengers', '1949-10-01', 119], ['AirPassengers', '1949-11-01', 104], ['AirPassengers', '1949-12-01', 118], ['AirPassengers', '1950-01-01', 115], ['AirPassengers', '1950-02-01', 126], ['AirPassengers', '1950-03-01', 141], ['AirPassengers', '1950-04-01', 135], ['AirPassengers', '1950-05-01', 125], ['AirPassengers', '1950-06-01', 149], ['AirPassengers', '1950-07-01', 170], ['AirPassengers', '1950-08-01', 170], ['AirPassengers', '1950-09-01', 158], ['AirPassengers', '1950-10-01', 133], ['AirPassengers', '1950-11-01', 114], ['AirPassengers', '1950-12-01', 140], ['AirPassengers', '1951-01-01', 145], ['AirPassengers', '1951-02-01', 150], ['AirPassengers', '1951-03-01', 178], ['AirPassengers', '1951-04-01', 163], ['AirPassengers', '1951-05-01', 172], ['AirPassengers', '1951-06-01', 178], ['AirPassengers', '1951-07-01', 199], ['AirPassengers', '1951-08-01', 199], ['AirPassengers', '1951-09-01', 184], ['AirPassengers', '1951-10-01', 162], ['AirPassengers', '1951-11-01', 146], ['AirPassengers', '1951-12-01', 166], ['AirPassengers', '1952-01-01', 171], ['AirPassengers', '1952-02-01', 180], ['AirPassengers', '1952-03-01', 193], ['AirPassengers', '1952-04-01', 181], ['AirPassengers', '1952-05-01', 183], ['AirPassengers', '1952-06-01', 218], ['AirPassengers', '1952-07-01', 230], ['AirPassengers', '1952-08-01', 242], ['AirPassengers', '1952-09-01', 209], ['AirPassengers', '1952-10-01', 191], ['AirPassengers', '1952-11-01', 172], ['AirPassengers', '1952-12-01', 194], ['AirPassengers', '1953-01-01', 196], ['AirPassengers', '1953-02-01', 196], ['AirPassengers', '1953-03-01', 236], ['AirPassengers', '1953-04-01', 235], ['AirPassengers', '1953-05-01', 229], ['AirPassengers', '1953-06-01', 243], ['AirPassengers', '1953-07-01', 264], ['AirPassengers', '1953-08-01', 272], ['AirPassengers', '1953-09-01', 237], ['AirPassengers', '1953-10-01', 211], ['AirPassengers', '1953-11-01', 180], ['AirPassengers', '1953-12-01', 201], ['AirPassengers', '1954-01-01', 204], ['AirPassengers', '1954-02-01', 188], ['AirPassengers', '1954-03-01', 235], ['AirPassengers', '1954-04-01', 227], ['AirPassengers', '1954-05-01', 234], ['AirPassengers', '1954-06-01', 264], ['AirPassengers', '1954-07-01', 302], ['AirPassengers', '1954-08-01', 293], ['AirPassengers', '1954-09-01', 259], ['AirPassengers', '1954-10-01', 229], ['AirPassengers', '1954-11-01', 203], ['AirPassengers', '1954-12-01', 229], ['AirPassengers', '1955-01-01', 242], ['AirPassengers', '1955-02-01', 233], ['AirPassengers', '1955-03-01', 267], ['AirPassengers', '1955-04-01', 269], ['AirPassengers', '1955-05-01', 270], ['AirPassengers', '1955-06-01', 315], ['AirPassengers', '1955-07-01', 364], ['AirPassengers', '1955-08-01', 347], ['AirPassengers', '1955-09-01', 312], ['AirPassengers', '1955-10-01', 274], ['AirPassengers', '1955-11-01', 237], ['AirPassengers', '1955-12-01', 278], ['AirPassengers', '1956-01-01', 284], ['AirPassengers', '1956-02-01', 277], ['AirPassengers', '1956-03-01', 317], ['AirPassengers', '1956-04-01', 313], ['AirPassengers', '1956-05-01', 318], ['AirPassengers', '1956-06-01', 374], ['AirPassengers', '1956-07-01', 413], ['AirPassengers', '1956-08-01', 405], ['AirPassengers', '1956-09-01', 355], ['AirPassengers', '1956-10-01', 306], ['AirPassengers', '1956-11-01', 271], ['AirPassengers', '1956-12-01', 306], ['AirPassengers', '1957-01-01', 315], ['AirPassengers', '1957-02-01', 301], ['AirPassengers', '1957-03-01', 356], ['AirPassengers', '1957-04-01', 348], ['AirPassengers', '1957-05-01', 355], ['AirPassengers', '1957-06-01', 422], ['AirPassengers', '1957-07-01', 465], ['AirPassengers', '1957-08-01', 467], ['AirPassengers', '1957-09-01', 404], ['AirPassengers', '1957-10-01', 347], ['AirPassengers', '1957-11-01', 305], ['AirPassengers', '1957-12-01', 336], ['AirPassengers', '1958-01-01', 340], ['AirPassengers', '1958-02-01', 318], ['AirPassengers', '1958-03-01', 362], ['AirPassengers', '1958-04-01', 348], ['AirPassengers', '1958-05-01', 363], ['AirPassengers', '1958-06-01', 435], ['AirPassengers', '1958-07-01', 491], ['AirPassengers', '1958-08-01', 505], ['AirPassengers', '1958-09-01', 404], ['AirPassengers', '1958-10-01', 359], ['AirPassengers', '1958-11-01', 310], ['AirPassengers', '1958-12-01', 337], ['AirPassengers', '1959-01-01', 360], ['AirPassengers', '1959-02-01', 342], ['AirPassengers', '1959-03-01', 406], ['AirPassengers', '1959-04-01', 396], ['AirPassengers', '1959-05-01', 420], ['AirPassengers', '1959-06-01', 472], ['AirPassengers', '1959-07-01', 548], ['AirPassengers', '1959-08-01', 559], ['AirPassengers', '1959-09-01', 463], ['AirPassengers', '1959-10-01', 407], ['AirPassengers', '1959-11-01', 362], ['AirPassengers', '1959-12-01', 405], ['AirPassengers', '1960-01-01', 417], ['AirPassengers', '1960-02-01', 391], ['AirPassengers', '1960-03-01', 419], ['AirPassengers', '1960-04-01', 461], ['AirPassengers', '1960-05-01', 472], ['AirPassengers', '1960-06-01', 535], ['AirPassengers', '1960-07-01', 622], ['AirPassengers', '1960-08-01', 606], ['AirPassengers', '1960-09-01', 508], ['AirPassengers', '1960-10-01', 461], ['AirPassengers', '1960-11-01', 390], ['AirPassengers', '1960-12-01', 432]]} None\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: MS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Calling Forecast Endpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131], 'columns': ['unique_id', 'ds', 'y'], 'data': [['AirPassengers', '1949-01-01', 112], ['AirPassengers', '1949-02-01', 118], ['AirPassengers', '1949-03-01', 132], ['AirPassengers', '1949-04-01', 129], ['AirPassengers', '1949-05-01', 121], ['AirPassengers', '1949-06-01', 135], ['AirPassengers', '1949-07-01', 148], ['AirPassengers', '1949-08-01', 148], ['AirPassengers', '1949-09-01', 136], ['AirPassengers', '1949-10-01', 119], ['AirPassengers', '1949-11-01', 104], ['AirPassengers', '1949-12-01', 118], ['AirPassengers', '1950-01-01', 115], ['AirPassengers', '1950-02-01', 126], ['AirPassengers', '1950-03-01', 141], ['AirPassengers', '1950-04-01', 135], ['AirPassengers', '1950-05-01', 125], ['AirPassengers', '1950-06-01', 149], ['AirPassengers', '1950-07-01', 170], ['AirPassengers', '1950-08-01', 170], ['AirPassengers', '1950-09-01', 158], ['AirPassengers', '1950-10-01', 133], ['AirPassengers', '1950-11-01', 114], ['AirPassengers', '1950-12-01', 140], ['AirPassengers', '1951-01-01', 145], ['AirPassengers', '1951-02-01', 150], ['AirPassengers', '1951-03-01', 178], ['AirPassengers', '1951-04-01', 163], ['AirPassengers', '1951-05-01', 172], ['AirPassengers', '1951-06-01', 178], ['AirPassengers', '1951-07-01', 199], ['AirPassengers', '1951-08-01', 199], ['AirPassengers', '1951-09-01', 184], ['AirPassengers', '1951-10-01', 162], ['AirPassengers', '1951-11-01', 146], ['AirPassengers', '1951-12-01', 166], ['AirPassengers', '1952-01-01', 171], ['AirPassengers', '1952-02-01', 180], ['AirPassengers', '1952-03-01', 193], ['AirPassengers', '1952-04-01', 181], ['AirPassengers', '1952-05-01', 183], ['AirPassengers', '1952-06-01', 218], ['AirPassengers', '1952-07-01', 230], ['AirPassengers', '1952-08-01', 242], ['AirPassengers', '1952-09-01', 209], ['AirPassengers', '1952-10-01', 191], ['AirPassengers', '1952-11-01', 172], ['AirPassengers', '1952-12-01', 194], ['AirPassengers', '1953-01-01', 196], ['AirPassengers', '1953-02-01', 196], ['AirPassengers', '1953-03-01', 236], ['AirPassengers', '1953-04-01', 235], ['AirPassengers', '1953-05-01', 229], ['AirPassengers', '1953-06-01', 243], ['AirPassengers', '1953-07-01', 264], ['AirPassengers', '1953-08-01', 272], ['AirPassengers', '1953-09-01', 237], ['AirPassengers', '1953-10-01', 211], ['AirPassengers', '1953-11-01', 180], ['AirPassengers', '1953-12-01', 201], ['AirPassengers', '1954-01-01', 204], ['AirPassengers', '1954-02-01', 188], ['AirPassengers', '1954-03-01', 235], ['AirPassengers', '1954-04-01', 227], ['AirPassengers', '1954-05-01', 234], ['AirPassengers', '1954-06-01', 264], ['AirPassengers', '1954-07-01', 302], ['AirPassengers', '1954-08-01', 293], ['AirPassengers', '1954-09-01', 259], ['AirPassengers', '1954-10-01', 229], ['AirPassengers', '1954-11-01', 203], ['AirPassengers', '1954-12-01', 229], ['AirPassengers', '1955-01-01', 242], ['AirPassengers', '1955-02-01', 233], ['AirPassengers', '1955-03-01', 267], ['AirPassengers', '1955-04-01', 269], ['AirPassengers', '1955-05-01', 270], ['AirPassengers', '1955-06-01', 315], ['AirPassengers', '1955-07-01', 364], ['AirPassengers', '1955-08-01', 347], ['AirPassengers', '1955-09-01', 312], ['AirPassengers', '1955-10-01', 274], ['AirPassengers', '1955-11-01', 237], ['AirPassengers', '1955-12-01', 278], ['AirPassengers', '1956-01-01', 284], ['AirPassengers', '1956-02-01', 277], ['AirPassengers', '1956-03-01', 317], ['AirPassengers', '1956-04-01', 313], ['AirPassengers', '1956-05-01', 318], ['AirPassengers', '1956-06-01', 374], ['AirPassengers', '1956-07-01', 413], ['AirPassengers', '1956-08-01', 405], ['AirPassengers', '1956-09-01', 355], ['AirPassengers', '1956-10-01', 306], ['AirPassengers', '1956-11-01', 271], ['AirPassengers', '1956-12-01', 306], ['AirPassengers', '1957-01-01', 315], ['AirPassengers', '1957-02-01', 301], ['AirPassengers', '1957-03-01', 356], ['AirPassengers', '1957-04-01', 348], ['AirPassengers', '1957-05-01', 355], ['AirPassengers', '1957-06-01', 422], ['AirPassengers', '1957-07-01', 465], ['AirPassengers', '1957-08-01', 467], ['AirPassengers', '1957-09-01', 404], ['AirPassengers', '1957-10-01', 347], ['AirPassengers', '1957-11-01', 305], ['AirPassengers', '1957-12-01', 336], ['AirPassengers', '1958-01-01', 340], ['AirPassengers', '1958-02-01', 318], ['AirPassengers', '1958-03-01', 362], ['AirPassengers', '1958-04-01', 348], ['AirPassengers', '1958-05-01', 363], ['AirPassengers', '1958-06-01', 435], ['AirPassengers', '1958-07-01', 491], ['AirPassengers', '1958-08-01', 505], ['AirPassengers', '1958-09-01', 404], ['AirPassengers', '1958-10-01', 359], ['AirPassengers', '1958-11-01', 310], ['AirPassengers', '1958-12-01', 337], ['AirPassengers', '1959-01-01', 360], ['AirPassengers', '1959-02-01', 342], ['AirPassengers', '1959-03-01', 406], ['AirPassengers', '1959-04-01', 396], ['AirPassengers', '1959-05-01', 420], ['AirPassengers', '1959-06-01', 472], ['AirPassengers', '1959-07-01', 548], ['AirPassengers', '1959-08-01', 559], ['AirPassengers', '1959-09-01', 463], ['AirPassengers', '1959-10-01', 407], ['AirPassengers', '1959-11-01', 362], ['AirPassengers', '1959-12-01', 405]]} None\n",
      "[]\n",
      "[65392623971200216, 65392623971200220, 65392623971200280, 65392623971200220, 65392623971200260, 65392623971200296, 65392623971200310, 65392623971200296, 65392623971200260, 65392623971200220, 65392623971200184, 65392623971200220]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/nixtla/nbs/timegpt.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m hyp \u001b[39min\u001b[39;00m hyps:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(hyp)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     fcst_test \u001b[39m=\u001b[39m timegpt\u001b[39m.\u001b[39;49mforecast(df_train, h\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhyp)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     fcst_test\u001b[39m.\u001b[39minsert(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, df_test[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     fcst_cv \u001b[39m=\u001b[39m timegpt\u001b[39m.\u001b[39mcross_validation(df_, h\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhyp)\n",
      "\u001b[1;32m/home/ubuntu/nixtla/nbs/timegpt.ipynb Cell 31\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forecast your time series using TimeGPT.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m    predictions (if level is not None).\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(df, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forecast(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         df\u001b[39m=\u001b[39;49mdf,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         h\u001b[39m=\u001b[39;49mh,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m         freq\u001b[39m=\u001b[39;49mfreq,    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m         id_col\u001b[39m=\u001b[39;49mid_col,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m         time_col\u001b[39m=\u001b[39;49mtime_col,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m         target_col\u001b[39m=\u001b[39;49mtarget_col,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m         X_df\u001b[39m=\u001b[39;49mX_df,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m         finetune_steps\u001b[39m=\u001b[39;49mfinetune_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m         clean_ex_first\u001b[39m=\u001b[39;49mclean_ex_first,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m         validate_token\u001b[39m=\u001b[39;49mvalidate_token,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m         add_history\u001b[39m=\u001b[39;49madd_history,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m         date_features\u001b[39m=\u001b[39;49mdate_features,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m         date_features_to_one_hot\u001b[39m=\u001b[39;49mdate_features_to_one_hot,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnixtlats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtimegpt\u001b[39;00m \u001b[39mimport\u001b[39;00m _DistributedTimeGPT\n",
      "\u001b[1;32m/home/ubuntu/nixtla/nbs/timegpt.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_x \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m model \u001b[39m=\u001b[39m _TimeGPTModel(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     h\u001b[39m=\u001b[39mh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=127'>128</a>\u001b[0m     date_features_to_one_hot\u001b[39m=\u001b[39mdate_features_to_one_hot,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m fcst_df \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforecast(df\u001b[39m=\u001b[39;49mdf, X_df\u001b[39m=\u001b[39;49mX_df, add_history\u001b[39m=\u001b[39;49madd_history)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_x \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mweights_x\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mreturn\u001b[39;00m fcst_df\n",
      "\u001b[1;32m/home/ubuntu/nixtla/nbs/timegpt.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=322'>323</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mweights_x\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m response_timegpt:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=323'>324</a>\u001b[0m     \u001b[39mprint\u001b[39m(response_timegpt[\u001b[39m'\u001b[39m\u001b[39mweights_x\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=324'>325</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=325'>326</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_cols,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=326'>327</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mweights\u001b[39;49m\u001b[39m'\u001b[39;49m: response_timegpt[\u001b[39m'\u001b[39;49m\u001b[39mweights_x\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=327'>328</a>\u001b[0m     })\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=328'>329</a>\u001b[0m fcst_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_timegpt[\u001b[39m'\u001b[39m\u001b[39mforecast\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmegafaas-instance/home/ubuntu/nixtla/nbs/timegpt.ipynb#Y264sdnNjb2RlLXJlbW90ZQ%3D%3D?line=329'>330</a>\u001b[0m \u001b[39mif\u001b[39;00m add_history:\n",
      "File \u001b[0;32m~/miniconda/envs/nixtlats/lib/python3.11/site-packages/pandas/core/frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nixtlats/lib/python3.11/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/miniconda/envs/nixtlats/lib/python3.11/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda/envs/nixtlats/lib/python3.11/site-packages/pandas/core/internals/construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# cross validation tests\n",
    "df_copy = df_.copy()\n",
    "timegpt.cross_validation(df_, h=12, n_windows=2)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_copy,\n",
    "    df_,\n",
    ")\n",
    "df_test = df_.groupby('unique_id').tail(12)\n",
    "df_train = df_.drop(df_test.index)\n",
    "hyps = [\n",
    "    dict(),\n",
    "    dict(date_features=['month']),\n",
    "    dict(finetune_steps=2),\n",
    "    dict(level=[80, 90]),\n",
    "]\n",
    "for hyp in hyps:\n",
    "    print(hyp)\n",
    "    fcst_test = timegpt.forecast(df_train, h=12, **hyp)\n",
    "    fcst_test.insert(2, 'y', df_test['y'].values)\n",
    "    fcst_cv = timegpt.cross_validation(df_, h=12, **hyp)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_test,\n",
    "        fcst_cv.drop(columns='cutoff'),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validating inputs...\n",
      "INFO:__main__:Preprocessing dataframes...\n",
      "INFO:__main__:Inferred freq: MS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Calling Forecast Endpoint...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>TimeGPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>421.209320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-02-01</td>\n",
       "      <td>409.877808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-03-01</td>\n",
       "      <td>452.973145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-04-01</td>\n",
       "      <td>452.345306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-05-01</td>\n",
       "      <td>476.039246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-06-01</td>\n",
       "      <td>535.992554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-07-01</td>\n",
       "      <td>610.464844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-08-01</td>\n",
       "      <td>624.355957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-09-01</td>\n",
       "      <td>511.534424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-10-01</td>\n",
       "      <td>457.592957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-11-01</td>\n",
       "      <td>407.409485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-12-01</td>\n",
       "      <td>430.787903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unique_id          ds     TimeGPT\n",
       "0   AirPassengers  1960-01-01  421.209320\n",
       "1   AirPassengers  1960-02-01  409.877808\n",
       "2   AirPassengers  1960-03-01  452.973145\n",
       "3   AirPassengers  1960-04-01  452.345306\n",
       "4   AirPassengers  1960-05-01  476.039246\n",
       "5   AirPassengers  1960-06-01  535.992554\n",
       "6   AirPassengers  1960-07-01  610.464844\n",
       "7   AirPassengers  1960-08-01  624.355957\n",
       "8   AirPassengers  1960-09-01  511.534424\n",
       "9   AirPassengers  1960-10-01  457.592957\n",
       "10  AirPassengers  1960-11-01  407.409485\n",
       "11  AirPassengers  1960-12-01  430.787903"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timegpt.forecast(df_train, h=12, finetune_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-02-01</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-03-01</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-04-01</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-05-01</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1959-08-01</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1959-09-01</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1959-10-01</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1959-11-01</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1959-12-01</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unique_id          ds    y\n",
       "0    AirPassengers  1949-01-01  112\n",
       "1    AirPassengers  1949-02-01  118\n",
       "2    AirPassengers  1949-03-01  132\n",
       "3    AirPassengers  1949-04-01  129\n",
       "4    AirPassengers  1949-05-01  121\n",
       "..             ...         ...  ...\n",
       "127  AirPassengers  1959-08-01  559\n",
       "128  AirPassengers  1959-09-01  463\n",
       "129  AirPassengers  1959-10-01  407\n",
       "130  AirPassengers  1959-11-01  362\n",
       "131  AirPassengers  1959-12-01  405\n",
       "\n",
       "[132 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-02-01</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-03-01</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-04-01</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1949-05-01</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-08-01</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-09-01</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-10-01</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-11-01</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>AirPassengers</td>\n",
       "      <td>1960-12-01</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unique_id          ds    y\n",
       "0    AirPassengers  1949-01-01  112\n",
       "1    AirPassengers  1949-02-01  118\n",
       "2    AirPassengers  1949-03-01  132\n",
       "3    AirPassengers  1949-04-01  129\n",
       "4    AirPassengers  1949-05-01  121\n",
       "..             ...         ...  ...\n",
       "139  AirPassengers  1960-08-01  606\n",
       "140  AirPassengers  1960-09-01  508\n",
       "141  AirPassengers  1960-10-01  461\n",
       "142  AirPassengers  1960-11-01  390\n",
       "143  AirPassengers  1960-12-01  432\n",
       "\n",
       "[144 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test pass dataframe with index\n",
    "df_ds_index = df_.set_index('ds')[['unique_id', 'y']]\n",
    "df_ds_index.index = pd.DatetimeIndex(df_ds_index.index)\n",
    "fcst_inferred_df_index = timegpt.forecast(df_ds_index, h=10)\n",
    "anom_inferred_df_index = timegpt.detect_anomalies(df_ds_index)\n",
    "fcst_inferred_df = timegpt.forecast(df_[['ds', 'unique_id', 'y']], h=10)\n",
    "anom_inferred_df = timegpt.detect_anomalies(df_[['ds', 'unique_id', 'y']])\n",
    "pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df, atol=1e-3)\n",
    "pd.testing.assert_frame_equal(anom_inferred_df_index, anom_inferred_df, atol=1e-3)\n",
    "df_ds_index = df_ds_index.groupby('unique_id').tail(80)\n",
    "for freq in ['Y', 'W-MON', 'Q-DEC', 'H']:\n",
    "    df_ds_index.index = np.concatenate(\n",
    "        df_ds_index['unique_id'].nunique() * [pd.date_range(end='2023-01-01', periods=80, freq=freq)]\n",
    "    )\n",
    "    fcst_inferred_df_index = timegpt.forecast(df_ds_index, h=10)\n",
    "    df_test = df_ds_index.reset_index()\n",
    "    fcst_inferred_df = timegpt.forecast(df_test, h=10)\n",
    "    pd.testing.assert_frame_equal(fcst_inferred_df_index, fcst_inferred_df, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features with exogenous variables \n",
    "# and multiple series\n",
    "date_features = ['year', 'month']\n",
    "df_date_features, future_df = _TimeGPTModel(\n",
    "    client=timegpt.client,\n",
    "    h=24, \n",
    "    freq='H', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=None,\n",
    ").add_date_features(df_,  X_df=df_actual_future)\n",
    "assert all(col in df_date_features for col in date_features)\n",
    "assert all(col in future_df for col in date_features)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_date_features[df_.columns],\n",
    "    df_,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    future_df[df_actual_future.columns],\n",
    "    df_actual_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test add date features one hot with exogenous variables \n",
    "# and multiple series\n",
    "date_features = ['month', 'day']\n",
    "df_date_features, future_df = _TimeGPTModel(\n",
    "    client=timegpt.client,\n",
    "    h=24, \n",
    "    freq='H', \n",
    "    date_features=date_features,\n",
    "    date_features_to_one_hot=date_features,\n",
    ").add_date_features(df_,  X_df=df_actual_future)\n",
    "pd.testing.assert_frame_equal(\n",
    "    df_date_features[df_.columns],\n",
    "    df_,\n",
    ")\n",
    "pd.testing.assert_frame_equal(\n",
    "    future_df[df_actual_future.columns],\n",
    "    df_actual_future,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test warning horizon too long\n",
    "timegpt.forecast(df=df.tail(3), h=100, time_col='timestamp', target_col='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with add_history\n",
    "test_fail(\n",
    "    lambda: timegpt.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', add_history=True),\n",
    "    contains='be sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with finetunning\n",
    "test_fail(\n",
    "    lambda: timegpt.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', finetune_steps=10),\n",
    "    contains='be sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# test short horizon with level\n",
    "test_fail(\n",
    "    lambda: timegpt.forecast(df=df.tail(3), h=12, time_col='timestamp', target_col='value', level=[80, 90]),\n",
    "    contains='be sure'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test custom url\n",
    "# same results\n",
    "_timegpt_fcst_df = _timegpt.forecast(df=df, h=12, time_col='timestamp', target_col='value')\n",
    "timegpt_fcst_df = timegpt.forecast(df=df, h=12, time_col='timestamp', target_col='value')\n",
    "pd.testing.assert_frame_equal(\n",
    "    _timegpt_fcst_df,\n",
    "    timegpt_fcst_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TimeGPT.detect_anomalies, title_level=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
